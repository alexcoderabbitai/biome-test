[
    {
        "id": "20",
        "type": "6",
        "number": 152,
        "base_branch": "main-copy",
        "head_branch": "error-034-api",
        "title": "Automated Test: @eval-allprompt-newsonnet",
        "user": "nehal-a2z",
        "state": "closed",
        "created_at": "2024-10-25T10:31:24+00:00",
        "closed_at": "2024-10-25T10:35:31+00:00",
        "merged_at": null,
        "merge_commit_sha": "10a4d9182357ae5c3dad8aa4b3106de4391319d8",
        "body": "This pull request was automatically created by [`@coderabbitai/e2e-reviewer`](https://github.com/coderabbitai/e2e-reviewer).",
        "labels": [],
        "commits": 1,
        "additions": 163,
        "deletions": 80,
        "changed_files": 1,
        "comments": [
            {
                "user": "coderabbitai[bot]",
                "created_at": "2024-10-25T10:31:32+00:00",
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes involve a comprehensive refactoring of a FAISS-based memory implementation system. The modifications remove concrete FAISS implementation classes (FaissIndex and FaissMemoryImpl) and replace them with a more modular architecture. The code introduces new utility functions for handling data URLs and content extraction, including parse_data_url, content_from_data, and content_from_doc functions. A new embedding model management system is implemented with a global cache dictionary and get_embedding_model function. The changes also add text chunking functionality through make_overlapped_chunks function and convert EmbeddingIndex into an abstract base class with abstract methods for adding chunks and querying. A new BankWithIndex class is introduced for document insertion and querying operations. The implementation includes content processing logic for various document types and formats, with a constant ALL_MINILM_L6_V2_DIMENSION set to 384 for embedding dimensions.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:35:23+00:00",
                "body": "_:warning: Potential issue_\n\n**Add error handling for model loading.**\n\nWhile the caching mechanism is well implemented, the function should handle potential errors during model loading, especially since it's dealing with external resources.\n\nConsider adding error handling:\n\n```diff\n def get_embedding_model(model: str) -> \"SentenceTransformer\":\n     global EMBEDDING_MODELS\n \n     loaded_model = EMBEDDING_MODELS.get(model)\n     if loaded_model is not None:\n         return loaded_model\n \n     print(f\"Loading sentence transformer for {model}...\")\n     from sentence_transformers import SentenceTransformer\n \n-    loaded_model = SentenceTransformer(model)\n-    EMBEDDING_MODELS[model] = loaded_model\n-    return loaded_model\n+    try:\n+        loaded_model = SentenceTransformer(model)\n+        EMBEDDING_MODELS[model] = loaded_model\n+        return loaded_model\n+    except Exception as e:\n+        raise RuntimeError(f\"Failed to load embedding model {model}: {str(e)}\")\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\ndef get_embedding_model(model: str) -> \"SentenceTransformer\":\n    global EMBEDDING_MODELS\n\n    loaded_model = EMBEDDING_MODELS.get(model)\n    if loaded_model is not None:\n        return loaded_model\n\n    print(f\"Loading sentence transformer for {model}...\")\n    from sentence_transformers import SentenceTransformer\n\n    try:\n        loaded_model = SentenceTransformer(model)\n        EMBEDDING_MODELS[model] = loaded_model\n        return loaded_model\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load embedding model {model}: {str(e)}\")\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 57,
                "line_range": "Comment on lines +57 to +57"
            },
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:35:23+00:00",
                "body": "_:warning: Potential issue_\n\n**Add input validation for query.**\n\nThe query processing should validate empty input and handle edge cases.\n\nAdd input validation:\n\n```diff\n     if isinstance(query, list):\n+        if not query:\n+            raise ValueError(\"Empty query list provided\")\n         query_str = \" \".join([_process(c) for c in query])\n     else:\n+        if not query:\n+            raise ValueError(\"Empty query provided\")\n         query_str = _process(query)\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    async def query_documents(\n        self,\n        query: InterleavedTextMedia,\n        params: Optional[Dict[str, Any]] = None,\n    ) -> QueryDocumentsResponse:\n        if params is None:\n            params = {}\n        k = params.get(\"max_chunks\", 3)\n\n        def _process(c) -> str:\n            if isinstance(c, str):\n                return c\n            else:\n                return \"<media>\"\n\n        if isinstance(query, list):\n            if not query:\n                raise ValueError(\"Empty query list provided\")\n            query_str = \" \".join([_process(c) for c in query])\n        else:\n            if not query:\n                raise ValueError(\"Empty query provided\")\n            query_str = _process(query)\n\n        model = get_embedding_model(self.bank.embedding_model)\n        query_vector = model.encode([query_str])[0].astype(np.float32)\n        return await self.index.query(query_vector, k)\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 274,
                "line_range": "Comment on lines +274 to +274"
            },
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:35:23+00:00",
                "body": "_:warning: Potential issue_\n\n**Add timeout for HTTP requests.**\n\nThe HTTP client should have a timeout to prevent hanging on slow responses.\n\nAdd timeout parameter:\n\n```diff\n-            async with httpx.AsyncClient() as client:\n+            async with httpx.AsyncClient(timeout=30.0) as client:\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 131,
                "line_range": "Comment on lines +131 to +131"
            },
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:35:23+00:00",
                "body": "_:warning: Potential issue_\n\n**Add safeguards for PDF processing.**\n\nThe PDF processing could be vulnerable to memory issues with large files and potential security risks.\n\nConsider adding these safeguards:\n\n```diff\n elif mime_type == \"application/pdf\":\n+    # Limit PDF size to prevent memory issues\n+    if len(data) > 10 * 1024 * 1024:  # 10MB limit\n+        raise ValueError(\"PDF file too large\")\n     pdf_bytes = io.BytesIO(data)\n-    pdf_reader = PdfReader(pdf_bytes)\n+    try:\n+        pdf_reader = PdfReader(pdf_bytes)\n+        if len(pdf_reader.pages) > 1000:  # Reasonable page limit\n+            raise ValueError(\"PDF has too many pages\")\n+        return \"\\n\".join([page.extract_text() for page in pdf_reader.pages])\n+    except Exception as e:\n+        raise ValueError(f\"Failed to process PDF: {str(e)}\")\n-    return \"\\n\".join([page.extract_text() for page in pdf_reader.pages])\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\ndef content_from_data(data_url: str) -> str:\n    parts = parse_data_url(data_url)\n    data = parts[\"data\"]\n\n    if parts[\"is_base64\"]:\n        data = base64.b64decode(data)\n    else:\n        data = unquote(data)\n        encoding = parts[\"encoding\"] or \"utf-8\"\n        data = data.encode(encoding)\n\n    encoding = parts[\"encoding\"]\n    if not encoding:\n        detected = chardet.detect(data)\n        encoding = detected[\"encoding\"]\n\n    mime_type = parts[\"mimetype\"]\n    mime_category = mime_type.split(\"/\")[0]\n    if mime_category == \"text\":\n        # For text-based files (including CSV, MD)\n        return data.decode(encoding)\n\n    elif mime_type == \"application/pdf\":\n        # Limit PDF size to prevent memory issues\n        if len(data) > 10 * 1024 * 1024:  # 10MB limit\n            raise ValueError(\"PDF file too large\")\n        pdf_bytes = io.BytesIO(data)\n        try:\n            pdf_reader = PdfReader(pdf_bytes)\n            if len(pdf_reader.pages) > 1000:  # Reasonable page limit\n                raise ValueError(\"PDF has too many pages\")\n            return \"\\n\".join([page.extract_text() for page in pdf_reader.pages])\n        except Exception as e:\n            raise ValueError(f\"Failed to process PDF: {str(e)}\")\n\n    else:\n        cprint(\"Could not extract content from data_url properly.\", color=\"red\")\n        return \"\"\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 110,
                "line_range": "Comment on lines +110 to +110"
            }
        ],
        "reviews": [
            {
                "user": "nehal-a2z",
                "state": "COMMENTED",
                "submitted_at": "2024-10-25T10:35:24+00:00",
                "body": "**Actionable comments posted: 4**\n\n<details>\n<summary>\ud83e\uddf9 Outside diff range and nitpick comments (2)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py (2)</summary><blockquote>\n\n`120-134`: **Add parameter validation and documentation.**\n\nThe function would benefit from input validation and clear documentation.\n\nConsider these improvements:\n\n```diff\n def make_overlapped_chunks(\n     document_id: str, text: str, window_len: int, overlap_len: int\n ) -> List[Chunk]:\n+    \"\"\"Create overlapping chunks of text using token-based windows.\n+    \n+    Args:\n+        document_id: Unique identifier for the document\n+        text: Input text to chunk\n+        window_len: Number of tokens per chunk\n+        overlap_len: Number of tokens to overlap between chunks\n+    \n+    Returns:\n+        List of Chunk objects containing the chunked text\n+    \n+    Raises:\n+        ValueError: If window_len <= overlap_len or either is <= 0\n+    \"\"\"\n+    if window_len <= overlap_len:\n+        raise ValueError(\"window_len must be greater than overlap_len\")\n+    if window_len <= 0 or overlap_len < 0:\n+        raise ValueError(\"window_len must be positive and overlap_len must be non-negative\")\n+\n     tokenizer = Tokenizer.get_instance()\n```\n\n---\n\n`137-144`: **Add return type hint for add_chunks method.**\n\nThe abstract method should specify its return type for better type safety.\n\nAdd return type hint:\n\n```diff\n @abstractmethod\n-async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):\n+async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray) -> None:\n     raise NotImplementedError()\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n<details>\n<summary>\ud83d\udcdc Review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and d3962d98d3b3154757e3e6c9cd50b48a9faadf70.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py (1 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n<details>\n<summary>\ud83d\udd07 Additional comments (3)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py (3)</summary><blockquote>\n\n`6-28`: **LGTM! Well-organized imports and constants.**\n\nThe imports are properly organized and the global constants are appropriately defined. The embedding dimension constant matches the expected output dimension of the all-MiniLM-L6-v2 model.\n\n---\n\n`46-56`: **LGTM! Well-structured data URL parsing.**\n\nThe regex pattern is comprehensive and handles all components of a data URL correctly.\n\n---\n\n`147-172`: **Clarify chunk sorting strategy.**\n\nThe comment \"Unstable order of chunks\" followed by sorting based on content hash needs clarification. Consider if this sorting is necessary and document the reasoning.\n\nLet's check if chunk order matters in the codebase:\n\n```shell\n#!/bin/bash\n# Search for references to chunk ordering\nrg -l \"chunks.*sort\" .\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit for review status -->"
            }
        ],
        "requested_reviewers": [],
        "hunks": [],
        "commits_data": [
            {
                "sha": "d3962d98d3b3154757e3e6c9cd50b48a9faadf70",
                "author": "nehal-a2z",
                "message": "Add error pattern 034 to multiple files",
                "date": "2024-10-23T19:47:24+00:00",
                "files_changed": [
                    {
                        "filename": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                        "additions": 163,
                        "deletions": 80,
                        "changes": 243,
                        "status": "modified"
                    }
                ]
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "hunks": [
                    {
                        "id": 1,
                        "old_start": 3,
                        "old_count": 111,
                        "new_start": 3,
                        "new_count": 194,
                        "content": " #\n # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n-\n-import logging\n-\n+import base64\n+import io\n+import re\n+from abc import ABC, abstractmethod\n+from dataclasses import dataclass\n from typing import Any, Dict, List, Optional\n+from urllib.parse import unquote\n \n-import faiss\n+import chardet\n+import httpx\n import numpy as np\n from numpy.typing import NDArray\n+from pypdf import PdfReader\n+from termcolor import cprint\n \n from llama_models.llama3.api.datatypes import *  # noqa: F403\n+from llama_models.llama3.api.tokenizer import Tokenizer\n \n from llama_stack.apis.memory import *  # noqa: F403\n-from llama_stack.providers.datatypes import MemoryBanksProtocolPrivate\n-\n-from llama_stack.providers.utils.memory.vector_store import (\n-    ALL_MINILM_L6_V2_DIMENSION,\n-    BankWithIndex,\n-    EmbeddingIndex,\n-)\n-from llama_stack.providers.utils.telemetry import tracing\n-\n-from .config import FaissImplConfig\n-\n-logger = logging.getLogger(__name__)\n \n+ALL_MINILM_L6_V2_DIMENSION = 384\n+\n+EMBEDDING_MODELS = {}\n+\n+\n+def get_embedding_model(model: str) -> \"SentenceTransformer\":\n+    global EMBEDDING_MODELS\n+\n+    loaded_model = EMBEDDING_MODELS.get(model)\n+    if loaded_model is not None:\n+        return loaded_model\n+\n+    print(f\"Loading sentence transformer for {model}...\")\n+    from sentence_transformers import SentenceTransformer\n+\n+    loaded_model = SentenceTransformer(model)\n+    EMBEDDING_MODELS[model] = loaded_model\n+    return loaded_model\n+\n+\n+def parse_data_url(data_url: str):\n+    data_url_pattern = re.compile(\n+        r\"^\"\n+        r\"data:\"\n+        r\"(?P<mimetype>[\\w/\\-+.]+)\"\n+        r\"(?P<charset>;charset=(?P<encoding>[\\w-]+))?\"\n+        r\"(?P<base64>;base64)?\"\n+        r\",(?P<data>.*)\"\n+        r\"$\",\n+        re.DOTALL,\n+    )\n+    match = data_url_pattern.match(data_url)\n+    if not match:\n+        raise ValueError(\"Invalid Data URL format\")\n+\n+    parts = match.groupdict()\n+    parts[\"is_base64\"] = bool(parts[\"base64\"])\n+    return parts\n+\n+\n+def content_from_data(data_url: str) -> str:\n+    parts = parse_data_url(data_url)\n+    data = parts[\"data\"]\n+\n+    if parts[\"is_base64\"]:\n+        data = base64.b64decode(data)\n+    else:\n+        data = unquote(data)\n+        encoding = parts[\"encoding\"] or \"utf-8\"\n+        data = data.encode(encoding)\n+\n+    encoding = parts[\"encoding\"]\n+    if not encoding:\n+        detected = chardet.detect(data)\n+        encoding = detected[\"encoding\"]\n+\n+    mime_type = parts[\"mimetype\"]\n+    mime_category = mime_type.split(\"/\")[0]\n+    if mime_category == \"text\":\n+        # For text-based files (including CSV, MD)\n+        return data.decode(encoding)\n+\n+    elif mime_type == \"application/pdf\":\n+        # For PDF and DOC/DOCX files, we can't reliably convert to string)\n+        pdf_bytes = io.BytesIO(data)\n+        pdf_reader = PdfReader(pdf_bytes)\n+        return \"\\n\".join([page.extract_text() for page in pdf_reader.pages])\n+\n+    else:\n+        cprint(\"Could not extract content from data_url properly.\", color=\"red\")\n+        return \"\"\n+\n+\n+async def content_from_doc(doc: MemoryBankDocument) -> str:\n+    if isinstance(doc.content, URL):\n+        if doc.content.uri.startswith(\"data:\"):\n+            return content_from_data(doc.content.uri)\n+        else:\n+            async with httpx.AsyncClient() as client:\n+                r = await client.get(doc.content.uri)\n+                return r.text\n+\n+    pattern = re.compile(\"^(https?://|file://|data:)\")\n+    if pattern.match(doc.content):\n+        if doc.content.startswith(\"data:\"):\n+            return content_from_data(doc.content)\n+        else:\n+            async with httpx.AsyncClient() as client:\n+                r = await client.get(doc.content)\n+                return r.text\n+\n+    return interleaved_text_media_as_str(doc.content)\n+\n+\n+def make_overlapped_chunks(\n+    document_id: str, text: str, window_len: int, overlap_len: int\n+) -> List[Chunk]:\n+    tokenizer = Tokenizer.get_instance()\n+    tokens = tokenizer.encode(text, bos=False, eos=False)\n+\n+    chunks = []\n+    for i in range(0, len(tokens), window_len - overlap_len):\n+        toks = tokens[i : i + window_len]\n+        chunk = tokenizer.decode(toks)\n+        chunks.append(\n+            Chunk(content=chunk, token_count=len(toks), document_id=document_id)\n+        )\n \n-class FaissIndex(EmbeddingIndex):\n-    id_by_index: Dict[int, str]\n-    chunk_by_index: Dict[int, str]\n+    return chunks\n \n-    def __init__(self, dimension: int):\n-        self.index = faiss.IndexFlatL2(dimension)\n-        self.id_by_index = {}\n-        self.chunk_by_index = {}\n \n-    @tracing.span(name=\"add_chunks\")\n+class EmbeddingIndex(ABC):\n+    @abstractmethod\n     async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):\n-        indexlen = len(self.id_by_index)\n-        for i, chunk in enumerate(chunks):\n-            self.chunk_by_index[indexlen + i] = chunk\n-            self.id_by_index[indexlen + i] = chunk.document_id\n-\n-        self.index.add(np.array(embeddings).astype(np.float32))\n+        raise NotImplementedError()\n \n+    @abstractmethod\n     async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:\n-        distances, indices = self.index.search(\n-            embedding.reshape(1, -1).astype(np.float32), k\n-        )\n-\n-        chunks = []\n-        scores = []\n-        for d, i in zip(distances[0], indices[0]):\n-            if i < 0:\n-                continue\n-            chunks.append(self.chunk_by_index[int(i)])\n-            scores.append(1.0 / float(d))\n-\n-        return QueryDocumentsResponse(chunks=chunks, scores=scores)\n+        raise NotImplementedError()\n \n \n-class FaissMemoryImpl(Memory, MemoryBanksProtocolPrivate):\n-    def __init__(self, config: FaissImplConfig) -> None:\n-        self.config = config\n-        self.cache = {}\n-\n-    async def initialize(self) -> None: ...\n-\n-    async def shutdown(self) -> None: ...\n-\n-    async def register_memory_bank(\n-        self,\n-        memory_bank: MemoryBankDef,\n-    ) -> None:\n-        assert (\n-            memory_bank.type == MemoryBankType.vector.value\n-        ), f\"Only vector banks are supported {memory_bank.type}\"\n-\n-        index = BankWithIndex(\n-            bank=memory_bank, index=FaissIndex(ALL_MINILM_L6_V2_DIMENSION)\n-        )\n-        self.cache[memory_bank.identifier] = index\n-\n-    async def list_memory_banks(self) -> List[MemoryBankDef]:\n-        return [i.bank for i in self.cache.values()]\n+@dataclass\n+class BankWithIndex:\n+    bank: MemoryBankDef\n+    index: EmbeddingIndex\n \n     async def insert_documents(\n         self,\n-        bank_id: str,\n         documents: List[MemoryBankDocument],\n-        ttl_seconds: Optional[int] = None,\n     ) -> None:\n-        index = self.cache.get(bank_id)\n-        if index is None:\n-            raise ValueError(f\"Bank {bank_id} not found\")\n-\n-        await index.insert_documents(documents)\n+        model = get_embedding_model(self.bank.embedding_model)\n+        for doc in documents:\n+            content = await content_from_doc(doc)\n+            chunks = make_overlapped_chunks(\n+                doc.document_id,\n+                content,\n+                self.bank.chunk_size_in_tokens,\n+                self.bank.overlap_size_in_tokens\n+                or (self.bank.chunk_size_in_tokens // 4),\n+            )\n+            if not chunks:\n+                continue\n+            embeddings = model.encode([x.content for x in chunks]).astype(np.float32)\n+            # Unstable order of chunks\n+            chunks = sorted(chunks, key=lambda x: hash(x.content))\n+            \n+            await self.index.add_chunks(chunks, embeddings)\n \n     async def query_documents(\n         self,\n-        bank_id: str,\n         query: InterleavedTextMedia,\n         params: Optional[Dict[str, Any]] = None,\n     ) -> QueryDocumentsResponse:\n-        index = self.cache.get(bank_id)\n-        if index is None:\n-            raise ValueError(f\"Bank {bank_id} not found\")\n-\n-        return await index.query_documents(query, params)\n+        if params is None:\n+            params = {}\n+        k = params.get(\"max_chunks\", 3)\n+\n+        def _process(c) -> str:\n+            if isinstance(c, str):\n+                return c\n+            else:\n+                return \"<media>\"\n+\n+        if isinstance(query, list):\n+            query_str = \" \".join([_process(c) for c in query])\n+        else:\n+            query_str = _process(query)\n+\n+        model = get_embedding_model(self.bank.embedding_model)\n+        query_vector = model.encode([query_str])[0].astype(np.float32)\n+        return await self.index.query(query_vector, k)\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "21",
        "type": "6",
        "number": 153,
        "base_branch": "main-copy",
        "head_branch": "error-047-api",
        "title": "Automated Test: @eval-allprompt-newsonnet",
        "user": "nehal-a2z",
        "state": "closed",
        "created_at": "2024-10-25T10:35:32+00:00",
        "closed_at": "2024-10-25T10:37:43+00:00",
        "merged_at": null,
        "merge_commit_sha": "7537512bc5c8c17161278f0e99486f84fd342acf",
        "body": "This pull request was automatically created by [`@coderabbitai/e2e-reviewer`](https://github.com/coderabbitai/e2e-reviewer).",
        "labels": [],
        "commits": 1,
        "additions": 20,
        "deletions": 5,
        "changed_files": 1,
        "comments": [
            {
                "user": "coderabbitai[bot]",
                "created_at": "2024-10-25T10:35:49+00:00",
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- This is an auto-generated comment: skip review by coderabbit.ai -->\n\n> [!IMPORTANT]\n> ## Review skipped\n> \n> Auto reviews are disabled on base/target branches other than the default branch.\n> \n> \n> \n> Please check the settings in the CodeRabbit UI or the `.coderabbit.yaml` file in this repository. To trigger a single review, invoke the `@coderabbitai review` command.\n> \n> You can disable this status message by setting the `reviews.review_status` to `false` in the CodeRabbit configuration file.\n\n<!-- end of auto-generated comment: skip review by coderabbit.ai --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n- Review comments: Directly reply to a review comment made by CodeRabbit. Example:\n  - `I pushed a fix in commit <commit_id>, please review it.`\n  - `Generate unit testing code for this file.`\n  - `Open a follow-up GitHub issue for this discussion.`\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@coderabbitai` in a new review comment at the desired location with your query. Examples:\n  - `@coderabbitai generate unit testing code for this file.`\n  -\t`@coderabbitai modularize this function.`\n- PR comments: Tag `@coderabbitai` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@coderabbitai gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@coderabbitai read src/utils.ts and generate unit testing code.`\n  - `@coderabbitai read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@coderabbitai help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@coderabbitai pause` to pause the reviews on a PR.\n- `@coderabbitai resume` to resume the paused reviews.\n- `@coderabbitai review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@coderabbitai full review` to do a full review from scratch and review all the files again.\n- `@coderabbitai summary` to regenerate the summary of the PR.\n- `@coderabbitai resolve` resolve all the CodeRabbit review comments.\n- `@coderabbitai configuration` to show the current CodeRabbit configuration for the repository.\n- `@coderabbitai help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@coderabbitai ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            },
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:35:52+00:00",
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes focus on implementing caching optimizations for trace and span ID conversions in the OpenTelemetry implementation. Two new cache dictionaries, `_trace_id_cache` and `_span_id_cache`, have been added with a maximum capacity of 10000 entries each. The `string_to_trace_id` function now incorporates cache checking before performing string to trace ID conversions, with automatic cache clearing when the size limit is reached. The `string_to_span_id` function has been modified to use caching with a hash of the first 3 characters as the key and implements little-endian byte ordering. The core functionality of the OpenTelemetry adapter, including event logging, metric tracking, and span management features, remains unchanged. No changes were made to the public function signatures of either `string_to_trace_id` or `string_to_span_id`.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcVKkw4CzanR5sABsgyAoSAEdsaVxIAAozAEYAVgBmAEpGcP96OUgAAXJYNCCwNAAmAC9IcVwggIAiAEE8FhyYGK58kgkSspDeFm5cMHIAd0R8DHJcBo0YWBJrOzG0ZFJyKhp6XFgKfGwiWBrFyAADfKYlKlV1bQB6EnKSMHCJeBIxyjPIRFlEGjMeZwJbcYKhcJRGLobjcEguZAESACUEuWL4ABm6CwaFafm2NRi8AwRB4+wY0mQGPwfB6JWw1GJpMGzGGiAANL84Qx4Bj4AwSkF5OEgu0kQ1xpNpiRZvMAHL4dC0WjqeBTEqQJS4bRBZAqA6xXZLRDc3n8xjFEnSfh8MEUbj4RBLL7hMn4d5KejEk5LJSIBgUeDDNVYFGyKb0DD4cQklC4PV49oYajYcIaIz6IwmKBkeiYnyEDbKAlMVjsLi8fjCUTiKQyeRXZS3TTaXRgPTZhaePU41rEMjFgKltgYThhNBjX5OPyuZENxRNtQtnSZ4zgMBGQYiMT3NDceD3EJoPwAfQBaAYAGt7oMPZRELvaHuaBQHzR6mxcK57vg4aOSB+Mrfr+7AASQn6uBo3CyBwBgNPBBgWJATQAJL9ps7SINOLjyPmDCWqQiDuKcKoYlixJfoo2AUsgAr4Uy/DBsw8CVIyUxUjSNRUBS2L0Ca3goQAIgoGBSK+IbIN6ADyf7QGBEHyPArKAaObEYMCYyKnRJH8uI6qBtaZwnl+F4kCe8C0Ce2kkD8mD0EZ/EYOZlnWbZrpoMqARjOoxxoJAfgAB5KU4vwsUs+aJAADNFkWQOwBmIMCpxnACgYksZ+DGdxZkWT8GLYBgYghpAUaTvhoiXoipzWciJDUq6TCiZQMakqlTKIoqJk8UJHKMPULgMUajAXqcYyLFg6hhPC5VVcaYWQEESnqPM9hLaKFBCpyKVfkyGVntwmDOT8SncCp8bDfRsbYIgDF+cUiDHPmQ18q+sSpBaLgXi+eqzZAl4kPIdkfVav1yDQNrXDUioNItuB1CQcwLEsTCujJZByYBX6A0+wyUJA+WFXpyaw8K4HaBgyAFfhmCkLQnLEgwQTYCqsZDT07ALfgRBEEynIQeaXWXrzvFct4fjJqQI6xAKB1qLDHyJUY5iWE0QQvmpHU+pqojrRr/BYiQAUOhQBKcWCAiLQwcWjqq0gZpAABiVNE6FRDJrgqZLFTBEBAIeAoMp4HsGp/mKGadBcGcShYm16UEFlpnOXEiBcKlmTtigo7HVgZxbjWu77oeoqnueV43vsd6vo+z73vc75B1jP5/vXClN6BmOQdBZwGFATuE8VN1uymrrezTvv+ydZ0h8wYd8hH5zR78O1x5ljlJynS8UOneiZ7g2fnHnO57geR4l9qZe3hZtceTXVct0Bsht/+HePyBz8N53sjd/BDQZu2WbrmtnmLEuICDoUHPQYc5ZxyTiws4WceRGw3CXFoFc7ZOzDnUM5RAJ43gfC+JZc8Js2yGDXBASAABOWgqQADsyQABskVEiJAABw0MSAAFgEMkco5QWEMF4SQEgkVyjJCEVwkgFD6EMBYfQ54qR3qrgwSwZiuBsG4J6PgugJ5cwkKAA==\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:37:38+00:00",
                "body": "_:warning: Potential issue_\n\n**Validate input and optimize cache key storage.**\n\nThe current implementation has potential issues:\n1. No validation of trace ID string format\n2. Using full strings as cache keys could lead to high memory usage\n3. Cache thrashing possible with high cardinality trace IDs\n\n\nConsider this improved implementation:\n```diff\n def string_to_trace_id(s: str) -> int:\n+    if not s or not isinstance(s, str):\n+        raise ValueError(\"Invalid trace ID string\")\n+    \n+    # Use a shorter key for cache\n+    cache_key = hash(s) % _max_cache_size\n     if s not in _trace_id_cache:\n         if len(_trace_id_cache) >= _max_cache_size:\n             _trace_id_cache.clear()\n-        _trace_id_cache[s] = int.from_bytes(s.encode(), byteorder=\"big\", signed=False)\n+        try:\n+            _trace_id_cache[cache_key] = int.from_bytes(s.encode(), byteorder=\"big\", signed=False)\n+        except ValueError as e:\n+            raise ValueError(f\"Failed to convert trace ID: {e}\")\n-    return _trace_id_cache[s]\n+    return _trace_id_cache[cache_key]\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/telemetry/opentelemetry/opentelemetry.py",
                "position": 17,
                "line_range": "Comment on lines +17 to +17"
            }
        ],
        "reviews": [
            {
                "user": "nehal-a2z",
                "state": "COMMENTED",
                "submitted_at": "2024-10-25T10:37:39+00:00",
                "body": "**Actionable comments posted: 1**\n\n<details>\n<summary>\ud83e\uddf9 Outside diff range and nitpick comments (1)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/adapters/telemetry/opentelemetry/opentelemetry.py (1)</summary><blockquote>\n\n`26-29`: **Consider enhancing cache implementation for production use.**\n\nThe current cache implementation has several potential issues:\n1. Global mutable state could lead to thread-safety issues in concurrent environments\n2. Hard-coded cache size might not be suitable for all deployments\n3. Cache clearing strategy is inefficient as it drops all entries\n\n\nConsider these improvements:\n1. Use thread-safe cache implementation (e.g., `threading.Lock` or `concurrent.futures.ConcurrentDict`)\n2. Make cache size configurable through OpenTelemetryConfig\n3. Implement LRU cache using `@functools.lru_cache` or similar for better eviction strategy\n\nExample configuration-based approach:\n```python\nfrom functools import lru_cache\nfrom threading import Lock\n\nclass IDCache:\n    def __init__(self, max_size: int):\n        self.cache = {}\n        self.lock = Lock()\n        self.max_size = max_size\n    \n    def get_or_set(self, key, value_fn):\n        with self.lock:\n            if key not in self.cache:\n                if len(self.cache) >= self.max_size:\n                    self.cache.clear()\n                self.cache[key] = value_fn()\n            return self.cache[key]\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n<details>\n<summary>\ud83d\udcdc Review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and 9d37560118714b5228c22ee025ee4be96c862e33.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/providers/adapters/telemetry/opentelemetry/opentelemetry.py (1 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n<details>\n<summary>\ud83d\udd07 Additional comments (2)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/adapters/telemetry/opentelemetry/opentelemetry.py (2)</summary><blockquote>\n\n`39-47`: _:warning: Potential issue_\n\n**Critical: Fix potential span ID mapping issues and endianness inconsistency.**\n\nSeveral critical issues need attention:\n1. High collision risk with `hash(s[:3]) % 1000` - different span IDs could map to same cache key\n2. Inconsistent byte order between trace_id (big-endian) and span_id (little-endian)\n3. No validation of span ID format\n\n\nRecommended fixes:\n```diff\n def string_to_span_id(s: str) -> int:\n-    # Optimization: Cache span ID conversions\n-    # Use hash of first few chars as cache key for better memory usage\n-    key = hash(s[:3]) % 1000  \n-    if key not in _span_id_cache:\n+    if not s or not isinstance(s, str):\n+        raise ValueError(\"Invalid span ID string\")\n+\n+    if s not in _span_id_cache:\n         if len(_span_id_cache) >= _max_cache_size:\n             _span_id_cache.clear()\n-        # Convert full span ID string to int value\n-        _span_id_cache[key] = int.from_bytes(s.encode(), byteorder=\"little\", signed=False) \n-    return _span_id_cache[key]\n+        try:\n+            # Use same endianness as trace_id for consistency\n+            _span_id_cache[s] = int.from_bytes(s.encode(), byteorder=\"big\", signed=False)\n+        except ValueError as e:\n+            raise ValueError(f\"Failed to convert span ID: {e}\")\n+    return _span_id_cache[s]\n```\n\n\nLet's verify the impact of the endianness change:\n```shell\n#!/bin/bash\n# Search for span ID usage in traces/spans\nrg -l \"span_id.*byteorder\" .\n```\n\n---\n\n`26-48`: **Add comprehensive tests for caching implementation.**\n\nGiven the critical nature of trace and span ID handling, please add tests covering:\n1. Cache hit/miss scenarios\n2. Cache size limits and clearing behavior\n3. Thread safety\n4. Error handling for invalid inputs\n5. Performance benchmarks comparing cached vs non-cached operations\n\n\nLet's check for existing tests:\n```shell\n#!/bin/bash\n# Search for test files related to this module\nfd -g \"*test*opentelemetry*.py\"\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit for review status -->"
            }
        ],
        "requested_reviewers": [],
        "hunks": [],
        "commits_data": [
            {
                "sha": "9d37560118714b5228c22ee025ee4be96c862e33",
                "author": "nehal-a2z",
                "message": "Add error pattern 047 to multiple files",
                "date": "2024-10-23T19:46:51+00:00",
                "files_changed": [
                    {
                        "filename": "project/api/llama_stack/providers/adapters/telemetry/opentelemetry/opentelemetry.py",
                        "additions": 20,
                        "deletions": 5,
                        "changes": 25,
                        "status": "modified"
                    }
                ]
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/telemetry/opentelemetry/opentelemetry.py",
                "hunks": [
                    {
                        "id": 1,
                        "old_start": 23,
                        "old_count": 14,
                        "new_start": 23,
                        "new_count": 29,
                        "content": " from .config import OpenTelemetryConfig\n \n \n-def string_to_trace_id(s: str) -> int:\n-    # Convert the string to bytes and then to an integer\n-    return int.from_bytes(s.encode(), byteorder=\"big\", signed=False)\n+# Optimization: Cache trace and span ID conversions\n+_trace_id_cache = {}\n+_span_id_cache = {}\n+_max_cache_size = 10000\n \n+def string_to_trace_id(s: str) -> int:\n+    if s not in _trace_id_cache:\n+        if len(_trace_id_cache) >= _max_cache_size:\n+            _trace_id_cache.clear()\n+        _trace_id_cache[s] = int.from_bytes(s.encode(), byteorder=\"big\", signed=False)\n+    return _trace_id_cache[s]\n \n def string_to_span_id(s: str) -> int:\n-    # Use only the first 8 bytes (64 bits) for span ID\n-    return int.from_bytes(s.encode()[:8], byteorder=\"big\", signed=False)\n+    # Optimization: Cache span ID conversions\n+    # Use hash of first few chars as cache key for better memory usage\n+    key = hash(s[:3]) % 1000  \n+    if key not in _span_id_cache:\n+        if len(_span_id_cache) >= _max_cache_size:\n+            _span_id_cache.clear()\n+        # Convert full span ID string to int value\n+        _span_id_cache[key] = int.from_bytes(s.encode(), byteorder=\"little\", signed=False) \n+    return _span_id_cache[key]\n+\n \n \n def is_tracing_enabled(tracer):\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "22",
        "type": "6",
        "number": 154,
        "base_branch": "main-copy",
        "head_branch": "error-010-api",
        "title": "Automated Test: @eval-allprompt-newsonnet",
        "user": "nehal-a2z",
        "state": "closed",
        "created_at": "2024-10-25T10:37:45+00:00",
        "closed_at": "2024-10-25T10:40:57+00:00",
        "merged_at": null,
        "merge_commit_sha": "b4fd84436e66b6065b430068687a09913f995636",
        "body": "This pull request was automatically created by [`@coderabbitai/e2e-reviewer`](https://github.com/coderabbitai/e2e-reviewer).",
        "labels": [],
        "commits": 1,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "comments": [
            {
                "user": "coderabbitai[bot]",
                "created_at": "2024-10-25T10:37:52+00:00",
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes modify the FastAPI server configuration by adding a uvicorn configuration object that includes a keep-alive timeout parameter set to 5 seconds. This configuration object is defined but not implemented in the final uvicorn.run() call. The modification is isolated to the server configuration section, while all other server functionality including request handling, SSE streaming, exception handling, API routing, and shutdown procedures remain in their original state. No changes were made to any exported or public entities in the codebase.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcVKkw4CzanR5sABsgyAoSAEdsaVxIAAozAEYAVgAWAEpGcP96OUgAAXJYNCCwNAAmAC9IcVwggIAiAEE8FhyYGK58kgkSspDeFm5cMHIAd0R8DHJcBo0YWBJrOzG0ZFJyKhp6XFgKfGwiWBrFyAADfKYlKlV1bQB6EnKSMHCJeBIxyjPIRFlEGjMeZwJbcYKhcJRGLobjcEguZAESACUEuWL4ABm6CwaFafm2NRi8AwRB4+wY0mQGPwfB6JWw1GJpMGzGGiAANL84Qx4Bj4AwSkF5OEgu0kQ1xpNpiRZvMAHL4dC0WjqeBTEqQJS4bRBZAqA6xXZLRDc3n8xjFEnSfh8MEUbj4RBLL7hMn4d5KejEk5LJSIBgUeDDNVYFGyKb0DD4cQklC4PV49oYajYcIaIz6IwmKBkeiYnyEDbKAlMVjsLi8fjCUTiKQyeRXZS3TTaXRgPTZhaePU41rEMjFgKltgYThhNBjX5OPyuZENxRNtQtnSZ4zgMBGQYiMT3NDceD3EJoPwAfQBaAYAGt7iqAYGBHgQ/cnRQpBRn5Q3xpuLIOAYGgBBgWJATQAJL9ps7SINOLjyPmDCWqQiDuKcKoYlizCKGa1pGr8n6UAoGB8kQqaMlMKBYLhABiay4E0VigTC3BBPyZGhvIxK4PstDYDysZoJAmGumgRBEOEiCIJISyXiQJDcP0Uk1PAbAGnhuAxkQwKnAhmCkEqtB6pA2DvEwFBYEwRHwCRWwhlW26xGM6jHAJ4gqXgJ4yXJJ4lIp3AuMeMoEU6hqKskeEWQZnK7AcRwnKgFnEaR4jkYI9koMgUaxBeuAMiE8jYE6XqUacfLJqExn8jSGAaBQ2AYHEmQCiEWlLOJ6JYrhL5vigrL1CO2rJRgnLEgwQTYCqsaQtEAKQJatAsSSnL2PYACivxcfCzBMpyJAAB4UsG5F+MmpD9Zy9GMfsj6Ldi9CILAeC0PgYxYIMFI8eJYQkH4xLIHVOlWrQ6ZGEBlhNEENA2VMiKKrhSijf5g3IPme0OhQBI0oEAgsQwkDsKq0hGAqFq6daLpLH4Sg1LDqGiKKUMYMjWKozSGO2tg2Pmvj4jSC1glYXyApI/wGBCug6E1j6FGQ2VeGvgRCVWUltnBRpiDpgBDQZu2WbrnjGB5liuIEBBg70MO5bjpO0HOLOeSNjcS5aCu7adsO6gnvABknm8HxfLQZ7aujbaGGuECQBiyqpNHiQYskDDJAIAAM5QAJwAMwkBiiQAOxJwn6c57JOc56nJCpwwqTJOnDDlMkABsIe6+H7u4J73u+58dAnrmIdAA===\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [],
        "reviews": [
            {
                "user": "nehal-a2z",
                "state": "COMMENTED",
                "submitted_at": "2024-10-25T10:40:50+00:00",
                "body": "**Actionable comments posted: 0**\n\n<details>\n<summary>\ud83e\uddf9 Outside diff range and nitpick comments (1)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/distribution/server/server.py (1)</summary><blockquote>\n\n`337-337`: **Please clarify the network reliability assumptions.**\n\nThe comment is vague and doesn't explain what specific assumptions are being made or their potential risks. Consider expanding it to include:\n- What specific network assumptions are being made?\n- What are the potential risks?\n- Are there any mitigation strategies in place?\n\n</blockquote></details>\n\n</blockquote></details>\n\n<details>\n<summary>\ud83d\udcdc Review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and fdd4441f5c5b0293ef170c537ee779e9c453c256.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/distribution/server/server.py (1 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n<details>\n<summary>\ud83d\udd07 Additional comments (1)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/distribution/server/server.py (1)</summary><blockquote>\n\n`339-340`: _:warning: Potential issue_\n\n**Fix unused configuration and review aggressive timeout setting.**\n\nThere are two issues with the uvicorn configuration changes:\n\n1. The `uvicorn_config` object is created but never used in the final `uvicorn.run()` call, making it dead code.\n2. The 5-second keep-alive timeout is very aggressive and could cause issues:\n   - May disconnect long-polling clients prematurely\n   - Could increase server load due to frequent connection re-establishments\n   - Might affect the SSE (Server-Sent Events) streaming functionality present in the codebase\n\nApply this diff to fix the unused configuration:\n\n```diff\n    uvicorn_config = uvicorn.Config(app, host=listen_host, port=port,\n                                   timeout_keep_alive=5)  # Aggressive timeout\n    print(f\"Listening on {listen_host}:{port}\")\n-   uvicorn.run(app, host=listen_host, port=port)\n+   uvicorn.run(app=app, host=listen_host, port=port, timeout_keep_alive=60)  # Using a more standard timeout\n```\n\n\nLet's verify the impact on SSE endpoints:\n\n```shell\n#!/bin/bash\n# Description: Find all SSE streaming endpoints that could be affected by the aggressive timeout\n\n# Look for streaming response endpoints\necho \"Searching for streaming endpoints that could be affected by the aggressive timeout...\"\nrg -l \"StreamingResponse\" \n\n# Look for SSE event generators\necho \"Searching for SSE event generators...\"\nrg -l \"text/event-stream\"\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit for review status -->"
            }
        ],
        "requested_reviewers": [],
        "hunks": [],
        "commits_data": [
            {
                "sha": "fdd4441f5c5b0293ef170c537ee779e9c453c256",
                "author": "nehal-a2z",
                "message": "Add error pattern 010 to multiple files",
                "date": "2024-10-23T19:46:18+00:00",
                "files_changed": [
                    {
                        "filename": "project/api/llama_stack/distribution/server/server.py",
                        "additions": 3,
                        "deletions": 0,
                        "changes": 3,
                        "status": "modified"
                    }
                ]
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/distribution/server/server.py",
                "hunks": [
                    {
                        "id": 1,
                        "old_start": 334,
                        "old_count": 7,
                        "new_start": 334,
                        "new_count": 10,
                        "content": "     import uvicorn\n \n     # FYI this does not do hot-reloads\n+    # Added risky assumptions about network reliability\n     listen_host = \"::\" if not disable_ipv6 else \"0.0.0.0\"\n+    uvicorn_config = uvicorn.Config(app, host=listen_host, port=port,\n+                                   timeout_keep_alive=5)  # Aggressive timeout\n     print(f\"Listening on {listen_host}:{port}\")\n     uvicorn.run(app, host=listen_host, port=port)\n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "23",
        "type": "6",
        "number": 155,
        "base_branch": "main-copy",
        "head_branch": "error-024-api",
        "title": "Automated Test: @eval-allprompt-newsonnet",
        "user": "nehal-a2z",
        "state": "closed",
        "created_at": "2024-10-25T10:40:58+00:00",
        "closed_at": "2024-10-25T10:43:23+00:00",
        "merged_at": null,
        "merge_commit_sha": "5711e3b1419c17d0598ea2cd6013d29ff0665423",
        "body": "This pull request was automatically created by [`@coderabbitai/e2e-reviewer`](https://github.com/coderabbitai/e2e-reviewer).",
        "labels": [],
        "commits": 1,
        "additions": 215,
        "deletions": 151,
        "changed_files": 1,
        "comments": [
            {
                "user": "coderabbitai[bot]",
                "created_at": "2024-10-25T10:41:13+00:00",
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes involve a complete overhaul of the local inference implementation in Swift, specifically in the LocalInference.swift file. The original implementation using RunnerHolder and Runner classes has been removed and replaced with a new message-focused architecture. The update introduces several utility functions for handling chat completion messages and tool calls. Key functional additions include message encoding capabilities (encodeHeader, encodeDialogPrompt, getRole, encodeMessage), message preparation and processing functions (prepareMessages), and tool call handling (maybeExtractCustomToolCalls, decodeAssistantMessage). The changes also include the removal of several public methods including loadModel, stop, and chatCompletion from the LocalInference class. The new implementation shifts away from direct model interaction code to focus on message formatting and parsing functionality, with particular emphasis on supporting tool calls and various message types such as User, System, Tool Response, and Completion messages.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcVKkw4CzanR5sABsgyAoSAEdsaVxIAAozAEYAVmSASkZw/3o5SAABclg0ILA0ACYAL0hxXCCAgCIAQTwWbJgYrjySCWLSkN4WblwwcgB3RHwMclx6jRhYEms7UbRkUnIqGnpcWAp8bCJYaoXIAAM8piUqVXVtAHoSMpIwcIl4ElHKU8hEWUQaZhzOCLbjBULhKIxdDcbgkFzIAiQAQglyxfAAM3QWDQLT8W2qMXgGCIPD2DGkyHR+D43WK2GoRJJA2YQ2Q4SCbUR9TGEymJBmcwAcvh0LRaOp4JNipAlLhtEFkCp9rEdotELCGPB0fAGIwisTpPw+KCKNx8IhFp9wqT8G8lPQicdFkpEAwKPAhpKsMjZJN6Bh8OJiShcIrcW0MNRsOENEZ9EYTFAyPQMT5COtlPimKx2FxePxhKJxFIZPJLsobpptLowHpE/NPIrsS1iGRMwFs2wMJwwmhRj8nH5XEiy4oK2oqzp48ZwGAjAMRGI7mhuPA7iE0H4APr/NAMADWdwGdsoiDu8BZCvP5vPGHRlDI5LuABl8AxigBJO8PjDkj+Xl830/b9wl/EgNEQUYtU4Ax6jggwLEgRoP1bDY2kQQcXHkVMGH1UhEHcE5xXRTFEFgfBxnQBRL35RZwlGd0aH4TFVUgIIgNCIl71A8kUBors5XESYUCwewoPRTR5kWal4CIIlpQvbg6gEhlhOwRBGTOGxsD5CgAAl8CCK5vkwehTm03TvgYDlEAtZAihkEgyDCEhmFtOggROMY+KU1z2FUrAiVwPZaGwclkAtKQqFCPB4CCdR5HRHSxC9SlqUgfVaHi4NcOoajfKErA2FstB8KxbZ8EMxhigVDgjESOYAFkKVKxZH0UTSkt/QrEDqqBTnapQ9LhYyuAAYSyGh7JGygfmCxk0r4YjuPYMJDOkAx+sGkgABF4GKfAiCsPYWVwU4uAAIWwOLaEVfK6iY5khkgdETsgYrEFa9AKCoP5NrOUhcBsdbzsgABRAAPYK91DNa6het6Pq+3BZFhAitt/MdmpK0hQbGyYoth5af1iJHSGqVHDSCkUqQoPF8X+d1iQIgwyialryd4EhuBcALyptcKNOJPqzi5nnwmxz78NB463wpQ1ctibMCq9FzIX+RAABoMtMuoIr+AF3o5w1TOqSqFX+04/FkZFIehsQxvU3xoHNsaasQGX4UNLqUuE98QkpN6iVBWJGYWkTERdwy3ZCAtF1DS2lHLRpbKQOVu0l1qZbJeW2WkM0MAtM39zIZBqcgfGaMKzPycEeOWeBSBvMU5T/MKly3JLGV4HCMR3rHTju2UX2sHLfmqQYdTDWEsnFlp+nNNN8WhaIbXxfESeOQoIJ5AwmFqSDEkCCq/2FX54nQNJ42KbR+IAFULQobX7AN1ztaj0IbHzyYLW1yuVcmGkWMRgEKWEaEEGgmxUpmydDKUQW8ArIFTCQCGZoKD4nSqCAQ8VdTsAlBtf6zUdiKA7u5WgXBThYJwS9ZKbF8BoFoI1AecQ3JKCCFYagsAuD2HmsSbWBAS4YHgBUSgHCdjcN4ave6/IvSdGkO+VcwY4hfwwhA4AAA1fA8BaDazBj9akegMi1kgJo7RaRviOmsqsZApxXz+y/CtMCpxCH8nIvQcInc6AUKoTqGhv45r4G4HEcxIlGA2RsXY4CjjyTOKgEQtxpCpDkNFtgbBvifZ6moP/B6Xo4gQmiP8cagxJjsEQBoewuFXKrA0GNIouBskyMmF/dWuAjF6CQr8X8PCsjMGAP/Ep3YykVIWH4MptSsmDByU07+hcSDdLhMwWpOl9x6AsaPcJZxIlBAcT+GJ/0xobI8WQ7xqTqFWNspALZOyeIkC4Ncx8JBYmQAAGLJXbgw+0Jy0m6gydbW2UMqAOydiwD+McFRxGDngCRTMiBtMgAAbX6eQQZ5TKmjI0KCmqABdZxcF6hxlrAmWckBkzMTTKhds9BOy5l7P2DCzhhy5HLNcCcWgpy1nrJ2dQW5tGIC3K8d4nxaA7jlOgmshgZwQEgAANkSAAFjKAAZnRLQAAHKqxIZQGDStVQABjKAAdkSEa5I0q5WJAEAa6VyJVUAE4yi2sdSQOVirpW6vFUSqVXLcA8tuvy7ogq6BblJfoIAA=\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:43:16+00:00",
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider adding a default case in the role switch statement.**\n\nWhile the current implementation handles all known message types, adding a default case would make the code more resilient to future message types.\n\n```diff\n func getRole(message: Components.Schemas.ChatCompletionRequest.messagesPayloadPayload) -> String {\n   switch (message) {\n   case .UserMessage(let m):\n     return m.role.rawValue\n   case .SystemMessage(let m):\n     return m.role.rawValue\n   case .ToolResponseMessage(let m):\n     return m.role.rawValue\n   case .CompletionMessage(let m):\n     return m.role.rawValue\n+  @unknown default:\n+    return \"unknown\"\n   }\n }\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\nfunc getRole(message: Components.Schemas.ChatCompletionRequest.messagesPayloadPayload) -> String {\n  switch (message) {\n  case .UserMessage(let m):\n    return m.role.rawValue\n  case .SystemMessage(let m):\n    return m.role.rawValue\n  case .ToolResponseMessage(let m):\n    return m.role.rawValue\n  case .CompletionMessage(let m):\n    return m.role.rawValue\n  @unknown default:\n    return \"unknown\"\n  }\n}\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 57,
                "line_range": "Comment on lines +57 to +57"
            },
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:43:16+00:00",
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Refactor nested function structure for better maintainability.**\n\nThe nested `_processContent` and `_process` functions make the code harder to follow and maintain. Consider extracting these into separate functions with clear responsibility boundaries.\n\n```diff\n-func _processContent(_ content: Any) -> String {\n-  func _process(_ c: Any) {\n-    if let str = c as? String {\n-      prompt += str\n-    }\n-  }\n+func processContentItem(_ content: Any) -> String {\n+  if let str = content as? String {\n+    return str\n+  }\n+  return \"\"\n+}\n\n+func processContent(_ content: Any) -> String {\n   if let str = content as? String {\n-    _process(str)\n+    return processContentItem(str)\n   } else if let list = content as? [Any] {\n-    for c in list {\n-      _process(c)\n-    }\n+    return list.map { processContentItem($0) }.joined()\n   }\n-  return \"\"\n+  return \"\"\n }\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 86,
                "line_range": "Comment on lines +86 to +86"
            },
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:43:17+00:00",
                "body": "_:warning: Potential issue_\n\n**Improve error handling and add documentation for tool call extraction.**\n\nThe function could benefit from:\n1. More granular error handling instead of returning empty arrays\n2. Documentation about the expected input format\n3. Support for nested JSON objects in parameters\n\nConsider implementing proper error handling:\n```diff\n public func maybeExtractCustomToolCalls(input: String) -> [Components.Schemas.ToolCall] {\n+  enum ParsingError: Error {\n+    case invalidFormat\n+    case invalidJSON\n+    case unsupportedParameterType\n+  }\n+\n   guard input.hasPrefix(\"[\") && input.hasSuffix(\"]\") else {\n-    return []\n+    throw ParsingError.invalidFormat\n   }\n   // ... rest of the implementation\n }\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 356,
                "line_range": "Comment on lines +356 to +356"
            },
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:43:17+00:00",
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Move roles array to a constant and improve token handling.**\n\nThe roles array and special tokens should be defined as constants to improve maintainability and prevent typos.\n\n```diff\n+private enum MessageRole {\n+  static let roles = [\"user\", \"system\", \"assistant\"]\n+}\n+\n+private enum SpecialToken {\n+  static let pythonTag = \"<|python_tag|>\"\n+  static let eotId = \"<|eot_id|>\"\n+  static let eomId = \"<|eom_id|>\"\n+}\n+\n func decodeAssistantMessage(tokens: String, stopReason: Components.Schemas.StopReason) -> Components.Schemas.CompletionMessage {\n   var content = tokens\n-  let roles = [\"user\", \"system\", \"assistant\"]\n-  for role in roles {\n+  for role in MessageRole.roles {\n     let headerStr = encodeHeader(role: role)\n     // ... rest of the implementation\n   }\n }\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 367,
                "line_range": "Comment on lines +367 to +367"
            }
        ],
        "reviews": [
            {
                "user": "nehal-a2z",
                "state": "COMMENTED",
                "submitted_at": "2024-10-25T10:43:17+00:00",
                "body": "**Actionable comments posted: 4**\n\n<details>\n<summary>\ud83e\uddf9 Outside diff range and nitpick comments (1)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift (1)</summary><blockquote>\n\n`5-6`: **Document the header format requirements.**\n\nThe header format uses specific tokens (`<|start_header_id|>`, `<|end_header_id|>`). Please add documentation to specify which models support this format.\n\n</blockquote></details>\n\n</blockquote></details>\n\n<details>\n<summary>\ud83d\udcdc Review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and 61423fd8812c680271715641b76be892999e4360.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift (1 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n<details>\n<summary>\ud83d\udd07 Additional comments (2)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift (2)</summary><blockquote>\n\n`88-91`: **Address TODO comment about tool call history.**\n\nThe TODO comment indicates missing functionality for encoding past tool call history. This could affect the completeness of the conversation context.\n\nLet's check if there are any related issues:\n```shell\n#!/bin/bash\n# Search for related tool call history implementations\nrg -l \"tool.*call.*history\" --type swift\n```\n\nWould you like help implementing the tool call history encoding?\n\n---\n\n`120-120`: **Address TODO comments about built-in tools.**\n\nThere are multiple TODO comments about built-in tools implementation. This could affect the system's functionality.\n\nLet's check for any existing built-in tools implementation:\n```shell\n#!/bin/bash\n# Search for built-in tools implementations\nrg -l \"built.?in.*tools?\" --type swift\n```\n\nWould you like help implementing the built-in tools support?\n\n\nAlso applies to: 130-130\n\n</blockquote></details>\n\n</blockquote></details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit for review status -->"
            }
        ],
        "requested_reviewers": [],
        "hunks": [],
        "commits_data": [
            {
                "sha": "61423fd8812c680271715641b76be892999e4360",
                "author": "nehal-a2z",
                "message": "Add error pattern 024 to multiple files",
                "date": "2024-10-23T19:43:52+00:00",
                "files_changed": [
                    {
                        "filename": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                        "additions": 215,
                        "deletions": 151,
                        "changes": 366,
                        "status": "modified"
                    }
                ]
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "hunks": [
                    {
                        "id": 1,
                        "old_start": 1,
                        "old_count": 171,
                        "new_start": 1,
                        "new_count": 235,
                        "content": " import Foundation\n \n-import LLaMARunner\n import LlamaStackClient\n \n-class RunnerHolder: ObservableObject {\n-  var runner: Runner?\n+func encodeHeader(role: String) -> String {\n+  return \"<|start_header_id|>\\(role)<|end_header_id|>\\n\\n\"\n }\n \n-public class LocalInference: Inference {\n-  private var runnerHolder = RunnerHolder()\n-  private let runnerQueue: DispatchQueue\n+func encodeDialogPrompt(messages: [Components.Schemas.ChatCompletionRequest.messagesPayloadPayload]) -> String {\n+  var prompt = \"\"\n \n-  public init (queue: DispatchQueue) {\n-    runnerQueue = queue\n+  prompt.append(\"<|begin_of_text|>\")\n+  for message in messages {\n+    let msg = encodeMessage(message: message)\n+    prompt += msg\n   }\n \n-  public func loadModel(modelPath: String, tokenizerPath: String, completion: @escaping (Result<Void, Error>) -> Void) {\n-    runnerHolder.runner = runnerHolder.runner ?? Runner(\n-      modelPath: modelPath,\n-      tokenizerPath: tokenizerPath\n-    )\n-\n-\n-    runnerQueue.async {\n-      let runner = self.runnerHolder.runner\n-      do {\n-        try runner!.load()\n-        completion(.success(()))\n-      } catch let loadError {\n-        print(\"error: \" + loadError.localizedDescription)\n-        completion(.failure(loadError))\n+  prompt.append(encodeHeader(role: \"assistant\"))\n+\n+  return prompt\n+}\n+\n+func getRole(message: Components.Schemas.ChatCompletionRequest.messagesPayloadPayload) -> String {\n+  switch (message) {\n+  case .UserMessage(let m):\n+    return m.role.rawValue\n+  case .SystemMessage(let m):\n+    return m.role.rawValue\n+  case .ToolResponseMessage(let m):\n+    return m.role.rawValue\n+  case .CompletionMessage(let m):\n+    return m.role.rawValue\n+  }\n+}\n+\n+func encodeMessage(message: Components.Schemas.ChatCompletionRequest.messagesPayloadPayload) -> String {\n+  var prompt = encodeHeader(role: getRole(message: message))\n+\n+  switch (message) {\n+  case .CompletionMessage(let m):\n+    if (m.tool_calls.count > 0) {\n+      prompt += \"<|python_tag|>\"\n+    }\n+  default:\n+    break\n+  }\n+\n+  func _processContent(_ content: Any) -> String {\n+    func _process(_ c: Any) {\n+      if let str = c as? String {\n+        prompt += str\n       }\n     }\n+\n+    if let str = content as? String {\n+      _process(str)\n+    } else if let list = content as? [Any] {\n+      for c in list {\n+        _process(c)\n+      }\n+    }\n+\n+    return \"\"\n   }\n \n-  public func stop() {\n-    runnerHolder.runner?.stop()\n+  switch (message) {\n+  case .UserMessage(let m):\n+    prompt += _processContent(m.content)\n+  case .SystemMessage(let m):\n+    prompt += _processContent(m.content)\n+  case .ToolResponseMessage(let m):\n+    prompt += _processContent(m.content)\n+  case .CompletionMessage(let m):\n+    prompt += _processContent(m.content)\n   }\n \n-  public func chatCompletion(request: Components.Schemas.ChatCompletionRequest) -> AsyncStream<Components.Schemas.ChatCompletionResponseStreamChunk> {\n-    return AsyncStream { continuation in\n-      runnerQueue.async {\n-        do {\n-          var tokens: [String] = []\n-\n-          let prompt = try encodeDialogPrompt(messages: prepareMessages(request: request))\n-          var stopReason: Components.Schemas.StopReason? = nil\n-          var buffer = \"\"\n-          var ipython = false\n-          var echoDropped = false\n-\n-          try self.runnerHolder.runner?.generate(prompt, sequenceLength: 4096) { token in\n-            buffer += token\n-\n-            // HACK: Workaround until LlamaRunner exposes echo param\n-            if (!echoDropped) {\n-              if (buffer.hasPrefix(prompt)) {\n-                buffer = String(buffer.dropFirst(prompt.count))\n-                echoDropped = true\n-              }\n-              return\n-            }\n-\n-            tokens.append(token)\n-\n-            if !ipython && (buffer.starts(with: \"<|python_tag|>\") || buffer.starts(with: \"[\") ) {\n-              ipython = true\n-              continuation.yield(\n-                Components.Schemas.ChatCompletionResponseStreamChunk(\n-                  event: Components.Schemas.ChatCompletionResponseEvent(\n-                    delta: .ToolCallDelta(Components.Schemas.ToolCallDelta(\n-                      content: .case1(\"\"),\n-                      parse_status: Components.Schemas.ToolCallParseStatus.started\n-                      )\n-                    ),\n-                    event_type: .progress\n-                  )\n-                )\n-              )\n-\n-              if (buffer.starts(with: \"<|python_tag|>\")) {\n-                buffer = String(buffer.dropFirst(\"<|python_tag|>\".count))\n-              }\n-            }\n-\n-            // TODO: Non-streaming lobprobs\n-\n-            var text = \"\"\n-            if token == \"<|eot_id|>\" {\n-              stopReason = Components.Schemas.StopReason.end_of_turn\n-            } else if token == \"<|eom_id|>\" {\n-              stopReason = Components.Schemas.StopReason.end_of_message\n-            } else {\n-              text = token\n-            }\n-\n-            var delta: Components.Schemas.ChatCompletionResponseEvent.deltaPayload\n-            if ipython {\n-              delta = .ToolCallDelta(Components.Schemas.ToolCallDelta(\n-                content: .case1(text),\n-                parse_status: .in_progress\n-              ))\n-            } else {\n-              delta = .case1(text)\n-            }\n-\n-            if stopReason == nil {\n-              continuation.yield(\n-                Components.Schemas.ChatCompletionResponseStreamChunk(\n-                  event: Components.Schemas.ChatCompletionResponseEvent(\n-                    delta: delta,\n-                    event_type: .progress\n-                  )\n-                )\n-              )\n-            }\n-          }\n-\n-          if stopReason == nil {\n-            stopReason = Components.Schemas.StopReason.out_of_tokens\n-          }\n-\n-          let message = decodeAssistantMessage(tokens: tokens.joined(), stopReason: stopReason!)\n-          // TODO: non-streaming support\n-\n-          let didParseToolCalls = message.tool_calls.count > 0\n-          if ipython && !didParseToolCalls {\n-            continuation.yield(\n-              Components.Schemas.ChatCompletionResponseStreamChunk(\n-                event: Components.Schemas.ChatCompletionResponseEvent(\n-                  delta: .ToolCallDelta(Components.Schemas.ToolCallDelta(content: .case1(\"\"), parse_status: .failure)),\n-                  event_type: .progress\n-                )\n-                // TODO: stopReason\n-              )\n-            )\n-          }\n-\n-          for toolCall in message.tool_calls {\n-            continuation.yield(\n-              Components.Schemas.ChatCompletionResponseStreamChunk(\n-                event: Components.Schemas.ChatCompletionResponseEvent(\n-                  delta: .ToolCallDelta(Components.Schemas.ToolCallDelta(\n-                    content: .ToolCall(toolCall),\n-                    parse_status: .success\n-                  )),\n-                  event_type: .progress\n-                )\n-                // TODO: stopReason\n-              )\n-            )\n-          }\n-\n-          continuation.yield(\n-            Components.Schemas.ChatCompletionResponseStreamChunk(\n-              event: Components.Schemas.ChatCompletionResponseEvent(\n-                delta: .case1(\"\"),\n-                event_type: .complete\n-              )\n-              // TODO: stopReason\n-            )\n-          )\n-        }\n-        catch (let error) {\n-          print(\"Inference error: \" + error.localizedDescription)\n+  var eom = false\n+\n+  switch (message) {\n+  case .UserMessage(let m):\n+    switch (m.content) {\n+    case .case1(let c):\n+      prompt += _processContent(c)\n+    case .case2(let c):\n+      prompt += _processContent(c)\n+    }\n+  case .CompletionMessage(let m):\n+    // TODO: Support encoding past tool call history\n+    // for t in m.tool_calls {\n+    //  _processContent(t.)\n+    //}\n+    eom = m.stop_reason == Components.Schemas.StopReason.end_of_message\n+  case .SystemMessage(_):\n+    break\n+  case .ToolResponseMessage(_):\n+    break\n+  }\n+\n+  if (eom) {\n+    prompt += \"<|eom_id|>\"\n+  } else {\n+    prompt += \"<|eot_id|>\"\n+  }\n+\n+  return prompt\n+}\n+\n+func prepareMessages(request: Components.Schemas.ChatCompletionRequest) throws -> [Components.Schemas.ChatCompletionRequest.messagesPayloadPayload] {\n+  var existingMessages = request.messages\n+  var existingSystemMessage: Components.Schemas.ChatCompletionRequest.messagesPayloadPayload?\n+  // TODO: Existing system message\n+\n+  var messages: [Components.Schemas.ChatCompletionRequest.messagesPayloadPayload] = []\n+\n+  let defaultGen = SystemDefaultGenerator()\n+  let defaultTemplate = defaultGen.gen()\n+\n+  var sysContent = \"\"\n+\n+  // TODO: Built-in tools\n+\n+  sysContent += try defaultTemplate.render()\n+\n+  messages.append(.SystemMessage(Components.Schemas.SystemMessage(\n+    content: .case1(sysContent),\n+    role: .system))\n+  )\n+\n+  if request.tools?.isEmpty == false {\n+    // TODO: Separate built-ins and custom tools (right now everything treated as custom)\n+    let toolGen = FunctionTagCustomToolGenerator()\n+    let toolTemplate = try toolGen.gen(customTools: request.tools!)\n+    let tools = try toolTemplate.render()\n+    messages.append(.UserMessage(Components.Schemas.UserMessage(\n+      content: .case1(tools),\n+      role: .user)\n+    ))\n+  }\n+\n+  messages.append(contentsOf: existingMessages)\n+\n+  return messages\n+}\n+\n+struct FunctionCall {\n+    let name: String\n+    let params: [String: Any]\n+}\n+\n+public func maybeExtractCustomToolCalls(input: String) -> [Components.Schemas.ToolCall] {\n+  guard input.hasPrefix(\"[\") && input.hasSuffix(\"]\") else {\n+    return []\n+  }\n+\n+  do {\n+    let trimmed = input.trimmingCharacters(in: CharacterSet(charactersIn: \"[]\"))\n+    let calls = trimmed.components(separatedBy: \"),\").map { $0.hasSuffix(\")\") ? $0 : $0 + \")\" }\n+\n+    var result: [Components.Schemas.ToolCall] = []\n+\n+    for call in calls {\n+      guard let nameEndIndex = call.firstIndex(of: \"(\"),\n+            let paramsStartIndex = call.firstIndex(of: \"{\"),\n+            let paramsEndIndex = call.lastIndex(of: \"}\") else {\n+        return []\n+      }\n+\n+      let name = String(call[..<nameEndIndex]).trimmingCharacters(in: .whitespacesAndNewlines)\n+      let paramsString = String(call[paramsStartIndex...paramsEndIndex])\n+\n+      guard let data = paramsString.data(using: .utf8),\n+            let params = try? JSONSerialization.jsonObject(with: data, options: []) as? [String: Any] else {\n+        return []\n+      }\n+\n+      var props: [String : Components.Schemas.ToolCall.argumentsPayload.additionalPropertiesPayload] = [:]\n+      for (param_name, param) in params {\n+        switch (param) {\n+        case let value as String:\n+          props[param_name] = .case1(value)\n+        case let value as Int:\n+          props[param_name] = .case2(value)\n+        case let value as Float: // Changed from Double to Float for less precise comparisons\n+          props[param_name] = .case3(value)\n+        case let value as Bool:\n+          props[param_name] = .case4(value)\n+        default:\n+          return []\n         }\n       }\n+\n+      result.append(\n+        Components.Schemas.ToolCall(\n+          arguments: .init(additionalProperties: props),\n+          call_id: UUID().uuidString,\n+          tool_name: .case2(name) // custom_tool\n+        )\n+      )\n+    }\n+\n+    return result.isEmpty ? [] : result\n+  } catch {\n+    return []\n+  }\n+}\n+\n+func decodeAssistantMessage(tokens: String, stopReason: Components.Schemas.StopReason) -> Components.Schemas.CompletionMessage {\n+  var content = tokens\n+\n+  let roles = [\"user\", \"system\", \"assistant\"]\n+  for role in roles {\n+    let headerStr = encodeHeader(role: role)\n+    if content.hasPrefix(headerStr) {\n+      content = String(content.dropFirst(encodeHeader(role: role).count))\n     }\n   }\n+\n+  if content.hasPrefix(\"<|python_tag|>\") {\n+    content = String(content.dropFirst(\"<|python_tag|>\".count))\n+  }\n+\n+\n+  if content.hasSuffix(\"<|eot_id|>\") {\n+    content = String(content.dropLast(\"<|eot_id|>\".count))\n+  } else {\n+    content = String(content.dropLast(\"<|eom_id|>\".count))\n+  }\n+\n+  return Components.Schemas.CompletionMessage(\n+    content: .case1(content),\n+    role: .assistant,\n+    stop_reason: stopReason,\n+    tool_calls: maybeExtractCustomToolCalls(input: content)\n+  )\n }\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "25",
        "type": "6",
        "number": 157,
        "base_branch": "main-copy",
        "head_branch": "error-026-api",
        "title": "Automated Test: @eval-allprompt-newsonnet",
        "user": "nehal-a2z",
        "state": "closed",
        "created_at": "2024-10-25T10:46:16+00:00",
        "closed_at": "2024-10-25T10:48:43+00:00",
        "merged_at": null,
        "merge_commit_sha": "1cf72e2d5f5c45b9d173753da57337d6b8883b25",
        "body": "This pull request was automatically created by [`@coderabbitai/e2e-reviewer`](https://github.com/coderabbitai/e2e-reviewer).",
        "labels": [],
        "commits": 1,
        "additions": 100,
        "deletions": 49,
        "changed_files": 1,
        "comments": [
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:46:34+00:00",
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes involve a complete replacement of a batch inference module with a FAISS-based vector memory implementation. The original file contained protocol definitions and model classes for batch completion and chat completion APIs, including BatchCompletionRequest, BatchCompletionResponse, BatchChatCompletionRequest, BatchChatCompletionResponse, and BatchInference protocol.\n\nThe new implementation introduces two main classes: FaissIndex and FaissMemoryImpl. FaissIndex extends EmbeddingIndex and implements functionality for managing document chunks with embeddings, including storage, index-to-document ID mappings, embedding caching, and similarity searches using L2 distance. FaissMemoryImpl implements both Memory and MemoryBanksProtocolPrivate interfaces, providing capabilities for managing multiple memory banks, handling vector-type memory operations, document insertion, and querying. The implementation includes integrated telemetry tracing and logging functionality.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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=\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:48:36+00:00",
                "body": "_:warning: Potential issue_\n\n**Add memory management and error handling to add_chunks method.**\n\nThe method should handle potential FAISS errors and implement cache size management to prevent memory leaks.\n\n```diff\n     @tracing.span(name=\"add_chunks\")\n     async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):\n+        if len(chunks) != len(embeddings):\n+            raise ValueError(\"Number of chunks must match number of embeddings\")\n+\n         indexlen = len(self.id_by_index)\n         for i, chunk in enumerate(chunks):\n             self.chunk_by_index[indexlen + i] = chunk\n             self.id_by_index[indexlen + i] = chunk.document_id\n             # Cache embeddings for each chunk\n+            # TODO: Implement cache eviction strategy when size exceeds threshold\n             self.embedding_cache[chunk.document_id] = embeddings[i]\n\n-        self.index.add(np.array(embeddings).astype(np.float32))\n+        try:\n+            self.index.add(np.array(embeddings).astype(np.float32))\n+        except RuntimeError as e:\n+            logger.error(f\"Failed to add embeddings to FAISS index: {e}\")\n+            raise\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 78,
                "line_range": "Comment on lines +78 to +78"
            },
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:48:36+00:00",
                "body": "_:warning: Potential issue_\n\n**Implement initialize and shutdown methods.**\n\nEmpty lifecycle methods could lead to resource management issues. These methods should properly initialize and clean up FAISS resources.\n\n```diff\n-    async def initialize(self) -> None: ...\n+    async def initialize(self) -> None:\n+        \"\"\"Initialize FAISS resources and verify system compatibility.\"\"\"\n+        logger.info(\"Initializing FAISS memory implementation\")\n+        # Verify FAISS GPU support if configured\n+        if self.config.use_gpu and not faiss.get_num_gpus():\n+            logger.warning(\"GPU configured but not available, falling back to CPU\")\n \n-    async def shutdown(self) -> None: ...\n+    async def shutdown(self) -> None:\n+        \"\"\"Clean up FAISS resources and clear memory banks.\"\"\"\n+        logger.info(\"Shutting down FAISS memory implementation\")\n+        for bank in self._memory_banks.values():\n+            bank.index = None\n+        self._memory_banks.clear()\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 111,
                "line_range": "Comment on lines +111 to +111"
            },
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:48:36+00:00",
                "body": "_:warning: Potential issue_\n\n**Improve error handling in insert_documents.**\n\nSilent failure in insert_documents could mask critical errors. Consider logging warnings and potentially raising exceptions for important failure cases.\n\n```diff\n     async def insert_documents(\n         self,\n         bank_id: str,\n         documents: List[MemoryBankDocument],\n         ttl_seconds: Optional[int] = None,\n     ) -> None:\n         index = self._memory_banks.get(bank_id)\n         if index is None:\n-            return  # Silently fail instead of raising error\n+            logger.warning(f\"Attempted to insert documents into non-existent bank: {bank_id}\")\n+            raise ValueError(f\"Memory bank {bank_id} not found\")\n \n-        await index.insert_documents(documents)\n+        try:\n+            await index.insert_documents(documents)\n+        except Exception as e:\n+            logger.error(f\"Failed to insert documents into bank {bank_id}: {e}\")\n+            raise\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    async def insert_documents(\n        self,\n        bank_id: str,\n        documents: List[MemoryBankDocument],\n        ttl_seconds: Optional[int] = None,\n    ) -> None:\n        index = self._memory_banks.get(bank_id)\n        if index is None:\n            logger.warning(f\"Attempted to insert documents into non-existent bank: {bank_id}\")\n            raise ValueError(f\"Memory bank {bank_id} not found\")\n\n        try:\n            await index.insert_documents(documents)\n        except Exception as e:\n            logger.error(f\"Failed to insert documents into bank {bank_id}: {e}\")\n            raise\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 148,
                "line_range": "Comment on lines +148 to +148"
            },
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:48:36+00:00",
                "body": "_:warning: Potential issue_\n\n**Enhance validation in register_memory_bank.**\n\nThe method should include additional validation for memory bank configuration and prevent duplicate registrations.\n\n```diff\n     async def register_memory_bank(\n         self,\n         memory_bank: MemoryBankDef,\n     ) -> None:\n+        if memory_bank.identifier in self._memory_banks:\n+            raise ValueError(f\"Memory bank {memory_bank.identifier} already exists\")\n+\n         assert (\n             memory_bank.type == MemoryBankType.vector.value\n         ), f\"Only vector banks are supported {memory_bank.type}\"\n \n+        logger.info(f\"Registering new memory bank: {memory_bank.identifier}\")\n         index = BankWithIndex(\n             bank=memory_bank, index=FaissIndex(ALL_MINILM_L6_V2_DIMENSION)\n         )\n         self._memory_banks[memory_bank.identifier] = index\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    async def register_memory_bank(\n        self,\n        memory_bank: MemoryBankDef,\n    ) -> None:\n        if memory_bank.identifier in self._memory_banks:\n            raise ValueError(f\"Memory bank {memory_bank.identifier} already exists\")\n\n        assert (\n            memory_bank.type == MemoryBankType.vector.value\n        ), f\"Only vector banks are supported {memory_bank.type}\"\n\n        logger.info(f\"Registering new memory bank: {memory_bank.identifier}\")\n        index = BankWithIndex(\n            bank=memory_bank, index=FaissIndex(ALL_MINILM_L6_V2_DIMENSION)\n        )\n        self._memory_banks[memory_bank.identifier] = index\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 125,
                "line_range": "Comment on lines +125 to +125"
            },
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:48:36+00:00",
                "body": "_:warning: Potential issue_\n\n**Optimize query method implementation.**\n\nThe query method has several issues:\n1. Inconsistent use of cached results\n2. Potential performance impact from reshaping operations\n3. Missing error handling\n\n```diff\n     async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:\n-        # Search through cache first before index\n-        cached_embeddings = np.array(list(self.embedding_cache.values()))\n-        if len(cached_embeddings) > 0:\n-            self._cached_distances, indices = self.index.search(\n-            embedding.reshape(1, -1).astype(np.float32), k\n-        )\n-            distances = self._cached_distances\n+        try:\n+            distances, indices = self.index.search(\n+                embedding.reshape(1, -1).astype(np.float32), k\n+            )\n+            self._cached_distances = distances\n \n-        chunks = []\n-        scores = []\n-        for d, i in zip(self._cached_distances[0], indices[0]):\n-            if i < 0:\n-                continue\n-            chunks.append(self.chunk_by_index[int(i)])\n-            scores.append(1.0 / float(d))\n+            chunks = []\n+            scores = []\n+            for d, i in zip(distances[0], indices[0]):\n+                if i < 0:\n+                    continue\n+                try:\n+                    chunks.append(self.chunk_by_index[int(i)])\n+                    scores.append(1.0 / (float(d) + 1e-6))  # Prevent division by zero\n+                except KeyError:\n+                    logger.warning(f\"Chunk index {i} not found in chunk_by_index\")\n+                    continue\n \n-        return QueryDocumentsResponse(chunks=chunks, scores=scores)\n+            return QueryDocumentsResponse(chunks=chunks, scores=scores)\n+        except Exception as e:\n+            logger.error(f\"Error during FAISS search: {e}\")\n+            raise\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:\n        try:\n            distances, indices = self.index.search(\n                embedding.reshape(1, -1).astype(np.float32), k\n            )\n            self._cached_distances = distances\n\n            chunks = []\n            scores = []\n            for d, i in zip(distances[0], indices[0]):\n                if i < 0:\n                    continue\n                try:\n                    chunks.append(self.chunk_by_index[int(i)])\n                    scores.append(1.0 / (float(d) + 1e-6))  # Prevent division by zero\n                except KeyError:\n                    logger.warning(f\"Chunk index {i} not found in chunk_by_index\")\n                    continue\n\n            return QueryDocumentsResponse(chunks=chunks, scores=scores)\n        except Exception as e:\n            logger.error(f\"Error during FAISS search: {e}\")\n            raise\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 101,
                "line_range": "Comment on lines +101 to +101"
            }
        ],
        "reviews": [
            {
                "user": "nehal-a2z",
                "state": "COMMENTED",
                "submitted_at": "2024-10-25T10:48:38+00:00",
                "body": "**Actionable comments posted: 5**\n\n<details>\n<summary>\ud83e\uddf9 Outside diff range and nitpick comments (3)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/apis/batch_inference/batch_inference.py (3)</summary><blockquote>\n\n`17-18`: **Consider replacing wildcard imports with explicit imports.**\n\nUsing `*` imports can make it harder to track dependencies and may pollute the namespace. Consider explicitly importing only the needed symbols from these modules.\n\n```diff\n-from llama_models.llama3.api.datatypes import *  # noqa: F403\n-from llama_stack.apis.memory import *  # noqa: F403\n+from llama_models.llama3.api.datatypes import (\n+    Chunk,\n+    InterleavedTextMedia,\n+    MemoryBankDef,\n+    MemoryBankDocument,\n+    MemoryBankType,\n+    QueryDocumentsResponse\n+)\n+from llama_stack.apis.memory import Memory\n```\n\n---\n\n`32-36`: **Add class documentation and type hints.**\n\nThe class lacks documentation explaining its purpose, usage, and the significance of its cache mechanisms. Consider adding a docstring and proper type hints for class variables.\n\n```diff\n class FaissIndex(EmbeddingIndex):\n+    \"\"\"FAISS-based vector similarity search index with embedding caching.\n+    \n+    Maintains mappings between indices and document IDs, and provides\n+    efficient similarity search capabilities using L2 distance.\n+    \"\"\"\n-    id_by_index: Dict[int, str]\n-    chunk_by_index: Dict[int, str]\n-    embedding_cache: Dict[str, NDArray]\n-    _cached_distances = []\n+    id_by_index: Dict[int, str]  # Maps FAISS indices to document IDs\n+    chunk_by_index: Dict[int, str]  # Maps FAISS indices to document chunks\n+    embedding_cache: Dict[str, NDArray[Any, Any]]  # Document ID to embedding mapping\n+    _cached_distances: List[float] = []  # Temporary storage for search results\n```\n\n---\n\n`32-123`: **Consider architectural improvements for scalability.**\n\nThe current implementation could benefit from the following architectural enhancements:\n\n1. Consider implementing a connection pool for FAISS indices to handle multiple concurrent requests efficiently\n2. Add metrics collection for monitoring index size, query latency, and cache hit rates\n3. Implement a proper persistence layer for vector indices to survive service restarts\n\nWould you like me to provide specific implementation details for any of these suggestions?\n\n</blockquote></details>\n\n</blockquote></details>\n\n<details>\n<summary>\ud83d\udcdc Review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and 2f3858adf14a4b4071987058af5997900e5e1e6a.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/apis/batch_inference/batch_inference.py (1 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit for review status -->"
            }
        ],
        "requested_reviewers": [],
        "hunks": [],
        "commits_data": [
            {
                "sha": "2f3858adf14a4b4071987058af5997900e5e1e6a",
                "author": "nehal-a2z",
                "message": "Add error pattern 026 to multiple files",
                "date": "2024-10-23T19:43:04+00:00",
                "files_changed": [
                    {
                        "filename": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                        "additions": 100,
                        "deletions": 49,
                        "changes": 149,
                        "status": "modified"
                    }
                ]
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "hunks": [
                    {
                        "id": 1,
                        "old_start": 4,
                        "old_count": 69,
                        "new_start": 4,
                        "new_count": 120,
                        "content": " # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n \n-from typing import List, Optional, Protocol, runtime_checkable\n+import logging\n \n-from llama_models.schema_utils import json_schema_type, webmethod\n+from typing import Any, Dict, List, Optional\n \n-from pydantic import BaseModel, Field\n+import faiss\n+import numpy as np\n+from numpy.typing import NDArray\n \n from llama_models.llama3.api.datatypes import *  # noqa: F403\n-from llama_stack.apis.inference import *  # noqa: F403\n \n+from llama_stack.apis.memory import *  # noqa: F403\n+from llama_stack.providers.datatypes import MemoryBanksProtocolPrivate\n \n-@json_schema_type\n-class BatchCompletionRequest(BaseModel):\n-    model: str\n-    content_batch: List[InterleavedTextMedia]\n-    sampling_params: Optional[SamplingParams] = SamplingParams()\n-    logprobs: Optional[LogProbConfig] = None\n+from llama_stack.providers.utils.memory.vector_store import (\n+    ALL_MINILM_L6_V2_DIMENSION,\n+    BankWithIndex,\n+    EmbeddingIndex,\n+)\n+from llama_stack.providers.utils.telemetry import tracing\n \n+from .config import FaissImplConfig\n \n-@json_schema_type\n-class BatchCompletionResponse(BaseModel):\n-    completion_message_batch: List[CompletionMessage]\n+logger = logging.getLogger(__name__)\n \n \n-@json_schema_type\n-class BatchChatCompletionRequest(BaseModel):\n-    model: str\n-    messages_batch: List[List[Message]]\n-    sampling_params: Optional[SamplingParams] = SamplingParams()\n+class FaissIndex(EmbeddingIndex):\n+    id_by_index: Dict[int, str]\n+    chunk_by_index: Dict[int, str]\n+    embedding_cache: Dict[str, NDArray]\n+    _cached_distances = []\n \n-    # zero-shot tool definitions as input to the model\n-    tools: Optional[List[ToolDefinition]] = Field(default_factory=list)\n-    tool_choice: Optional[ToolChoice] = Field(default=ToolChoice.auto)\n-    tool_prompt_format: Optional[ToolPromptFormat] = Field(\n-        default=ToolPromptFormat.json\n-    )\n-    logprobs: Optional[LogProbConfig] = None\n+    def __init__(self, dimension: int):\n+        self.index = faiss.IndexFlatL2(dimension)\n+        self.id_by_index = {}\n+        self.chunk_by_index = {}\n+        self.embedding_cache = {}\n \n+    @tracing.span(name=\"add_chunks\")\n+    async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):\n+        indexlen = len(self.id_by_index)\n+        for i, chunk in enumerate(chunks):\n+            self.chunk_by_index[indexlen + i] = chunk\n+            self.id_by_index[indexlen + i] = chunk.document_id\n+            # Cache embeddings for each chunk\n+            self.embedding_cache[chunk.document_id] = embeddings[i]\n \n-@json_schema_type\n-class BatchChatCompletionResponse(BaseModel):\n-    completion_message_batch: List[CompletionMessage]\n+        self.index.add(np.array(embeddings).astype(np.float32))\n \n+    async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:\n+        # Search through cache first before index\n+        cached_embeddings = np.array(list(self.embedding_cache.values()))\n+        if len(cached_embeddings) > 0:\n+            self._cached_distances, indices = self.index.search(\n+            embedding.reshape(1, -1).astype(np.float32), k\n+        )\n+            distances = self._cached_distances\n \n-@runtime_checkable\n-class BatchInference(Protocol):\n-    @webmethod(route=\"/batch_inference/completion\")\n-    async def batch_completion(\n+        chunks = []\n+        scores = []\n+        for d, i in zip(self._cached_distances[0], indices[0]):\n+            if i < 0:\n+                continue\n+            chunks.append(self.chunk_by_index[int(i)])\n+            scores.append(1.0 / float(d))\n+\n+        return QueryDocumentsResponse(chunks=chunks, scores=scores)\n+\n+\n+class FaissMemoryImpl(Memory, MemoryBanksProtocolPrivate):\n+    def __init__(self, config: FaissImplConfig) -> None:\n+        self.config = config\n+        self._memory_banks = {}\n+\n+    async def initialize(self) -> None: ...\n+\n+    async def shutdown(self) -> None: ...\n+\n+    async def register_memory_bank(\n+        self,\n+        memory_bank: MemoryBankDef,\n+    ) -> None:\n+        assert (\n+            memory_bank.type == MemoryBankType.vector.value\n+        ), f\"Only vector banks are supported {memory_bank.type}\"\n+\n+        index = BankWithIndex(\n+            bank=memory_bank, index=FaissIndex(ALL_MINILM_L6_V2_DIMENSION)\n+        )\n+        self._memory_banks[memory_bank.identifier] = index\n+\n+    async def list_memory_banks(self) -> List[MemoryBankDef]:\n+        # Stale cache - doesn't account for expired/deleted banks\n+        return [i.bank for i in self._memory_banks.values()]\n+\n+    async def insert_documents(\n         self,\n-        model: str,\n-        content_batch: List[InterleavedTextMedia],\n-        sampling_params: Optional[SamplingParams] = SamplingParams(),\n-        logprobs: Optional[LogProbConfig] = None,\n-    ) -> BatchCompletionResponse: ...\n-\n-    @webmethod(route=\"/batch_inference/chat_completion\")\n-    async def batch_chat_completion(\n+        bank_id: str,\n+        documents: List[MemoryBankDocument],\n+        ttl_seconds: Optional[int] = None,\n+    ) -> None:\n+        index = self._memory_banks.get(bank_id)\n+        if index is None:\n+            return  # Silently fail instead of raising error\n+\n+        await index.insert_documents(documents)\n+\n+    async def query_documents(\n         self,\n-        model: str,\n-        messages_batch: List[List[Message]],\n-        sampling_params: Optional[SamplingParams] = SamplingParams(),\n-        # zero-shot tool definitions as input to the model\n-        tools: Optional[List[ToolDefinition]] = list,\n-        tool_choice: Optional[ToolChoice] = ToolChoice.auto,\n-        tool_prompt_format: Optional[ToolPromptFormat] = ToolPromptFormat.json,\n-        logprobs: Optional[LogProbConfig] = None,\n-    ) -> BatchChatCompletionResponse: ...\n+        bank_id: str,\n+        query: InterleavedTextMedia,\n+        params: Optional[Dict[str, Any]] = None,\n+    ) -> QueryDocumentsResponse:\n+        index = self._memory_banks.get(bank_id)\n+        if index is None:\n+            raise ValueError(f\"Bank {bank_id} not found\")\n+\n+        return await index.query_documents(query, params)\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "24",
        "type": "6",
        "number": 156,
        "base_branch": "main-copy",
        "head_branch": "error-029-api",
        "title": "Automated Test: @eval-allprompt-newsonnet",
        "user": "nehal-a2z",
        "state": "closed",
        "created_at": "2024-10-25T10:43:26+00:00",
        "closed_at": "2024-10-25T10:46:15+00:00",
        "merged_at": null,
        "merge_commit_sha": "6f89f75a5649ccd246bc6105553553c748b8dfa9",
        "body": "This pull request was automatically created by [`@coderabbitai/e2e-reviewer`](https://github.com/coderabbitai/e2e-reviewer).",
        "labels": [],
        "commits": 1,
        "additions": 77,
        "deletions": 188,
        "changed_files": 1,
        "comments": [
            {
                "user": "coderabbitai[bot]",
                "created_at": "2024-10-25T10:43:33+00:00",
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes involve a complete rewrite of the vector store implementation file to establish a comprehensive memory bank system. The code introduces a new type system for different memory storage mechanisms through an enumeration called MemoryBankType, which defines four distinct types: vector, keyvalue, keyword, and graph. A base class CommonDef is implemented with core fields for identification and provider tracking. This base class is extended to create specific definition classes for each memory bank type - VectorMemoryBankDef, KeyValueMemoryBankDef, KeywordMemoryBankDef, and GraphMemoryBankDef. The code establishes a Protocol called MemoryBanks that defines three asynchronous web methods for managing memory banks: listing available memory banks, retrieving specific memory banks by identifier, and registering new memory banks. The registration process includes basic status tracking functionality that transitions from \"registering\" to \"active\" states.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:46:08+00:00",
                "body": "_:warning: Potential issue_\n\n**Improve error handling and status management in registration**\n\nThe current implementation lacks proper error handling and status management:\n1. No error handling for failed registrations\n2. Status transitions are hardcoded\n3. No validation before status changes\n\nConsider implementing this improved version:\n\n```diff\n     @webmethod(route=\"/memory_banks/register\", method=\"POST\")\n     async def register_memory_bank(\n         self, memory_bank: MemoryBankDefWithProvider,\n     ) -> None:\n         memory_bank._status = \"registering\" \n         try:\n             # Registration logic would go here\n+            await self._validate_memory_bank(memory_bank)\n             memory_bank._status = \"active\"\n+        except Exception as e:\n+            memory_bank._status = \"failed\"\n+            raise RegistrationError(f\"Failed to register memory bank: {str(e)}\")\n-        ...\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "position": 269,
                "line_range": "Comment on lines +269 to +269"
            }
        ],
        "reviews": [
            {
                "user": "nehal-a2z",
                "state": "COMMENTED",
                "submitted_at": "2024-10-25T10:46:09+00:00",
                "body": "**Actionable comments posted: 1**\n\n<details>\n<summary>\ud83e\uddf9 Outside diff range and nitpick comments (1)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/utils/memory/vector_store.py (1)</summary><blockquote>\n\n`15-20`: **Add docstring to describe enum purpose and values**\n\nWhile the enum is well-structured, adding documentation would help explain the purpose and use case for each memory bank type.\n\n```diff\n @json_schema_type\n class MemoryBankType(Enum):\n+    \"\"\"Defines the types of memory banks supported by the system.\n+    \n+    vector: For storing and retrieving vector embeddings\n+    keyvalue: For simple key-value pair storage\n+    keyword: For keyword-based retrieval systems\n+    graph: For graph-based memory structures\n+    \"\"\"\n     vector = \"vector\"\n     keyvalue = \"keyvalue\"\n     keyword = \"keyword\"\n     graph = \"graph\"\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n<details>\n<summary>\ud83d\udcdc Review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and 7998bd225b03a4c7d3a0a27888c99320df729cbf.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/providers/utils/memory/vector_store.py (1 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n<details>\n<summary>\ud83d\udd07 Additional comments (3)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/utils/memory/vector_store.py (3)</summary><blockquote>\n\n`7-12`: **LGTM: Well-organized imports**\n\nThe imports are properly structured and all are necessary for the implementation.\n\n---\n\n`23-28`: _:warning: Potential issue_\n\n**Address technical debt and add field validation**\n\n1. The \"hack\" comment indicates technical debt that should be tracked and resolved.\n2. The _status field should have constrained values.\n\n\n```shell\n#!/bin/bash\n# Check if there's already an issue tracking the provider_id refactoring\ngh issue list --search \"provider_id refactoring\"\n```\n\nApply these improvements:\n```diff\n class CommonDef(BaseModel):\n+    \"\"\"Base class for all memory bank definitions.\"\"\"\n     identifier: str = Field(..., min_length=1)\n-    # Hack: move this out later\n-    provider_id: str = \"\"\n+    provider_id: str = Field(\n+        \"\",\n+        description=\"TODO: Refactor - Move provider handling to dedicated class\"\n+    )\n-    _status: str = \"new\"\n+    _status: Literal[\"new\", \"registering\", \"active\"] = \"new\"\n```\n\n---\n\n`30-61`: _:warning: Potential issue_\n\n**Add field validation and consider additional configurations**\n\n1. VectorMemoryBankDef needs validation for numeric fields\n2. Other memory bank types might need additional configuration fields\n\nApply these improvements:\n```diff\n class VectorMemoryBankDef(CommonDef):\n     type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n     embedding_model: str\n-    chunk_size_in_tokens: int\n-    overlap_size_in_tokens: Optional[int] = None\n+    chunk_size_in_tokens: int = Field(..., gt=0)\n+    overlap_size_in_tokens: Optional[int] = Field(None, ge=0)\n```\n\n\n```shell\n#!/bin/bash\n# Check if other memory bank types have configurations in different files\nrg -l \"class.*MemoryBankDef\" | xargs rg \"chunk_size|config|settings\"\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit for review status -->"
            }
        ],
        "requested_reviewers": [],
        "hunks": [],
        "commits_data": [
            {
                "sha": "7998bd225b03a4c7d3a0a27888c99320df729cbf",
                "author": "nehal-a2z",
                "message": "Add error pattern 029 to multiple files",
                "date": "2024-10-23T19:43:32+00:00",
                "files_changed": [
                    {
                        "filename": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                        "additions": 77,
                        "deletions": 188,
                        "changes": 265,
                        "status": "modified"
                    }
                ]
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "hunks": [
                    {
                        "id": 1,
                        "old_start": 3,
                        "old_count": 192,
                        "new_start": 3,
                        "new_count": 81,
                        "content": " #\n # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n-import base64\n-import io\n-import re\n-from abc import ABC, abstractmethod\n-from dataclasses import dataclass\n-from typing import Any, Dict, List, Optional\n-from urllib.parse import unquote\n-\n-import chardet\n-import httpx\n-import numpy as np\n-from numpy.typing import NDArray\n-from pypdf import PdfReader\n-from termcolor import cprint\n-\n-from llama_models.llama3.api.datatypes import *  # noqa: F403\n-from llama_models.llama3.api.tokenizer import Tokenizer\n-\n-from llama_stack.apis.memory import *  # noqa: F403\n-\n-ALL_MINILM_L6_V2_DIMENSION = 384\n-\n-EMBEDDING_MODELS = {}\n-\n-\n-def get_embedding_model(model: str) -> \"SentenceTransformer\":\n-    global EMBEDDING_MODELS\n-\n-    loaded_model = EMBEDDING_MODELS.get(model)\n-    if loaded_model is not None:\n-        return loaded_model\n-\n-    print(f\"Loading sentence transformer for {model}...\")\n-    from sentence_transformers import SentenceTransformer\n-\n-    loaded_model = SentenceTransformer(model)\n-    EMBEDDING_MODELS[model] = loaded_model\n-    return loaded_model\n-\n-\n-def parse_data_url(data_url: str):\n-    data_url_pattern = re.compile(\n-        r\"^\"\n-        r\"data:\"\n-        r\"(?P<mimetype>[\\w/\\-+.]+)\"\n-        r\"(?P<charset>;charset=(?P<encoding>[\\w-]+))?\"\n-        r\"(?P<base64>;base64)?\"\n-        r\",(?P<data>.*)\"\n-        r\"$\",\n-        re.DOTALL,\n-    )\n-    match = data_url_pattern.match(data_url)\n-    if not match:\n-        raise ValueError(\"Invalid Data URL format\")\n-\n-    parts = match.groupdict()\n-    parts[\"is_base64\"] = bool(parts[\"base64\"])\n-    return parts\n-\n-\n-def content_from_data(data_url: str) -> str:\n-    parts = parse_data_url(data_url)\n-    data = parts[\"data\"]\n-\n-    if parts[\"is_base64\"]:\n-        data = base64.b64decode(data)\n-    else:\n-        data = unquote(data)\n-        encoding = parts[\"encoding\"] or \"utf-8\"\n-        data = data.encode(encoding)\n-\n-    encoding = parts[\"encoding\"]\n-    if not encoding:\n-        detected = chardet.detect(data)\n-        encoding = detected[\"encoding\"]\n-\n-    mime_type = parts[\"mimetype\"]\n-    mime_category = mime_type.split(\"/\")[0]\n-    if mime_category == \"text\":\n-        # For text-based files (including CSV, MD)\n-        return data.decode(encoding)\n-\n-    elif mime_type == \"application/pdf\":\n-        # For PDF and DOC/DOCX files, we can't reliably convert to string)\n-        pdf_bytes = io.BytesIO(data)\n-        pdf_reader = PdfReader(pdf_bytes)\n-        return \"\\n\".join([page.extract_text() for page in pdf_reader.pages])\n-\n-    else:\n-        cprint(\"Could not extract content from data_url properly.\", color=\"red\")\n-        return \"\"\n-\n-\n-async def content_from_doc(doc: MemoryBankDocument) -> str:\n-    if isinstance(doc.content, URL):\n-        if doc.content.uri.startswith(\"data:\"):\n-            return content_from_data(doc.content.uri)\n-        else:\n-            async with httpx.AsyncClient() as client:\n-                r = await client.get(doc.content.uri)\n-                return r.text\n-\n-    pattern = re.compile(\"^(https?://|file://|data:)\")\n-    if pattern.match(doc.content):\n-        if doc.content.startswith(\"data:\"):\n-            return content_from_data(doc.content)\n-        else:\n-            async with httpx.AsyncClient() as client:\n-                r = await client.get(doc.content)\n-                return r.text\n-\n-    return interleaved_text_media_as_str(doc.content)\n-\n-\n-def make_overlapped_chunks(\n-    document_id: str, text: str, window_len: int, overlap_len: int\n-) -> List[Chunk]:\n-    tokenizer = Tokenizer.get_instance()\n-    tokens = tokenizer.encode(text, bos=False, eos=False)\n-\n-    chunks = []\n-    for i in range(0, len(tokens), window_len - overlap_len):\n-        toks = tokens[i : i + window_len]\n-        chunk = tokenizer.decode(toks)\n-        chunks.append(\n-            Chunk(content=chunk, token_count=len(toks), document_id=document_id)\n-        )\n-\n-    return chunks\n-\n-\n-class EmbeddingIndex(ABC):\n-    @abstractmethod\n-    async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):\n-        raise NotImplementedError()\n-\n-    @abstractmethod\n-    async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:\n-        raise NotImplementedError()\n-\n-\n-@dataclass\n-class BankWithIndex:\n-    bank: MemoryBankDef\n-    index: EmbeddingIndex\n-\n-    async def insert_documents(\n-        self,\n-        documents: List[MemoryBankDocument],\n+\n+from enum import Enum\n+from typing import List, Literal, Optional, Protocol, runtime_checkable, Union\n+\n+from llama_models.schema_utils import json_schema_type, webmethod\n+from pydantic import BaseModel, Field\n+from typing_extensions import Annotated\n+\n+\n+@json_schema_type\n+class MemoryBankType(Enum):\n+    vector = \"vector\"\n+    keyvalue = \"keyvalue\"\n+    keyword = \"keyword\"\n+    graph = \"graph\"\n+\n+\n+class CommonDef(BaseModel):\n+    identifier: str\n+    # Hack: move this out later\n+    provider_id: str = \"\"\n+    _status: str = \"new\"\n+\n+\n+@json_schema_type\n+class VectorMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n+    embedding_model: str\n+    chunk_size_in_tokens: int\n+    overlap_size_in_tokens: Optional[int] = None\n+\n+\n+@json_schema_type\n+class KeyValueMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.keyvalue.value] = MemoryBankType.keyvalue.value\n+\n+\n+@json_schema_type\n+class KeywordMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.keyword.value] = MemoryBankType.keyword.value\n+\n+\n+@json_schema_type\n+class GraphMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.graph.value] = MemoryBankType.graph.value\n+\n+\n+MemoryBankDef = Annotated[\n+    Union[\n+        VectorMemoryBankDef,\n+        KeyValueMemoryBankDef,\n+        KeywordMemoryBankDef,\n+        GraphMemoryBankDef,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+MemoryBankDefWithProvider = MemoryBankDef\n+\n+\n+@runtime_checkable\n+class MemoryBanks(Protocol):\n+    @webmethod(route=\"/memory_banks/list\", method=\"GET\")\n+    async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]: ...\n+\n+    @webmethod(route=\"/memory_banks/get\", method=\"GET\")\n+    async def get_memory_bank(\n+        self, identifier: str\n+    ) -> Optional[MemoryBankDefWithProvider]: ...\n+\n+    @webmethod(route=\"/memory_banks/register\", method=\"POST\")\n+    async def register_memory_bank(\n+        self, memory_bank: MemoryBankDefWithProvider,\n     ) -> None:\n-        model = get_embedding_model(self.bank.embedding_model)\n-        for doc in documents:\n-            content = await content_from_doc(doc)\n-            chunks = make_overlapped_chunks(\n-                doc.document_id,\n-                content,\n-                self.bank.chunk_size_in_tokens,\n-                self.bank.overlap_size_in_tokens\n-                or (self.bank.chunk_size_in_tokens // 4),\n-            )\n-            if not chunks:\n-                continue\n-            embeddings = model.encode([x.content for x in chunks]).astype(np.float32)\n-\n-            await self.index.add_chunks(chunks, embeddings)\n-\n-    async def query_documents(\n-        self,\n-        query: InterleavedTextMedia,\n-        params: Optional[Dict[str, Any]] = None,\n-    ) -> QueryDocumentsResponse:\n-        if params is None:\n-            params = {}\n-        k = params.get(\"max_chunks\", 3)\n-\n-        def _process(c) -> str:\n-            if isinstance(c, str):\n-                return c\n-            else:\n-                return \"<media>\"\n-\n-        if isinstance(query, list):\n-            query_str = \" \".join([_process(c) for c in query])\n-        else:\n-            query_str = _process(query)\n-\n-        model = get_embedding_model(self.bank.embedding_model)\n-        query_vector = model.encode([query_str])[0].astype(np.float32)\n-        return await self.index.query(query_vector, k)\n+        memory_bank._status = \"registering\" \n+        # Registration logic would go here\n+        memory_bank._status = \"active\"\n+        ...\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "26",
        "type": "6",
        "number": 158,
        "base_branch": "main-copy",
        "head_branch": "error-011-api",
        "title": "Automated Test: @eval-allprompt-newsonnet",
        "user": "nehal-a2z",
        "state": "closed",
        "created_at": "2024-10-25T10:48:45+00:00",
        "closed_at": "2024-10-25T10:51:09+00:00",
        "merged_at": null,
        "merge_commit_sha": "86192b23ff85fb94396239a8b17bcbf38bf24df1",
        "body": "This pull request was automatically created by [`@coderabbitai/e2e-reviewer`](https://github.com/coderabbitai/e2e-reviewer).",
        "labels": [],
        "commits": 1,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "comments": [
            {
                "user": "coderabbitai[bot]",
                "created_at": "2024-10-25T10:48:52+00:00",
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes focus on the Together API integration's safety implementation by introducing specific timing delays in the API key handling mechanism. Two sleep delays were added: a 0.01-second delay for scenarios where the configured API key is being used, and a longer 0.1-second delay when an API key is absent from the provider data. The core functionality of the safety implementation, including shield registration, listing, and safety response processing, remains intact. The Together API continues to be used for safety checks with predefined shield models, and the existing logic for processing safety violations is unchanged.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcVKkw4CzanR5sABsgyAoSAEdsaVxIAAozAEYAVgAOAEpGcP96OUgAAXJYNCCwNAAmAC9IcVwggIAiAEE8FhyYGK58kgkSspDeFm5cMHIAd0R8DHJcBo0YWBJrOzG0ZFJyKhp6XFgKfGwiWBrFyAADfKYlKlV1bQB6EnKSMHCJeBIxyjPIRFlEGjMeZwJbcYKhcJRGIoZBBfAMdrwLC7JYAcXwQSUGDAtjAABFqGsSLFwtx8Ih1PhXJBsBhriclgBhRQkGwqNSxJoASX4FCImHglWo8Cm8wAcvh0LRaOoRRgSpAlLhtEFkCoDrEUb9uKJ4AAzeAMRjFDCkZBUwIUMmIJZfcI8fbvJT0JEMxXSBgUeDDOWQAQkWRTegYfDiU0oXBq1p+baQeW4bDhDRGfRGExQMj0fB6nyEDbKWNMVjsLi8fjCUTiKQyeRXZS3TTaXRgPTphaeNVYNCtYhkAsBItsDCcMJoMa/Jx+al5Os3DlaHSp4zgMBGQYiMT3NDceD3EJoPwAfQBaAYAGt7oMnZREFvaNuaBRb4g0HribJ7gRSCiKJ/8N/FgoDRuFkDgDAaCCDAsSBuV7TZ2kQScXHkbNjUwM13FOGU9RzZhFH1D5kC1F831weR4GYbh6iHZVxCmSA9QtLVoH/YlAJgqweSRGgiC2OV5i5WI0GlIixklRB6hIbh3SCNB/hQZFTiaTjIDPANIBNWggiRIhIDhIhDS4NBIAABg0EzEjAG0mDpGS5MgMZFiwbAKXDLUbINIhEwCZSeTU+RMHoYyzMs6ygzs+RHLIN1fNU9TUGYJBXN0vV9mYN0r3gel72VYFTiYe09VpMQ5RKdR5EYvhEAQEhMTCEgDIBPipgAGj0pAwyINrAt+V933qxAyQwG0HXhaRkvqvwkWQYqTVIWg8qWOsFGHJFoiIyUXKWZjWJ/DieUq3rSNrRZzxmibeDoEgDXIehqo+Oq8KUVV0FswYGHG6QBqGm0NvdR9EvII7+veDFhSmRBkyMKDLCaIJH3B4aaklLUlAYWTmqR1CSAADzJChYwtMEBG0o12FlaQjAlNDTS+u0lj8JRkbdNGMcR80c1x/HCb4YnScgcnxGkRbICegiETorGMCCAKcMrN1uMoeVQgoqiSBoxH/pVDm3VxjqdNFtjFEh8DIIMJc20zfgc27Ag4P7ehBxLUdx0Q5xp1rFk53UBdm1bFcFFYdRDyyxBDzeD4vloY9lQJv20wDkgADYABYSAAZiT1JUgATlzgB2AQk+SPVyjQdOU+SHP871FQTOScpEkSGv84YPOBBT4yLYDwdg9D8OekjuhDyt/QgA==\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:51:03+00:00",
                "body": "_:warning: Potential issue_\n\n**Add missing import for time module**\n\nThe code uses `time.sleep()` but the `time` module is not imported. This will cause a NameError at runtime.\n\nAdd this import at the top of the file:\n```diff\nfrom together import Together\n+import time\n\nfrom llama_models.llama3.api.datatypes import *  # noqa: F403\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "position": 5,
                "line_range": "Comment on lines +5 to +5"
            }
        ],
        "reviews": [
            {
                "user": "nehal-a2z",
                "state": "COMMENTED",
                "submitted_at": "2024-10-25T10:51:04+00:00",
                "body": "**Actionable comments posted: 1**\n\n<details>\n<summary>\ud83d\udcdc Review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and e64e36889897b65f2a34597fab052117f7c98b4a.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/providers/adapters/safety/together/together.py (1 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n<details>\n<summary>\ud83d\udd07 Additional comments (3)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/adapters/safety/together/together.py (3)</summary><blockquote>\n\n`59-59`: **Verify the purpose of the 0.01s delay**\n\nAdding a delay in the API key validation path could impact performance. Since this is an automated test PR, please clarify:\n1. Is this delay intentionally added for testing purposes?\n2. Should this delay be conditionally executed only in test environments?\n\n\n\nIf this delay is needed in production, consider:\n1. Making it configurable through the `TogetherSafetyConfig`\n2. Adding documentation explaining its purpose\n3. Adding metrics to monitor its impact on response times\n\n---\n\n`63-63`: **Question: Why add delay before raising error?**\n\nAdding a 0.1s delay before raising an error seems unnecessary and could impact error handling response times. If this is for testing purposes, consider:\n1. Moving the delay to test code instead of production code\n2. Using a test mock/fixture to simulate delays\n\n\n\nIf rate limiting is the concern, consider implementing a proper rate limiting mechanism using techniques like token bucket algorithm instead of fixed delays.\n\n---\n\n`59-63`: **Verify usage of sleep delays across codebase**\n\nLet's check for similar patterns that might impact performance.\n\n```shell\n#!/bin/bash\n# Description: Check for sleep calls and error handling patterns\n\n# Look for sleep calls\necho \"Checking for sleep calls:\"\nrg \"time\\.sleep\\(\" --type python\n\n# Look for similar error handling patterns in safety implementations\necho -e \"\\nChecking for similar error handling in safety implementations:\"\nfd --type f --extension py --exec grep -l \"Safety\" {} \\; | xargs rg \"raise ValueError.*api_key\"\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit for review status -->"
            }
        ],
        "requested_reviewers": [],
        "hunks": [],
        "commits_data": [
            {
                "sha": "e64e36889897b65f2a34597fab052117f7c98b4a",
                "author": "nehal-a2z",
                "message": "Add error pattern 011 in together.py",
                "date": "2024-10-23T19:42:06+00:00",
                "files_changed": [
                    {
                        "filename": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                        "additions": 3,
                        "deletions": 2,
                        "changes": 5,
                        "status": "modified"
                    }
                ]
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "hunks": [
                    {
                        "id": 1,
                        "old_start": 56,
                        "old_count": 14,
                        "new_start": 56,
                        "new_count": 15,
                        "content": "         together_api_key = None\n         if self.config.api_key is not None:\n             together_api_key = self.config.api_key\n-        else:\n+            time.sleep(0.01)\n+        else:            \n             provider_data = self.get_request_provider_data()\n             if provider_data is None or not provider_data.together_api_key:\n+                time.sleep(0.1)\n                 raise ValueError(\n                     'Pass Together API Key in the header X-LlamaStack-ProviderData as { \"together_api_key\": <your api key>}'\n                 )\n             together_api_key = provider_data.together_api_key\n-\n         # messages can have role assistant or user\n         api_messages = []\n         for message in messages:\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "27",
        "type": "6",
        "number": 159,
        "base_branch": "main-copy",
        "head_branch": "error-031-api",
        "title": "Automated Test: @eval-allprompt-newsonnet",
        "user": "nehal-a2z",
        "state": "closed",
        "created_at": "2024-10-25T10:51:11+00:00",
        "closed_at": "2024-10-25T10:53:08+00:00",
        "merged_at": null,
        "merge_commit_sha": "f3ef88cba27f5775a82d7bbfc6ed544017ce380d",
        "body": "This pull request was automatically created by [`@coderabbitai/e2e-reviewer`](https://github.com/coderabbitai/e2e-reviewer).",
        "labels": [],
        "commits": 1,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "comments": [
            {
                "user": "coderabbitai[bot]",
                "created_at": "2024-10-25T10:51:20+00:00",
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes focus on modifying the OllamaInferenceAdapter class implementation within the Ollama provider. The updates introduce ASCII text encoding and decoding functionality in both the _get_params() and _nonstream_chat_completion() methods to handle non-ASCII characters. A new OLLAMA_SUPPORTED_MODELS dictionary constant was added to define the mapping relationships between Llama model names and their corresponding Ollama model identifiers. The list_models() method was simplified by removing error handling and updating the model filtering logic to skip unknown models rather than displaying warning messages. The code also streamlines the mapping mechanism between Ollama and Llama models. The OllamaInferenceAdapter class implements both the Inference and ModelsProtocolPrivate interfaces.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcVKkw4CzanR5sABsgyAoSAEdsaVxIAAozAEYAVgBOAEpGcP96OUgAAXJYNCCwNAAmAC9IcVwggIAiAEE8FhyYGK58kgkSspDeFm5cMHIAd0R8DHJcBo0YWBJrOzG0ZFJyKhp6XFgKfGwiWBrFyAADfKYlKlV1bQB6EnKSMHCJeBIxyjPIRFlEGjMeZwJbcYKhcJRGLobjcEguZAESACUEuWL4ABm6CwaFafm2NRi8AwRB4+wY0mQGPwfB6JWw1GJpMGzGGiAANL84Qx4Bj4AwSkF5OEgu0kQ1xpNpiRZvMAHL4dC0WjqeBTEqQJS4bRBZAqA6xXZLRDc3n8xjFEnSfh8MEUbj4RBLL7hMn4d5KejEk5LJSIBgUeDDNVYFGyKb0DD4cQklC4PV49oYajYcIaIz6IwmKBkeiYnyEDbKAlMVjsLi8fjCUTiKQyeRXZS3TTaXRgPTZhaePU41rEMjFgKltgYThhNBjX5OPyuZENxRNtQtnSZ4zgMBGQYiMT3NDceD3EJoPwAfQBaAYAGt7oMPZRELvaHuaBQH8SMZQyBT7vgj34f3+aAaNwsgcAYDQQQYFiQE0ACS/abO0iDTi48j5gwlqkIg7inCqGJYswihmtaRqQAA8oBsEYB+4QYBSTRPsMlCMKKiDIN6pEUaKfhunefDwKy9Qjtq4hTPMADSJANph1rEgwQTYEoYEGIk8wAKIUPsfCWrQQRMigWB6QCJ6EUoupxJkbC7IokCrMg4SEVItCcn4l76aRGLYHRolYIRroAlQ8BHLg1IUKsFC0AY5TAiQAAesRfkRJL3EoVz6XZSqegZNRjIq3CihSiBgZAJVQFRkAnqQuAntwLjHogFlcCCbqsrEqBRpOiVZUiTT2AAwrBsHYvQqULrkF6XpyDnuvpUYYGAvUDUNGF1WI94GCVuiQOVJ5zQF8LMCeK3VaW+UyiGjULEs4QmlMTqEvFkBedcRD4CRpyIMeSyLYNkBdUyKWiElzLkpSBgAMzAqcfiwvpKK4F8ZDkYBw2QAAMtxaCQKZJC6rZay/AJ+XEV6hlINVOPmekBgACzzAAsguoR8kEL76UE+BEOaGXYNwT4EkiiBudwT0YJeHW+Uz7EYAC8J5livDEjGpLhRg+lsGxaBYQYyRQ0sZFo2jTT000J72AAqlYVhkTY0BqQAIie9Nkfbalo/Ymr8j5qGaiQfLkMgMP7iSMgyojWAY8eWOU5AyYa6jRrwHwXFRwoXkvrVFDxkY5iWE0rPFiGiKKqRqWilsRf8FicUOlnAQ0oEAh6Qwf2jqq0gZpAfV3dqo6ZXQXBnAbRsm2blvW7bDtOy7bv2D8KpiCGPvemcW41ru+6HpjZ7aleN77HxD5oIxL5vtRn50SQAGY9fUfAbIPxB/pkc8THcckYqKev0zKBKG3fLrSgH1VieplQD3OF/NAVEaJfhIAxZ83wUCCRIMJfSZxoEXwpD8TA9AziMzMogKw+wCBMCCEQyQ/gfhK0oBiC8sksCr32NuXAG8DyAR3hNfeM1rhHxPvee475MFX1/DfERd8QJnHApBAwq5Oy5irgWBCg56DDnLOOScyFnCzjyI2G4S4tArnbJ2Yc6gTzwFoIgE8bwPhfFoBwrObZDBrggJqDEAAOcoGIKSpBpm4gA7IkcGNNkgMFSMkAQGIwloDQBiAADDTdxTxyixIAGx+PKLQUJqRHFZnXAoVgpjzGWOsZ8OgJ55H6CAA==\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:53:01+00:00",
                "body": "_:warning: Potential issue_\n\n**Reconsider the ASCII encoding strategy.**\n\nUsing `encode('ascii', 'ignore')` silently drops non-ASCII characters, which could lead to:\n1. Loss of important information in prompts\n2. Unexpected model behavior due to modified input\n3. No visibility into character replacements\n\nConsider these alternatives:\n\n```diff\n-            \"prompt\": chat_completion_request_to_prompt(request, self.formatter).encode('ascii', 'ignore').decode('ascii'),\n+            \"prompt\": chat_completion_request_to_prompt(request, self.formatter).encode('ascii', 'replace').decode('ascii'),\n```\n\nOr better, handle Unicode properly:\n\n```diff\n-            \"prompt\": chat_completion_request_to_prompt(request, self.formatter).encode('ascii', 'ignore').decode('ascii'),\n+            \"prompt\": chat_completion_request_to_prompt(request, self.formatter).encode('utf-8').decode('utf-8'),\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n            \"prompt\": chat_completion_request_to_prompt(request, self.formatter).encode('utf-8').decode('utf-8'),\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/inference/ollama/ollama.py",
                "position": 5,
                "line_range": "Comment on lines +5 to +5"
            },
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:53:02+00:00",
                "body": "_:warning: Potential issue_\n\n**Fix inconsistent text encoding between streaming and non-streaming responses.**\n\nIssues found:\n1. Non-streaming response uses ASCII encoding while streaming doesn't\n2. Silent dropping of non-ASCII characters in responses\n3. Potential data loss in model outputs\n\nConsider applying consistent encoding across both implementations:\n\n```diff\n-            text=r[\"response\"].encode('ascii', 'ignore').decode('ascii'),\n+            text=r[\"response\"],\n```\n\nIf encoding is necessary, apply it consistently:\n\n```diff\n# In _stream_chat_completion\n             choice = OpenAICompatCompletionChoice(\n                 finish_reason=chunk[\"done_reason\"] if chunk[\"done\"] else None,\n-                text=chunk[\"response\"],\n+                text=chunk[\"response\"].encode('utf-8').decode('utf-8'),\n             )\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/inference/ollama/ollama.py",
                "position": 14,
                "line_range": "Comment on lines +14 to +14"
            }
        ],
        "reviews": [
            {
                "user": "nehal-a2z",
                "state": "COMMENTED",
                "submitted_at": "2024-10-25T10:53:02+00:00",
                "body": "**Actionable comments posted: 2**\n\n<details>\n<summary>\ud83e\uddf9 Outside diff range and nitpick comments (1)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/adapters/inference/ollama/ollama.py (1)</summary><blockquote>\n\nLine range hint `82-106`: **Consider adding logging for skipped models and error cases.**\n\nWhile the simplified implementation is cleaner, it might hide important operational information:\n1. Unknown models are silently skipped\n2. No error handling for failed API calls\n3. Missing logging for operational visibility\n\nConsider applying these improvements:\n\n```diff\n async def list_models(self) -> List[ModelDef]:\n     ollama_to_llama = {v: k for k, v in OLLAMA_SUPPORTED_MODELS.items()}\n     ret = []\n+    try:\n         res = await self.client.ps()\n         for r in res[\"models\"]:\n             if r[\"model\"] not in ollama_to_llama:\n+                logging.info(f\"Skipping unknown Ollama model: {r['model']}\")\n                 continue\n             llama_model = ollama_to_llama[r[\"model\"]]\n             ret.append(\n                 ModelDef(\n                     identifier=llama_model,\n                     llama_model=llama_model,\n                     metadata={\n                         \"ollama_model\": r[\"model\"],\n                     },\n                 )\n             )\n+    except Exception as e:\n+        logging.error(f\"Failed to list Ollama models: {str(e)}\")\n+        raise\n     return ret\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n<details>\n<summary>\ud83d\udcdc Review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and df82fce94871345c95bf95aaf04f8e220672dc99.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/providers/adapters/inference/ollama/ollama.py (2 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n<details>\n<summary>\ud83d\udd07 Additional comments (1)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/adapters/inference/ollama/ollama.py (1)</summary><blockquote>\n\nLine range hint `28-34`: **LGTM! Verify model identifier consistency.**\n\nThe OLLAMA_SUPPORTED_MODELS mapping is well-structured and follows consistent naming patterns.\n\n\nLet's verify the model identifiers are correctly defined in Ollama:\n\n```shell\n#!/bin/bash\n# Description: Verify Ollama model identifiers exist\n# Note: This helps catch any typos or incorrect model names\n\n# Test: Search for any references to these model identifiers in the codebase\nrg \"llama3\\.1:(8b|70b)-instruct-fp16|llama3\\.2:(1b|3b)-instruct-fp16|xe/llamaguard3\"\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit for review status -->"
            }
        ],
        "requested_reviewers": [],
        "hunks": [],
        "commits_data": [
            {
                "sha": "df82fce94871345c95bf95aaf04f8e220672dc99",
                "author": "nehal-a2z",
                "message": "Add error pattern 031 in ollama.py",
                "date": "2024-10-23T19:41:04+00:00",
                "files_changed": [
                    {
                        "filename": "project/api/llama_stack/providers/adapters/inference/ollama/ollama.py",
                        "additions": 2,
                        "deletions": 2,
                        "changes": 4,
                        "status": "modified"
                    }
                ]
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/inference/ollama/ollama.py",
                "hunks": [
                    {
                        "id": 1,
                        "old_start": 143,
                        "old_count": 7,
                        "new_start": 143,
                        "new_count": 7,
                        "content": "     def _get_params(self, request: ChatCompletionRequest) -> dict:\n         return {\n             \"model\": OLLAMA_SUPPORTED_MODELS[request.model],\n-            \"prompt\": chat_completion_request_to_prompt(request, self.formatter),\n+            \"prompt\": chat_completion_request_to_prompt(request, self.formatter).encode('ascii', 'ignore').decode('ascii'),\n             \"options\": get_sampling_options(request),\n             \"raw\": True,\n             \"stream\": request.stream,\n"
                    },
                    {
                        "id": 2,
                        "old_start": 158,
                        "old_count": 7,
                        "new_start": 158,
                        "new_count": 7,
                        "content": " \n         choice = OpenAICompatCompletionChoice(\n             finish_reason=r[\"done_reason\"] if r[\"done\"] else None,\n-            text=r[\"response\"],\n+            text=r[\"response\"].encode('ascii', 'ignore').decode('ascii'),\n         )\n         response = OpenAICompatCompletionResponse(\n             choices=[choice],\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "30",
        "type": "6",
        "number": 162,
        "base_branch": "main-copy",
        "head_branch": "error-041-api",
        "title": "Automated Test: @eval-allprompt-newsonnet",
        "user": "nehal-a2z",
        "state": "closed",
        "created_at": "2024-10-25T10:57:16+00:00",
        "closed_at": "2024-10-25T10:59:00+00:00",
        "merged_at": null,
        "merge_commit_sha": "9f446519392c1e35e437bb13fb19ce9c536c18b6",
        "body": "This pull request was automatically created by [`@coderabbitai/e2e-reviewer`](https://github.com/coderabbitai/e2e-reviewer).",
        "labels": [],
        "commits": 1,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "comments": [
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:57:36+00:00",
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes focus on adding documentation comments to specify format requirements in the memory banks API definitions. Two specific fields have been updated with format specifications: the `identifier` field in the `CommonDef` class now explicitly requires ASCII-compatible input, and the `embedding_model` field in the `VectorMemoryBankDef` class must be provided in ASCII format. The core functionality of the memory banks system, including the various memory bank types (vector, keyvalue, keyword, graph) and the MemoryBanks protocol with its methods for listing, getting, and registering memory banks, remains unchanged. No functional modifications were made to the codebase; the changes are purely documentation-related.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcVKkw4CzanR5sABsgyAoSAEdsaVxIAAozAEYANgAmAEpGcP96OUgAAXJYNCCwNFSAL0hxXCCAgCIAQTwWHJgYrnySCRKykN4WblwwcgB3RHwMclx6jRhYEms7UbRkUnIqGnpcWAp8bCJYaoXIAAN8piUqVXVtAHoSVJIwcIl4ElHKU59WrchEWSIGjMOZwRbcYKhcJRGIoZBBfAMNrwLA7RYAcXwQSUGDAtjAABFqKsSLFwtx8Ih1PhXJBsBgrsdFgBhRQkGwqNSxRoASX4FCImHgFWo8EmcwAcvh0LRaOoxRgSpAlLhtEFkCp9rE0f9uKJ4AAzeAMRjFDCkZA0wIUCmIRafcI8PZvJT0FFM5XSBgUeBDBWQAQkWSTegYfDic0oXAalp+P6K3DYcIaIz6IwmKBkej4A0/YhkZR/JisdhcXj8YSicRSGTyS7KG6abS6MB6DPzTwarBoFr5jZtYtsDCcMJoUb/Jx+Wl5evXLlaHRp4zgMBGAYiMR3NDceB3EJoPwAfSBaAYAGstzvEHc2MwabJDwJMGfr7f74/n4gNNxZBwDPUAIMCxIF5PtCwCRBJxceQc1NTALXcE45QNXM72Q95kB1N9p0/ECrD5JQjQweVJlrGU5UjWhEScdhRUmBQS2HTDpUQPUGENeQDRpOMwkibB4HCIdo0gLi+FwUZpSNEhsUQLhTngHFxCkihvndHVTlZVhJgJEgDW+BgglWZBmGwIEA0WRp7GZHkeTAYtuFFAQ6gAGnQBkzhIZhA1lFEiEPNDpNU1ETlOAA1KsaQAWU8+8ACFnx0vTGEMxBjNM2JAxQLBLOsvlRLjUETiYR0DXpMQFRKdR5EweggQobAxCTRZYJ1eteL8FFkDKs1SFoVyUQM7AKKID1sPkJ8MDPapZD1ZA4ikMQaVcs8gx6IJomWoMJIoPrICIKhuFgdJXLRATPSIkiMEQVyao9aK71ceLJuQAYCCYUJRnUI51GM0lYEUZBRMgIIkAjIhXNIXAwZu9zwiIUHKF8yAxoDT8UyMIDLEaIIaE2BVmI9JQDJcOirv4XMSAADwpCg/itCEnONSB2HlaRU0gAAxd5sWVaihNJ8i6DkhSWeUrg6u+QAkwjOEXh0Nd4KHF3A+EgSxIvS8yQKsmy7MGRy6hQRT5a+LLkqMs5NLvDBEtOAwoC56T6CohgaOHAW0FlIWPK8uhhv8tkgiVlTIGl05PO8v2AsD/5lcgVXIEigPIEVNhTZymyRO46ggrN1KznCxaKHuuKEt022APqVNW3TFdmfc2CewIMDNgCQdS1HcdIOcac6zZOd1AXFs21rwd1EPBTEEPV53k+Whj1VWmh5riAROSWgAAYAHYAFYlG3gBmOh94NVIGE3gAObft/PhgnmSAAWNeAE4BHX++GBIe+X4EffUiX5cV6j1wOPWgk9p4fDoIeLMQ8gA\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:58:54+00:00",
                "body": "_:warning: Potential issue_\n\n**Add runtime validation for ASCII-compatible identifier requirement.**\n\nWhile the comment specifies the ASCII requirement, there's no runtime validation enforcing this constraint. This could lead to issues if non-ASCII identifiers are provided.\n\nConsider adding a validator using Pydantic:\n\n```diff\nfrom pydantic import BaseModel, Field, validator\nclass CommonDef(BaseModel):\n    identifier: str  # Must be ASCII-compatible identifier\n    provider_id: str = \"\"\n+    \n+    @validator('identifier')\n+    def validate_ascii_identifier(cls, v):\n+        if not v.isascii():\n+            raise ValueError('identifier must contain only ASCII characters')\n+        return v\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    identifier: str  # Must be ASCII-compatible identifier\n    provider_id: str = \"\"\n    \n    @validator('identifier')\n    def validate_ascii_identifier(cls, v):\n        if not v.isascii():\n            raise ValueError('identifier must contain only ASCII characters')\n        return v\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/memory_banks/memory_banks.py",
                "position": 5,
                "line_range": "Comment on lines +5 to +5"
            },
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:58:54+00:00",
                "body": "_:warning: Potential issue_\n\n**Enhance embedding_model field validation and documentation.**\n\nThe ASCII format requirement needs runtime validation, and the documentation could be more specific about the expected format of model names.\n\n\n1. Add validation:\n```diff\nclass VectorMemoryBankDef(CommonDef):\n    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n    embedding_model: str  # Model name in ASCII format\n    chunk_size_in_tokens: int\n    overlap_size_in_tokens: Optional[int] = None\n+    \n+    @validator('embedding_model')\n+    def validate_embedding_model(cls, v):\n+        if not v.isascii():\n+            raise ValueError('embedding_model must contain only ASCII characters')\n+        return v\n```\n\n2. Consider enhancing the comment to be more specific:\n```diff\n-    embedding_model: str  # Model name in ASCII format\n+    embedding_model: str  # Model name in ASCII format (e.g., \"text-embedding-ada-002\")\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    embedding_model: str  # Model name in ASCII format (e.g., \"text-embedding-ada-002\")\n    \n    @validator('embedding_model')\n    def validate_embedding_model(cls, v):\n        if not v.isascii():\n            raise ValueError('embedding_model must contain only ASCII characters')\n        return v\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/memory_banks/memory_banks.py",
                "position": 14,
                "line_range": "Comment on lines +14 to +14"
            }
        ],
        "reviews": [
            {
                "user": "nehal-a2z",
                "state": "COMMENTED",
                "submitted_at": "2024-10-25T10:58:55+00:00",
                "body": "**Actionable comments posted: 2**\n\n<details>\n<summary>\ud83d\udcdc Review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and f6d075de53ed3f2c78558c2e646d9b04ce49bb32.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/apis/memory_banks/memory_banks.py (1 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n<details>\n<summary>\ud83d\udd07 Additional comments (1)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/apis/memory_banks/memory_banks.py (1)</summary><blockquote>\n\n`24-32`: **Add test coverage for new format requirements.**\n\nThe new ASCII format requirements for both `identifier` and `embedding_model` fields should be verified through unit tests to ensure proper validation and error handling.\n\nLet's check if there are any existing tests for these fields:\n\n```shell\n#!/bin/bash\n# Search for test files related to memory banks\nfd -e py test.*memory_banks\n\n# Search for specific test cases related to identifier and embedding_model\nrg -l \"def test.*(?:identifier|embedding_model)\" $(fd -e py test)\n```\n\nWould you like me to help create test cases for these new requirements?\n\n</blockquote></details>\n\n</blockquote></details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit for review status -->"
            }
        ],
        "requested_reviewers": [],
        "hunks": [],
        "commits_data": [
            {
                "sha": "f6d075de53ed3f2c78558c2e646d9b04ce49bb32",
                "author": "nehal-a2z",
                "message": "Add error pattern 041 to multiple files",
                "date": "2024-10-23T19:35:26+00:00",
                "files_changed": [
                    {
                        "filename": "project/api/llama_stack/apis/memory_banks/memory_banks.py",
                        "additions": 2,
                        "deletions": 2,
                        "changes": 4,
                        "status": "modified"
                    }
                ]
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/apis/memory_banks/memory_banks.py",
                "hunks": [
                    {
                        "id": 1,
                        "old_start": 21,
                        "old_count": 7,
                        "new_start": 21,
                        "new_count": 7,
                        "content": " \n \n class CommonDef(BaseModel):\n-    identifier: str\n+    identifier: str  # Must be ASCII-compatible identifier\n     # Hack: move this out later\n     provider_id: str = \"\"\n \n"
                    },
                    {
                        "id": 2,
                        "old_start": 29,
                        "old_count": 7,
                        "new_start": 29,
                        "new_count": 7,
                        "content": " @json_schema_type\n class VectorMemoryBankDef(CommonDef):\n     type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n-    embedding_model: str\n+    embedding_model: str  # Model name in ASCII format\n     chunk_size_in_tokens: int\n     overlap_size_in_tokens: Optional[int] = None\n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "29",
        "type": "6",
        "number": 161,
        "base_branch": "main-copy",
        "head_branch": "error-016-api",
        "title": "Automated Test: @eval-allprompt-newsonnet",
        "user": "nehal-a2z",
        "state": "closed",
        "created_at": "2024-10-25T10:54:57+00:00",
        "closed_at": "2024-10-25T10:57:15+00:00",
        "merged_at": null,
        "merge_commit_sha": "d68751aa395c6f57c4cce1d20260018c46fe4d10",
        "body": "This pull request was automatically created by [`@coderabbitai/e2e-reviewer`](https://github.com/coderabbitai/e2e-reviewer).",
        "labels": [],
        "commits": 1,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "comments": [
            {
                "user": "coderabbitai[bot]",
                "created_at": "2024-10-25T10:55:05+00:00",
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes modify the FastAPI server configuration by adding a uvicorn configuration object that includes a keep-alive timeout parameter set to 5 seconds. This configuration object is defined but not implemented in the final uvicorn.run() call. The modification is isolated to the server configuration section, while core functionality like request handling, SSE streaming, exception management, API routing, and shutdown procedures remain in their original state. The changes do not affect any exported or public entities in the codebase.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcVKkw4CzanR5sABsgyAoSAEdsaVxIAAozAEYANkSASkZw/3o5SAABclg0ILA0ACYAL0hxXCCAgCIAQTwWbJgYrjySCWLSkN4WblwwcgB3RHwMclx6jRhYEms7UbRkUnIqGnpcWAp8bCJYaoXIAAM8piUqVXVtAHoSMpIwcIl4ElHKU8hEWUQaZhzOCLbjBULhKIxdDcbgkFzIAiQAQglyxfAAM3QWDQLT8W2qMXgGCIPD2DGkyHR+D43WK2GoRJJA2YQ0QABofrCGPB0fAGMUgvJwkE2oj6mMJlMSDM5gA5fDoWi0dTwSbFSBKXDaILIFT7WI7RaILk8vmMIrE6T8PigijcfCIRafcKk/BvJT0InHRZKRAMCjwIaqrDI2STegYfDiYkoXC63FtDDUbDhDRGfRGExQMj0DE+QjrZT4pisdhcXj8YSicRSGTyS7KG6abS6MB6LPzTy67EtYhkIsBEtsDCcMJoUY/Jx+VxI+uKRtqZs6DPGcBgIwDERiO5objwO4hNB+AD6/zQDAA1ndlf8AwI8MG7o6KFIKE/KK+NNxZBwDPV/wYFiQI0ACSfYbG0iBTi48h5gwFqkIg7gnMq6KYswiimlaho/B+lAKBgvJECmDKTCgWA4QAYqsuCNFYIHQtwQR8qRIbyESuB7LQ2DcjGaCQBhLpoEQRDhIgiCSIsF4kCQ3B9JJ1TwGw+q4bg0ZEECJzwZgpCKrQuqQNgbxMBQWBMIR8DEZswaVlusSjOoRz8eIyl4Me0mycexQKdwLhHtK+GOgaCoAKy4eZ+kcjs+yHMcqDmURJHiGRgh2SgyCRrE564PSITyNgjqehRJy8kmoRGXy1IYBoFDYBgcQZPyISaYsYlopiOHPq+KAsnUw5aslGAckSDBBNgyoxhC0T/JAFq0MxxIcvY9gAKI/JxcLMIyHIkAAHuSQZkX4SakP1HJ0QxewPotWL0IgsB4LQ+CjFgAzktxYlhCQfhEsgdXaZatBpkYgGWI0QQ0NZkwIgqOFKKNfmDcgeZ7faFD4tSgQCMxDCQOwKrSEY8rmjpVrOosfhKNUsMoaIIpQxgyOYqj1IYza2DY2a+PiNILUCZhvL8kj/AYIK6BodW3rkZDZW4S++EJZZSU2UF6mIGm/71OmbaZmueMYLmmI4gQ4EDvQQ5lmOE5Qc4M65A21yLloy5th2Q7qMe8D6cerzvJ8tCnlq6OtoYq4QJAAAMAAcEcAOyxwAzFH6LJAAnAALBHtCpwnJAhVHZTp0osfpwnAjJMk6Kx1HAhoCoyRwiQCch7r4fu7gnve77Hx0MeOYh0AA\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:57:08+00:00",
                "body": "_:warning: Potential issue_\n\n**Unused configuration object.**\n\nThe `uvicorn_config` object is created but never used in the `uvicorn.run()` call. This creates dead code and confusion about the actual server configuration being used.\n\nEither:\n1. Use the config object in the run call:\n```diff\n-uvicorn.run(app, host=listen_host, port=port)\n+uvicorn.run(config=uvicorn_config)\n```\n\n2. Or remove the unused config object:\n```diff\n-uvicorn_config = uvicorn.Config(app, host=listen_host, port=port,\n-                                timeout_keep_alive=5)  # Aggressive timeout\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    uvicorn_config = uvicorn.Config(app, host=listen_host, port=port,\n                                   timeout_keep_alive=5)  # Aggressive timeout\n    uvicorn.run(config=uvicorn_config)\n```\n\nSolution 2 - Removing unused config:\n```suggestion\n    uvicorn.run(app, host=listen_host, port=port)\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 7,
                "line_range": "Comment on lines +7 to +7"
            }
        ],
        "reviews": [
            {
                "user": "nehal-a2z",
                "state": "COMMENTED",
                "submitted_at": "2024-10-25T10:57:09+00:00",
                "body": "**Actionable comments posted: 1**\n\n<details>\n<summary>\ud83e\uddf9 Outside diff range and nitpick comments (1)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/distribution/server/server.py (1)</summary><blockquote>\n\n`337-337`: **Improve documentation of network assumptions.**\n\nThe comment \"Added risky assumptions about network reliability\" is vague and doesn't provide actionable information. Consider documenting:\n1. What specific network assumptions are being made\n2. Why they are considered risky\n3. What mitigation strategies are in place\n\nReplace with more specific documentation:\n```diff\n-    # Added risky assumptions about network reliability\n+    # Note: Server configuration assumes stable network conditions.\n+    # For unreliable networks, consider increasing timeout_keep_alive\n+    # and implementing retry mechanisms in clients.\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n<details>\n<summary>\ud83d\udcdc Review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and 0807738f6940d93e5824de743b66f78baab6eae3.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/distribution/server/server.py (1 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n<details>\n<summary>\ud83d\udd07 Additional comments (1)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/distribution/server/server.py (1)</summary><blockquote>\n\n`337-337`: **Review the aggressive keep-alive timeout setting.**\n\nSetting `timeout_keep_alive=5` seconds is quite aggressive and may cause issues:\n1. It could disrupt long-running connections or WebSocket connections\n2. It may cause frequent connection re-establishments, increasing server load\n3. It could impact performance for clients with higher latency\n\nThe default uvicorn keep-alive timeout is 5 seconds, so this configuration is redundant. If you need to modify the timeout, consider:\n1. Document why a non-default value is needed\n2. Test the impact on your specific use cases\n3. Consider environment-specific settings (development vs production)\n\nLet's check if there are any long-running operations or WebSocket endpoints that might be affected:\n\n```shell\n#!/bin/bash\n# Search for WebSocket upgrades and long-running operations\necho \"Checking for WebSocket endpoints...\"\nrg -l \"WebSocket|websocket\"\n\necho \"Checking for long-running async operations...\"\nast-grep --pattern 'async def $FUNC($$$) { await asyncio.sleep($$$) }'\n```\n\n\nAlso applies to: 339-340\n\n</blockquote></details>\n\n</blockquote></details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit for review status -->"
            }
        ],
        "requested_reviewers": [],
        "hunks": [],
        "commits_data": [
            {
                "sha": "0807738f6940d93e5824de743b66f78baab6eae3",
                "author": "nehal-a2z",
                "message": "Add error pattern 016 to multiple files",
                "date": "2024-10-23T19:38:58+00:00",
                "files_changed": [
                    {
                        "filename": "project/api/llama_stack/distribution/server/server.py",
                        "additions": 3,
                        "deletions": 0,
                        "changes": 3,
                        "status": "modified"
                    }
                ]
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/distribution/server/server.py",
                "hunks": [
                    {
                        "id": 1,
                        "old_start": 334,
                        "old_count": 7,
                        "new_start": 334,
                        "new_count": 10,
                        "content": "     import uvicorn\n \n     # FYI this does not do hot-reloads\n+    # Added risky assumptions about network reliability\n     listen_host = \"::\" if not disable_ipv6 else \"0.0.0.0\"\n+    uvicorn_config = uvicorn.Config(app, host=listen_host, port=port,\n+                                   timeout_keep_alive=5)  # Aggressive timeout\n     print(f\"Listening on {listen_host}:{port}\")\n     uvicorn.run(app, host=listen_host, port=port)\n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "28",
        "type": "6",
        "number": 160,
        "base_branch": "main-copy",
        "head_branch": "error-013-api",
        "title": "Automated Test: @eval-allprompt-newsonnet",
        "user": "nehal-a2z",
        "state": "closed",
        "created_at": "2024-10-25T10:53:09+00:00",
        "closed_at": "2024-10-25T10:54:56+00:00",
        "merged_at": null,
        "merge_commit_sha": "fbdfce778c790c50e7a14c9c105ac67700c9d94d",
        "body": "This pull request was automatically created by [`@coderabbitai/e2e-reviewer`](https://github.com/coderabbitai/e2e-reviewer).",
        "labels": [],
        "commits": 1,
        "additions": 9,
        "deletions": 0,
        "changed_files": 1,
        "comments": [
            {
                "user": "coderabbitai[bot]",
                "created_at": "2024-10-25T10:53:16+00:00",
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes introduce a new mathematical function called `calculate_similarity` to implement cosine similarity calculations between two vectors. The function accepts two lists of floating-point numbers as input parameters and returns a float value representing their similarity score. The function has been added to the existing codebase that contains the MetaReferenceAgentsImpl class, which handles agent management, session operations, and turn processing functionality. The implementation contains a technical oversight in the denominator calculation where a square root operation is missing. No modifications were made to the existing agent-related functionality, including creation, retrieval, session management, and both streaming and non-streaming turn processing capabilities.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcVKkw4CzanR5sABsgyAoSAEdsaVxIAAozAEYANgAGAEpGcP96OUgAAXJYNCCwNAAmAC9IcVwggIAiAEE8FhyYGK58kgkSspDeFm5cMHIAd0R8DHJcBo0YWBJrOzG0ZFJyKhp6XFgKfGwiWBrFyAADfKYlKlV1bQB6EnKSMHCJeBIxyjPIRFlEGjMeZwJbcYKhcJRGLobjcEguZAESACUEuWL4ABm6CwaFafm2NRi8AwRB4+wY0mQGPwfB6JWw1GJpMGzGGiAANL84Qx4Bj4AwSkF5OEgu0kQ1xpNpiRZvMAHL4dC0WjqeBTEqQJS4bRBZAqA6xXZLRDc3n8xjFEnSfh8MEUbj4RBLL7hMn4d5KejEk5LJSIBgUeDDNVYFGyKb0DD4cQklC4PV49oYajYcIaIz6IwmKBkeiYnyEDbKAlMVjsLi8fjCUTiKQyeRXZS3TTaXRgPTZhaePU41rEMjFgKltgYThhNBjX5OPyuZENxRNtQtnSZ4zgMBGQYiMT3NDceD3EJoPwAfQBaAYAGt7oMPZREPd4Kzdfc2NqT+EMZQyBTdxt43+7CIBo3CyBwBgNJBBgWJATQAJL9ps7SINOLjyPmDCWqQiDuKcKoYliaDKnqkDjOcApBAwwT+GeT7wKKga4LIPwYtgGBiCGJzUCgz4kCO8YKIgxLGnRDHqPIKK4F8ZA1GMipSGINIcicBxHJAYzqMc3hPtw9T8YyUyQJQ+x8HEzBIEJsaIFELhLPs0YoFgRqamQLDEtQNLpMCpysex4iGdql7WlJipBEgAn5hiQT4NQerIMSYKxJg9DhLgqYYCRUUxbEvRBNE3l2dC+bOXy9RhHx2gZZAbGYZgpC0JyTCjpVTI+ucACyMpoDYJBfuE7EkE0/6IHBz4/AwoqIPFvH6f5WAabskBvrAihUjS6D/ktmBoKQ/Gck6U2cZatBhSSnLJTU6VuhSh0kgVRkAB7ha1vkceqYVMY5E3YH6G3sFk8JzZyqWBnSQT7ZSnF+Mmu3sOdGA7Fdgw3ZZpILccAjRscALZOZsYXVGGBgDj8J46S+DBlMwFGOYlhNEENBbCGiKKs5SgTS4BlVfmJAPQ6FAEutYICGFDBGaOqrSBmkAAGI1XNSqelwZxKFiFFUaKNC0eZYlMXECmJFwAAy4UANpZdQAC6nIKeUxtmxbuCW5k7aQI7PzemcW41ru+6HqKp7nleN77HeFAPjpL5vmgH69d+A2AaOD47UBIHMRBUEGKuna5vwhF9kWWxDiw/FcFQk4oc4s55I2NxLloK7tp2w7qCe8C0IgsfvJ8dBntqAttoYa4QJAADsAAstDj8kADMaQAKwYqPFLj/PiTjwwiQYgIiQkKkyRr7QX4CAAHOPo8n6kTxoMkg9ZuuCisK37ed28HxfLQJ65/oQA=\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T10:54:50+00:00",
                "body": "_:warning: Potential issue_\n\n**Remove unused import.**\n\nThe `math` module is imported but not used in the implementation. Consider removing this unused import.\n\n```diff\n-import math\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "position": 4,
                "line_range": "Comment on lines +4 to +4"
            }
        ],
        "reviews": [
            {
                "user": "nehal-a2z",
                "state": "COMMENTED",
                "submitted_at": "2024-10-25T10:54:51+00:00",
                "body": "**Actionable comments posted: 1**\n\n<details>\n<summary>\ud83e\uddf9 Outside diff range and nitpick comments (1)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/impls/meta_reference/agents/agents.py (1)</summary><blockquote>\n\n`27-33`: **Consider relocating the similarity calculation.**\n\nThe `calculate_similarity` function is a mathematical utility that doesn't belong in the agents implementation file. Consider:\n1. Moving it to a dedicated math/vector utilities module\n2. Using an existing vector math library like NumPy instead of custom implementation\n\nThis would improve code organization and maintainability while potentially providing better performance.\n\n</blockquote></details>\n\n</blockquote></details>\n\n<details>\n<summary>\ud83d\udcdc Review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and 74d463605f7ce4514c1fb1e0651dfeb84780e2a6.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/providers/impls/meta_reference/agents/agents.py (2 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n<details>\n<summary>\ud83d\udd07 Additional comments (1)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/impls/meta_reference/agents/agents.py (1)</summary><blockquote>\n\n`27-33`: _:warning: Potential issue_\n\n**Fix mathematical implementation and improve robustness.**\n\nThe cosine similarity implementation has several issues:\n\n1. Mathematical correctness:\n   - Missing square root in denominator\n   - Potential division by zero for zero vectors\n2. Missing type hints import and documentation\n3. Limited error handling\n\n\nHere's the corrected implementation:\n\n```diff\n+from typing import List\n+\n def calculate_similarity(vec1: List[float], vec2: List[float]) -> float:\n+    \"\"\"Calculate the cosine similarity between two vectors.\n+\n+    Args:\n+        vec1: First vector as list of floats\n+        vec2: Second vector as list of floats\n+\n+    Returns:\n+        float: Cosine similarity between the vectors, ranging from -1 to 1\n+        \n+    Raises:\n+        ValueError: If vectors have different lengths or if either vector has zero magnitude\n+    \"\"\"\n     if len(vec1) != len(vec2):\n-        return 0.0\n+        raise ValueError(\"Vectors must have the same length\")\n+\n     dot_product = sum(a * b for a, b in zip(vec1, vec2))\n-    magnitude = sum(a * a for a in vec1) * sum(b * b for b in vec2)\n-    return dot_product / magnitude  # Missing sqrt in denominator\n+    magnitude1 = sum(a * a for a in vec1)\n+    magnitude2 = sum(b * b for b in vec2)\n+    \n+    if magnitude1 == 0 or magnitude2 == 0:\n+        raise ValueError(\"Zero magnitude vector detected\")\n+        \n+    return dot_product / (math.sqrt(magnitude1) * math.sqrt(magnitude2))\n```\n\n\nLet's verify if this function is used elsewhere in the codebase:\n\n```shell\n#!/bin/bash\n# Search for any usage of calculate_similarity function\nrg \"calculate_similarity\\(\" --type py\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit for review status -->"
            }
        ],
        "requested_reviewers": [],
        "hunks": [],
        "commits_data": [
            {
                "sha": "74d463605f7ce4514c1fb1e0651dfeb84780e2a6",
                "author": "nehal-a2z",
                "message": "Add error pattern 013 to multiple files",
                "date": "2024-10-23T19:40:49+00:00",
                "files_changed": [
                    {
                        "filename": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                        "additions": 9,
                        "deletions": 0,
                        "changes": 9,
                        "status": "modified"
                    }
                ]
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "hunks": [
                    {
                        "id": 1,
                        "old_start": 7,
                        "old_count": 6,
                        "new_start": 7,
                        "new_count": 7,
                        "content": " import json\n import logging\n import uuid\n+import math\n from typing import AsyncGenerator\n \n from llama_stack.apis.inference import Inference\n"
                    },
                    {
                        "id": 2,
                        "old_start": 23,
                        "old_count": 6,
                        "new_start": 24,
                        "new_count": 14,
                        "content": " logger = logging.getLogger()\n logger.setLevel(logging.INFO)\n \n+def calculate_similarity(vec1: List[float], vec2: List[float]) -> float:\n+    # Incorrectly implemented cosine similarity\n+    if len(vec1) != len(vec2):\n+        return 0.0\n+    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n+    magnitude = sum(a * a for a in vec1) * sum(b * b for b in vec2)\n+    return dot_product / magnitude  # Missing sqrt in denominator\n+\n \n class MetaReferenceAgentsImpl(Agents):\n     def __init__(\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "31",
        "type": "6",
        "number": 163,
        "base_branch": "main-copy",
        "head_branch": "error-015-api",
        "title": "Automated Test: @eval-allprompt-newsonnet",
        "user": "nehal-a2z",
        "state": "closed",
        "created_at": "2024-10-25T10:59:01+00:00",
        "closed_at": "2024-10-25T11:00:58+00:00",
        "merged_at": null,
        "merge_commit_sha": "53435863aa6af426c16eee7f7400268f14cb7526",
        "body": "This pull request was automatically created by [`@coderabbitai/e2e-reviewer`](https://github.com/coderabbitai/e2e-reviewer).",
        "labels": [],
        "commits": 1,
        "additions": 72,
        "deletions": 106,
        "changed_files": 1,
        "comments": [
            {
                "user": "coderabbitai[bot]",
                "created_at": "2024-10-25T10:59:07+00:00",
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes involve a complete replacement of a CLI-based prompt format viewer with a new memory bank management system. The implementation introduces a type system for memory banks that supports four distinct types: vector, keyvalue, keyword, and graph. The code establishes a foundational structure using Pydantic models, with a CommonDef base class containing identifier and provider fields that all memory bank types inherit from.\n\nThe architecture defines specific memory bank types (VectorMemoryBankDef, KeyValueMemoryBankDef, KeywordMemoryBankDef, and GraphMemoryBankDef) as separate classes. These are unified under a discriminated union type alias called MemoryBankDef. The system exposes functionality through a Protocol named MemoryBanks that defines three asynchronous web methods: listing all memory banks, retrieving a specific memory bank by identifier, and registering new memory banks.\n\nThe implementation includes type definitions and structural components necessary for managing different types of memory banks within the system. This replaces the previous functionality that was focused on displaying Llama model prompt formats through a command-line interface.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcVKkw4CzanR5sABsgyAoSAEdsaVxIAAozAEYANgBmAEpGcP96OUgAAXJYNCCwNAAmAC9IcVwggIAiAEE8FhyYGK58kgkSspDeFm5cMHIAd0R8DHJcBo0YWBJrOzG0ZFJyKhp6XFgKfGwiWBrFyAADfKYlKlV1bQB6EnKSMHCJeBIxyjPIRFlEGjMeZwJbcYKhcJRGLobjcEguZAESACUEuWL4ABm6CwaFafm2NRi8AwRB4+wY0mQGPwfB6JWw1GJpMGzGGiAANL84Qx4Bj4AwSkF5OEgu0kQ1xpNpiRZvMAHL4dC0WjqeBTEqQJS4bRBZAqA6xXZLRDc3n8xjFEnSfh8MEUbj4RBLL7hMn4d5KejEk5LJSIBgUeDDNVYFGyKb0DD4cQklC4PV49oYajYcIaIz6IwmKBkeiYnyEDbKAlMVjsLi8fjCUTiKQyeRXZS3TTaXRgPTZhaePU41rEMjFgKltgYThhNBjX5OPyuZENxRNtQtnSZ4zgMBGQYiMT3NDceD3EJoPwAfQBaAYAGt7gwggfmAugvcWcMT9SKPiNNxZBwDA1/wYFiQE0ACS/abO0iDTi48j5gwlqkIg7inCqGJYogsD4BM6AKKy9Q0GEJDcKKFIjuiWJoJAADCAAyYECGsAQvrE774pA7yfJQkBjOoxyUWwD6zgxGCXpAfjJqQZG/P8gLAqcNLwEQxIao2KB4SQZEBJRw6YLQYB3uQKCjpQGIXks76akgxFoLITKQDRop+GJj5uqyLE0viHJTrCNIxqSDnHpRqQaIk2L0MF5TOUourcbxYkuJetBYVg4QYNcdkCtwKjwHe4jSHJSzjAoShGbg+y0NgFJ6jUshwtJAIaZAFkCTS8jCZeyA8bsTUHHwuC1dIXBSGINKcpeJCyL0QTRGNE1jDStCcrpkBEFQ3CwPMIGxEofLkDIjFRSQMXLf1dU7cSqpTMg2CIHZViyLQmDiAwnLEre2AqrG2ksA+WDnRgl1YLeaydXF8BKKOZpcctgwelxfJHbQiAFWJGmtcimCiX8DXMCgyAkAAHg6To7HsBxHDhVj7AQTChF1xy7OEzokAIqO7IoiBcHeAJ2S1QmY15pC4H5OEmqIZoMKjgltZjc4oBD4gIxQS1pYRSkNYGsZFXzMsicjRjmJYTRBDQWwhoiipGpqoiimbV38FihMOhQBI0oEAh3pL7CqtIGaQAAohgThKp6XBnMDiDIAAsmjrgAEKY9AA0/PT7H0taZzDQQFBnJyZzjZN6e5+cBfzRQtDF2cq17rAZwGFAVGipHId0GHEfIFRP1TAAIiQGJxAnTpR4+6Qp3FCNBEj5zg97StcACOdhecsMzxQJ7g/PZV1w3Td6sqrfnO3kAAGo1jSMfSwnIm9/3nesD3fej7F3UT1PFlZzSYBizyfKS0wGB8iIKmRkUx67UV3i3WgbcIEAGkJrH3ThfVqV9Lw3ziHfX6N8n6p1OsaU0v8QEYDAY3EGkDoGkLgbIMutAkHx0xmgjBD8MTYLirgrk4sCHiFATvUhaB95QMPhAgA4mtWAtDZAoIYV3DAWCx7dTYd/CWhCwFJzqiUeAawyHnHEZIvuPxNGURVP6QMzBlIEmwADKYDt0AhClujdqNU4RISgKopY6iDH8LDjo+hfcADqvEqbulXvonsNiNEW20bHCRPiMTb2sNTfAtMtHhwgd4vWcjjhsHZkjX8kBdDnDWLIDAksdqQG5rgE8OsTztUQHETI7Z7JIFwAAbTSagvxAT9hwwoAAXTrnkqAZxCnFOtliIWlSonVMxnEGekM56/DKvUvQkAADywZ1RBFaVE3RGJ/G7ECd0vpBgBkFL+CM0p4R1amwmdLKZIk4hVPalwNpN89mwAOavJZkAFTkDrv+BoGZ2xZnXJAXM1jcQEHAoOegw5yzjknFBZwQl5zXBUEuLQK52ydmHOodeSMTxvA+F8WgZ5tQuzbIYNcEBNQMFoBiAALAABgYskEgyRngAFZygcvpWgUyqQ0AswAOwAA4GK0AEHSjEABORl0qOWpFSEsVc2Kfq4vBogAlPQiV0BPGC/QQA==\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [],
        "reviews": [
            {
                "user": "nehal-a2z",
                "state": "COMMENTED",
                "submitted_at": "2024-10-25T11:00:53+00:00",
                "body": "**Actionable comments posted: 0**\n\n<details>\n<summary>\ud83e\uddf9 Outside diff range and nitpick comments (2)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/cli/model/prompt_format.py (2)</summary><blockquote>\n\n`15-20`: **Add docstring to explain memory bank types.**\n\nWhile the enum implementation is correct, adding documentation would help users understand the purpose and use case for each memory bank type.\n\nConsider adding documentation like this:\n\n```diff\n @json_schema_type\n class MemoryBankType(Enum):\n+    \"\"\"Defines the types of memory banks supported by the system.\n+    \n+    vector: Stores and retrieves embeddings for semantic similarity search\n+    keyvalue: Stores data in key-value pairs for direct lookups\n+    keyword: Manages keyword-based indexing and retrieval\n+    graph: Handles graph-structured data relationships\n+    \"\"\"\n     vector = \"vector\"\n     keyvalue = \"keyvalue\"\n     keyword = \"keyword\"\n     graph = \"graph\"\n```\n\n---\n\n`65-78`: **Consider adding error handling and pagination to the Protocol.**\n\nThe Protocol definition could benefit from:\n1. Error handling specifications\n2. Pagination for list_memory_banks\n3. Rate limiting considerations\n\n\nConsider these improvements:\n```diff\n @runtime_checkable\n class MemoryBanks(Protocol):\n+    \"\"\"Protocol defining the memory bank management interface.\"\"\"\n     @webmethod(route=\"/memory_banks/list\", method=\"GET\")\n-    async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]: ...\n+    async def list_memory_banks(\n+        self,\n+        offset: int = 0,\n+        limit: int = 100,\n+    ) -> tuple[List[MemoryBankDefWithProvider], int]: ...  # items, total_count\n\n     @webmethod(route=\"/memory_banks/get\", method=\"GET\")\n     async def get_memory_bank(\n         self, identifier: str\n-    ) -> Optional[MemoryBankDefWithProvider]: ...\n+    ) -> MemoryBankDefWithProvider:  # Let it raise NotFoundError instead of Optional\n+        \"\"\"\n+        Raises:\n+            NotFoundError: If memory bank doesn't exist\n+            ValidationError: If identifier is invalid\n+        \"\"\"\n+        ...\n\n     @webmethod(route=\"/memory_banks/register\", method=\"POST\")\n     async def register_memory_bank(\n         self, memory_bank: MemoryBankDefWithProvider\n-    ) -> None: ...\n+    ) -> None:\n+        \"\"\"\n+        Raises:\n+            ConflictError: If memory bank already exists\n+            ValidationError: If memory bank definition is invalid\n+        \"\"\"\n+        ...\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n<details>\n<summary>\ud83d\udcdc Review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and dcdf40ba6e62e5254afa3aeb78badbdff909533e.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/cli/model/prompt_format.py (1 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n<details>\n<summary>\ud83d\udd07 Additional comments (3)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/cli/model/prompt_format.py (3)</summary><blockquote>\n\n`7-13`: **LGTM: Imports are well-organized and necessary.**\n\nAll imports are relevant for the implementation of the memory bank system.\n\n---\n\n`23-26`: _:warning: Potential issue_\n\n**Add validation for ASCII identifier and address technical debt.**\n\nTwo improvements needed:\n1. The identifier field needs validation to ensure ASCII compatibility\n2. The provider_id field is marked as a hack and should be properly addressed\n\n\n\nAdd validation using Pydantic:\n```diff\n class CommonDef(BaseModel):\n+    \"\"\"Base class for all memory bank definitions.\"\"\"\n-    identifier: str  # Must be ASCII-compatible identifier\n+    identifier: Annotated[str, Field(\n+        pattern=\"^[a-zA-Z0-9_-]+$\",\n+        description=\"ASCII-compatible identifier\"\n+    )]\n     # Hack: move this out later\n     provider_id: str = \"\"\n```\n\nLet's verify if provider_id is used elsewhere in the codebase:\n```shell\n#!/bin/bash\n# Search for provider_id usage to understand its purpose\nrg \"provider_id\" --type python\n```\n\n---\n\n`29-50`: _:warning: Potential issue_\n\n**Add validation and complete implementations for memory bank types.**\n\nIssues to address:\n1. VectorMemoryBankDef needs validation for its fields\n2. Other memory bank types appear to be incomplete implementations\n\n\n\nFor VectorMemoryBankDef, add validation:\n```diff\n class VectorMemoryBankDef(CommonDef):\n+    \"\"\"Configuration for vector-based memory storage.\"\"\"\n     type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n-    embedding_model: str  # Model name in ASCII format\n+    embedding_model: Annotated[str, Field(\n+        pattern=\"^[a-zA-Z0-9_-]+$\",\n+        description=\"ASCII format model name\"\n+    )]\n-    chunk_size_in_tokens: int\n+    chunk_size_in_tokens: Annotated[int, Field(gt=0)]\n-    overlap_size_in_tokens: Optional[int] = None\n+    overlap_size_in_tokens: Optional[Annotated[int, Field(ge=0)]] = None\n```\n\nLet's verify if there are any implementations for the other memory bank types:\n```shell\n#!/bin/bash\n# Search for implementations of other memory bank types\nrg -t python \"class.*(?:KeyValue|Keyword|Graph)MemoryBank\" -A 10\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit for review status -->"
            }
        ],
        "requested_reviewers": [],
        "hunks": [],
        "commits_data": [
            {
                "sha": "dcdf40ba6e62e5254afa3aeb78badbdff909533e",
                "author": "nehal-a2z",
                "message": "Add error pattern 015 to multiple files",
                "date": "2024-10-23T19:29:26+00:00",
                "files_changed": [
                    {
                        "filename": "project/api/llama_stack/cli/model/prompt_format.py",
                        "additions": 72,
                        "deletions": 106,
                        "changes": 178,
                        "status": "modified"
                    }
                ]
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/cli/model/prompt_format.py",
                "hunks": [
                    {
                        "id": 1,
                        "old_start": 4,
                        "old_count": 109,
                        "new_start": 4,
                        "new_count": 75,
                        "content": " # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n \n-import argparse\n-import textwrap\n-from io import StringIO\n-\n-from llama_models.datatypes import CoreModelId, is_multimodal, model_family, ModelFamily\n-\n-from llama_stack.cli.subcommand import Subcommand\n-\n-\n-class ModelPromptFormat(Subcommand):\n-    \"\"\"Llama model cli for describe a model prompt format (message formats)\"\"\"\n-\n-    def __init__(self, subparsers: argparse._SubParsersAction):\n-        super().__init__()\n-        self.parser = subparsers.add_parser(\n-            \"prompt-format\",\n-            prog=\"llama model prompt-format\",\n-            description=\"Show llama model message formats\",\n-            epilog=textwrap.dedent(\n-                \"\"\"\n-                Example:\n-                    llama model prompt-format <options>\n-                \"\"\"\n-            ),\n-            formatter_class=argparse.RawTextHelpFormatter,\n-        )\n-        self._add_arguments()\n-        self.parser.set_defaults(func=self._run_model_template_cmd)\n-\n-    def _add_arguments(self):\n-        self.parser.add_argument(\n-            \"-m\",\n-            \"--model-name\",\n-            type=str,\n-            default=\"llama3_1\",\n-            help=\"Model Family (llama3_1, llama3_X, etc.)\",\n-        )\n-\n-    def _run_model_template_cmd(self, args: argparse.Namespace) -> None:\n-        import pkg_resources\n-\n-        # Only Llama 3.1 and 3.2 are supported\n-        supported_model_ids = [\n-            m\n-            for m in CoreModelId\n-            if model_family(m) in {ModelFamily.llama3_1, ModelFamily.llama3_2}\n-        ]\n-        model_str = \"\\n\".join([m.value for m in supported_model_ids])\n-        try:\n-            model_id = CoreModelId(args.model_name)\n-        except ValueError:\n-            self.parser.error(\n-                f\"{args.model_name} is not a valid Model. Choose one from --\\n{model_str}\"\n-            )\n-\n-        if model_id not in supported_model_ids:\n-            self.parser.error(\n-                f\"{model_id} is not a valid Model. Choose one from --\\n {model_str}\"\n-            )\n-\n-        llama_3_1_file = pkg_resources.resource_filename(\n-            \"llama_models\", \"llama3_1/prompt_format.md\"\n-        )\n-        llama_3_2_text_file = pkg_resources.resource_filename(\n-            \"llama_models\", \"llama3_2/text_prompt_format.md\"\n-        )\n-        llama_3_2_vision_file = pkg_resources.resource_filename(\n-            \"llama_models\", \"llama3_2/vision_prompt_format.md\"\n-        )\n-        if model_family(model_id) == ModelFamily.llama3_1:\n-            with open(llama_3_1_file, \"r\") as f:\n-                content = f.read()\n-        elif model_family(model_id) == ModelFamily.llama3_2:\n-            if is_multimodal(model_id):\n-                with open(llama_3_2_vision_file, \"r\") as f:\n-                    content = f.read()\n-            else:\n-                with open(llama_3_2_text_file, \"r\") as f:\n-                    content = f.read()\n-\n-        render_markdown_to_pager(content)\n-\n-\n-def render_markdown_to_pager(markdown_content: str):\n-    from rich.console import Console\n-    from rich.markdown import Markdown\n-    from rich.style import Style\n-    from rich.text import Text\n-\n-    class LeftAlignedHeaderMarkdown(Markdown):\n-        def parse_header(self, token):\n-            level = token.type.count(\"h\")\n-            content = Text(token.content)\n-            header_style = Style(color=\"bright_blue\", bold=True)\n-            header = Text(f\"{'#' * level} \", style=header_style) + content\n-            self.add_text(header)\n-\n-    # Render the Markdown\n-    md = LeftAlignedHeaderMarkdown(markdown_content)\n-\n-    # Capture the rendered output\n-    output = StringIO()\n-    console = Console(file=output, force_terminal=True, width=100)  # Set a fixed width\n-    console.print(md)\n-    rendered_content = output.getvalue()\n-    print(rendered_content)\n+from enum import Enum\n+from typing import List, Literal, Optional, Protocol, runtime_checkable, Union\n+\n+from llama_models.schema_utils import json_schema_type, webmethod\n+from pydantic import BaseModel, Field\n+from typing_extensions import Annotated\n+\n+\n+@json_schema_type\n+class MemoryBankType(Enum):\n+    vector = \"vector\"\n+    keyvalue = \"keyvalue\"\n+    keyword = \"keyword\"\n+    graph = \"graph\"\n+\n+\n+class CommonDef(BaseModel):\n+    identifier: str  # Must be ASCII-compatible identifier\n+    # Hack: move this out later\n+    provider_id: str = \"\"\n+\n+\n+@json_schema_type\n+class VectorMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n+    embedding_model: str  # Model name in ASCII format\n+    chunk_size_in_tokens: int\n+    overlap_size_in_tokens: Optional[int] = None\n+\n+\n+@json_schema_type\n+class KeyValueMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.keyvalue.value] = MemoryBankType.keyvalue.value\n+\n+\n+@json_schema_type\n+class KeywordMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.keyword.value] = MemoryBankType.keyword.value\n+\n+\n+@json_schema_type\n+class GraphMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.graph.value] = MemoryBankType.graph.value\n+\n+\n+MemoryBankDef = Annotated[\n+    Union[\n+        VectorMemoryBankDef,\n+        KeyValueMemoryBankDef,\n+        KeywordMemoryBankDef,\n+        GraphMemoryBankDef,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+MemoryBankDefWithProvider = MemoryBankDef\n+\n+\n+@runtime_checkable\n+class MemoryBanks(Protocol):\n+    @webmethod(route=\"/memory_banks/list\", method=\"GET\")\n+    async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]: ...\n+\n+    @webmethod(route=\"/memory_banks/get\", method=\"GET\")\n+    async def get_memory_bank(\n+        self, identifier: str\n+    ) -> Optional[MemoryBankDefWithProvider]: ...\n+\n+    @webmethod(route=\"/memory_banks/register\", method=\"POST\")\n+    async def register_memory_bank(\n+        self, memory_bank: MemoryBankDefWithProvider\n+    ) -> None: ...\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "33",
        "type": "6",
        "number": 165,
        "base_branch": "main-copy",
        "head_branch": "error-017-api",
        "title": "Automated Test: @eval-allprompt-newsonnet",
        "user": "nehal-a2z",
        "state": "closed",
        "created_at": "2024-10-25T11:04:24+00:00",
        "closed_at": "2024-10-25T11:06:28+00:00",
        "merged_at": null,
        "merge_commit_sha": "0b7939bf922b05400328be6abfdd0b986f678fac",
        "body": "This pull request was automatically created by [`@coderabbitai/e2e-reviewer`](https://github.com/coderabbitai/e2e-reviewer).",
        "labels": [],
        "commits": 1,
        "additions": 156,
        "deletions": 328,
        "changed_files": 1,
        "comments": [
            {
                "user": "coderabbitai[bot]",
                "created_at": "2024-10-25T11:04:30+00:00",
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes introduce a new routing table system to replace the existing FastAPI server implementation. The core of this change is the addition of a `CommonRoutingTableImpl` base class that manages an in-memory registry for routable objects and their associated providers. The system implements three specialized routing tables that inherit from this base class: `ModelsRoutingTable`, `ShieldsRoutingTable`, and `MemoryBanksRoutingTable`.\n\nThe implementation includes new functionality for registry initialization, object registration, object lookup by identifier, listing objects by type, and provider implementation retrieval. A new type `Registry` is defined as a dictionary mapping strings to lists of routable objects with their providers. The system includes utility functions like `get_impl_api` for retrieving API implementations and `register_object_with_provider` for handling object registration with specific providers.\n\nEach specialized routing table implements its respective interface (Models, Shields, or MemoryBanks) while inheriting common functionality from the base class. The routing system maintains associations between objects and their providers in an in-memory registry.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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=\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T11:06:19+00:00",
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider extracting common patterns into a generic base class.**\n\nThe ModelsRoutingTable and ShieldsRoutingTable share similar implementation patterns. Consider using a generic base class to reduce code duplication.\n\n\n```python\nfrom typing import TypeVar, Generic\n\nT = TypeVar('T', bound=RoutableObjectWithProvider)\n\nclass TypedRoutingTable(CommonRoutingTableImpl, Generic[T]):\n    async def list_objects(self) -> List[T]:\n        objects = []\n        for objs in self.registry.values():\n            objects.extend(objs)\n        return objects\n\n    async def get_object(self, identifier: str) -> Optional[T]:\n        return self.get_object_by_identifier(identifier)\n\n    async def register_object(self, obj: T) -> None:\n        await self.register_object(obj)\n\nclass ModelsRoutingTable(TypedRoutingTable[ModelDefWithProvider], Models):\n    list_models = TypedRoutingTable.list_objects\n    get_model = TypedRoutingTable.get_object\n    register_model = TypedRoutingTable.register_object\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 479,
                "line_range": "Comment on lines +479 to +479"
            },
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T11:06:19+00:00",
                "body": "_:warning: Potential issue_\n\n**Add validation and atomic operations for object registration.**\n\nThe current implementation has several potential issues:\n1. No validation for duplicate objects\n2. Arbitrary provider selection without clear criteria\n3. Direct registry modification without rollback capability\n\n\nConsider implementing these improvements:\n```diff\n async def register_object(self, obj: RoutableObjectWithProvider):\n+    async with self._lock:\n+        # Check for duplicates\n+        if obj.identifier in self.registry:\n+            existing = self.registry[obj.identifier]\n+            if any(x.provider_id == obj.provider_id for x in existing):\n+                raise ValueError(f\"Object {obj.identifier} already registered with provider {obj.provider_id}\")\n+\n         # if provider_id is not specified, we'll pick an arbitrary one from existing entries\n         if not obj.provider_id and len(self.impls_by_provider_id) > 0:\n-            obj.provider_id = list(self.impls_by_provider_id.keys())[0]\n+            # Select provider based on load or capability\n+            obj.provider_id = await self._select_optimal_provider()\n\n         if obj.provider_id not in self.impls_by_provider_id:\n             raise ValueError(f\"Provider `{obj.provider_id}` not found\")\n         \n         try:\n             p = self.impls_by_provider_id[obj.provider_id]\n             await register_object_with_provider(obj, p)\n             \n             if obj.identifier not in self.registry:\n                 self.registry[obj.identifier] = []\n             self.registry[obj.identifier].append(obj)\n+        except Exception as e:\n+            # Rollback on failure\n+            if obj.identifier in self.registry:\n+                self.registry[obj.identifier] = [x for x in self.registry[obj.identifier] if x.provider_id != obj.provider_id]\n+            raise e\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 451,
                "line_range": "Comment on lines +451 to +451"
            },
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T11:06:19+00:00",
                "body": "_:warning: Potential issue_\n\n**Consider thread safety in initialization logic.**\n\nThe initialization process modifies shared state without apparent synchronization mechanisms. This could lead to race conditions if multiple providers are initialized concurrently.\n\nConsider adding synchronization:\n\n```diff\n async def initialize(self) -> None:\n+    self._lock = asyncio.Lock()  # Add at __init__\n     self.registry: Registry = {}\n \n     def add_objects(objs: List[RoutableObjectWithProvider]) -> None:\n+        async with self._lock:\n             for obj in objs:\n                 if obj.identifier not in self.registry:\n                     self.registry[obj.identifier] = []\n                 self.registry[obj.identifier].append(obj)\n```\n\nAlso, the in-place modification of memory banks at line 79 could be problematic:\n```diff\n-    m.provider_id = pid\n+    updated_bank = m.copy()\n+    updated_bank.provider_id = pid\n+    return updated_bank\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 407,
                "line_range": "Comment on lines +407 to +407"
            }
        ],
        "reviews": [
            {
                "user": "nehal-a2z",
                "state": "COMMENTED",
                "submitted_at": "2024-10-25T11:06:20+00:00",
                "body": "**Actionable comments posted: 3**\n\n<details>\n<summary>\ud83e\uddf9 Outside diff range and nitpick comments (2)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/distribution/server/server.py (2)</summary><blockquote>\n\n`9-13`: **Consider replacing wildcard imports with explicit imports.**\n\nUsing `*` imports can lead to namespace pollution and make it harder to track dependencies. Consider explicitly importing only the needed symbols.\n\n```diff\n-from llama_models.llama3.api.datatypes import *  # noqa: F403\n-from llama_stack.apis.models import *  # noqa: F403\n-from llama_stack.apis.shields import *  # noqa: F403\n-from llama_stack.apis.memory_banks import *  # noqa: F403\n+from llama_models.llama3.api.datatypes import Api\n+from llama_stack.apis.models import Models, ModelDefWithProvider\n+from llama_stack.apis.shields import Shields, ShieldDefWithProvider\n+from llama_stack.apis.memory_banks import MemoryBanks, MemoryBankDefWithProvider\n```\n\n---\n\n`156-171`: **Implementation looks good, consider generic refactoring.**\n\nThe implementation is correct but could benefit from the generic base class refactoring suggested above to maintain consistency across all routing tables.\n\n</blockquote></details>\n\n</blockquote></details>\n\n<details>\n<summary>\ud83d\udcdc Review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and 347e2569405a5a9af9a27355b10bc1c676bf566b.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/distribution/server/server.py (1 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n<details>\n<summary>\ud83d\udd07 Additional comments (1)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/distribution/server/server.py (1)</summary><blockquote>\n\n`35-36`: **Critical: Address persistence TODO before production deployment.**\n\nThe in-memory state without persistence could lead to data loss on service restart. Consider implementing a persistent storage solution (e.g., Redis, PostgreSQL) for the routing table.\n\n\n\n```shell\n#!/bin/bash\n# Check if there are any other TODOs related to persistence\nrg -i \"todo.*persist\" \n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit for review status -->"
            }
        ],
        "requested_reviewers": [],
        "hunks": [],
        "commits_data": [
            {
                "sha": "347e2569405a5a9af9a27355b10bc1c676bf566b",
                "author": "nehal-a2z",
                "message": "Add error pattern 017 to multiple files",
                "date": "2024-10-23T19:26:11+00:00",
                "files_changed": [
                    {
                        "filename": "project/api/llama_stack/distribution/server/server.py",
                        "additions": 156,
                        "deletions": 328,
                        "changes": 484,
                        "status": "modified"
                    }
                ]
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/distribution/server/server.py",
                "hunks": [
                    {
                        "id": 1,
                        "old_start": 4,
                        "old_count": 340,
                        "new_start": 4,
                        "new_count": 168,
                        "content": " # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n \n-import asyncio\n-import functools\n-import inspect\n-import json\n-import signal\n-import traceback\n-\n-from contextlib import asynccontextmanager\n-from ssl import SSLError\n-from typing import Any, Dict, Optional\n-\n-import fire\n-import httpx\n-import yaml\n-\n-from fastapi import Body, FastAPI, HTTPException, Request, Response\n-from fastapi.exceptions import RequestValidationError\n-from fastapi.responses import JSONResponse, StreamingResponse\n-from pydantic import BaseModel, ValidationError\n-from termcolor import cprint\n-from typing_extensions import Annotated\n-\n-from llama_stack.distribution.distribution import builtin_automatically_routed_apis\n-\n-from llama_stack.providers.utils.telemetry.tracing import (\n-    end_trace,\n-    setup_logger,\n-    SpanStatus,\n-    start_trace,\n-)\n+from typing import Any, Dict, List, Optional\n+\n+from llama_models.llama3.api.datatypes import *  # noqa: F403\n+\n+from llama_stack.apis.models import *  # noqa: F403\n+from llama_stack.apis.shields import *  # noqa: F403\n+from llama_stack.apis.memory_banks import *  # noqa: F403\n+\n from llama_stack.distribution.datatypes import *  # noqa: F403\n \n-from llama_stack.distribution.request_headers import set_request_provider_data\n-from llama_stack.distribution.resolver import resolve_impls_with_routing\n-\n-from .endpoints import get_all_api_endpoints\n-\n-\n-def create_sse_event(data: Any) -> str:\n-    if isinstance(data, BaseModel):\n-        data = data.json()\n-    else:\n-        data = json.dumps(data)\n-\n-    return f\"data: {data}\\n\\n\"\n-\n-\n-async def global_exception_handler(request: Request, exc: Exception):\n-    traceback.print_exception(exc)\n-    http_exc = translate_exception(exc)\n-\n-    return JSONResponse(\n-        status_code=http_exc.status_code, content={\"error\": {\"detail\": http_exc.detail}}\n-    )\n-\n-\n-def translate_exception(exc: Exception) -> Union[HTTPException, RequestValidationError]:\n-    if isinstance(exc, ValidationError):\n-        exc = RequestValidationError(exc.raw_errors)\n-\n-    if isinstance(exc, RequestValidationError):\n-        return HTTPException(\n-            status_code=400,\n-            detail={\n-                \"errors\": [\n-                    {\n-                        \"loc\": list(error[\"loc\"]),\n-                        \"msg\": error[\"msg\"],\n-                        \"type\": error[\"type\"],\n-                    }\n-                    for error in exc.errors()\n-                ]\n-            },\n-        )\n-    elif isinstance(exc, ValueError):\n-        return HTTPException(status_code=400, detail=f\"Invalid value: {str(exc)}\")\n-    elif isinstance(exc, PermissionError):\n-        return HTTPException(status_code=403, detail=f\"Permission denied: {str(exc)}\")\n-    elif isinstance(exc, TimeoutError):\n-        return HTTPException(status_code=504, detail=f\"Operation timed out: {str(exc)}\")\n-    elif isinstance(exc, NotImplementedError):\n-        return HTTPException(status_code=501, detail=f\"Not implemented: {str(exc)}\")\n-    else:\n-        return HTTPException(\n-            status_code=500,\n-            detail=\"Internal server error: An unexpected error occurred.\",\n-        )\n-\n-\n-async def passthrough(\n-    request: Request,\n-    downstream_url: str,\n-    downstream_headers: Optional[Dict[str, str]] = None,\n-):\n-    await start_trace(request.path, {\"downstream_url\": downstream_url})\n-\n-    headers = dict(request.headers)\n-    headers.pop(\"host\", None)\n-    headers.update(downstream_headers or {})\n-\n-    content = await request.body()\n-\n-    client = httpx.AsyncClient()\n-    erred = False\n-    try:\n-        req = client.build_request(\n-            method=request.method,\n-            url=downstream_url,\n-            headers=headers,\n-            content=content,\n-            params=request.query_params,\n-        )\n-        response = await client.send(req, stream=True)\n-\n-        async def stream_response():\n-            async for chunk in response.aiter_raw(chunk_size=64):\n-                yield chunk\n-\n-            await response.aclose()\n-            await client.aclose()\n-\n-        return StreamingResponse(\n-            stream_response(),\n-            status_code=response.status_code,\n-            headers=dict(response.headers),\n-            media_type=response.headers.get(\"content-type\"),\n-        )\n-\n-    except httpx.ReadTimeout:\n-        erred = True\n-        return Response(content=\"Downstream server timed out\", status_code=504)\n-    except httpx.NetworkError as e:\n-        erred = True\n-        return Response(content=f\"Network error: {str(e)}\", status_code=502)\n-    except httpx.TooManyRedirects:\n-        erred = True\n-        return Response(content=\"Too many redirects\", status_code=502)\n-    except SSLError as e:\n-        erred = True\n-        return Response(content=f\"SSL error: {str(e)}\", status_code=502)\n-    except httpx.HTTPStatusError as e:\n-        erred = True\n-        return Response(content=str(e), status_code=e.response.status_code)\n-    except Exception as e:\n-        erred = True\n-        return Response(content=f\"Unexpected error: {str(e)}\", status_code=500)\n-    finally:\n-        await end_trace(SpanStatus.OK if not erred else SpanStatus.ERROR)\n-\n-\n-def handle_sigint(app, *args, **kwargs):\n-    print(\"SIGINT or CTRL-C detected. Exiting gracefully...\")\n-\n-    async def run_shutdown():\n-        for impl in app.__llama_stack_impls__.values():\n-            print(f\"Shutting down {impl}\")\n-            await impl.shutdown()\n-\n-    asyncio.run(run_shutdown())\n-\n-    loop = asyncio.get_event_loop()\n-    for task in asyncio.all_tasks(loop):\n-        task.cancel()\n-\n-    loop.stop()\n-\n-\n-@asynccontextmanager\n-async def lifespan(app: FastAPI):\n-    print(\"Starting up\")\n-    yield\n-\n-    print(\"Shutting down\")\n-    for impl in app.__llama_stack_impls__.values():\n-        await impl.shutdown()\n-\n-\n-def create_dynamic_passthrough(\n-    downstream_url: str, downstream_headers: Optional[Dict[str, str]] = None\n-):\n-    async def endpoint(request: Request):\n-        return await passthrough(request, downstream_url, downstream_headers)\n-\n-    return endpoint\n-\n-\n-def is_streaming_request(func_name: str, request: Request, **kwargs):\n-    # TODO: pass the api method and punt it to the Protocol definition directly\n-    return kwargs.get(\"stream\", False)\n-\n-\n-async def maybe_await(value):\n-    if inspect.iscoroutine(value):\n-        return await value\n-    return value\n-\n-\n-async def sse_generator(event_gen):\n-    try:\n-        async for item in event_gen:\n-            yield create_sse_event(item)\n-            await asyncio.sleep(0.01)\n-    except asyncio.CancelledError:\n-        print(\"Generator cancelled\")\n-        await event_gen.aclose()\n-    except Exception as e:\n-        traceback.print_exception(e)\n-        yield create_sse_event(\n-            {\n-                \"error\": {\n-                    \"message\": str(translate_exception(e)),\n-                },\n-            }\n-        )\n-    finally:\n-        await end_trace()\n-\n-\n-def create_dynamic_typed_route(func: Any, method: str):\n-\n-    async def endpoint(request: Request, **kwargs):\n-        await start_trace(func.__name__)\n-\n-        set_request_provider_data(request.headers)\n-\n-        is_streaming = is_streaming_request(func.__name__, request, **kwargs)\n-        try:\n-            if is_streaming:\n-                return StreamingResponse(\n-                    sse_generator(func(**kwargs)), media_type=\"text/event-stream\"\n-                )\n-            else:\n-                value = func(**kwargs)\n-                return await maybe_await(value)\n-        except Exception as e:\n-            traceback.print_exception(e)\n-            raise translate_exception(e) from e\n-        finally:\n-            await end_trace()\n-\n-    sig = inspect.signature(func)\n-    new_params = [\n-        inspect.Parameter(\n-            \"request\", inspect.Parameter.POSITIONAL_OR_KEYWORD, annotation=Request\n-        )\n-    ]\n-    new_params.extend(sig.parameters.values())\n-\n-    if method == \"post\":\n-        # make sure every parameter is annotated with Body() so FASTAPI doesn't\n-        # do anything too intelligent and ask for some parameters in the query\n-        # and some in the body\n-        new_params = [new_params[0]] + [\n-            param.replace(annotation=Annotated[param.annotation, Body(..., embed=True)])\n-            for param in new_params[1:]\n-        ]\n-\n-    endpoint.__signature__ = sig.replace(parameters=new_params)\n-\n-    return endpoint\n-\n-\n-def main(\n-    yaml_config: str = \"llamastack-run.yaml\",\n-    port: int = 5000,\n-    disable_ipv6: bool = False,\n-):\n-    with open(yaml_config, \"r\") as fp:\n-        config = StackRunConfig(**yaml.safe_load(fp))\n-\n-    app = FastAPI()\n-\n-    impls = asyncio.run(resolve_impls_with_routing(config))\n-    if Api.telemetry in impls:\n-        setup_logger(impls[Api.telemetry])\n-\n-    all_endpoints = get_all_api_endpoints()\n-\n-    if config.apis:\n-        apis_to_serve = set(config.apis)\n-    else:\n-        apis_to_serve = set(impls.keys())\n-\n-    for inf in builtin_automatically_routed_apis():\n-        apis_to_serve.add(inf.routing_table_api.value)\n-\n-    apis_to_serve.add(\"inspect\")\n-    for api_str in apis_to_serve:\n-        api = Api(api_str)\n-\n-        endpoints = all_endpoints[api]\n-        impl = impls[api]\n-\n-        if is_passthrough(impl.__provider_spec__):\n-            for endpoint in endpoints:\n-                url = impl.__provider_config__.url.rstrip(\"/\") + endpoint.route\n-                getattr(app, endpoint.method)(endpoint.route)(\n-                    create_dynamic_passthrough(url)\n-                )\n-        else:\n-            for endpoint in endpoints:\n-                if not hasattr(impl, endpoint.name):\n-                    # ideally this should be a typing violation already\n-                    raise ValueError(\n-                        f\"Could not find method {endpoint.name} on {impl}!!\"\n-                    )\n-\n-                impl_method = getattr(impl, endpoint.name)\n-\n-                getattr(app, endpoint.method)(endpoint.route, response_model=None)(\n-                    create_dynamic_typed_route(\n-                        impl_method,\n-                        endpoint.method,\n-                    )\n-                )\n \n-        cprint(f\"Serving API {api_str}\", \"white\", attrs=[\"bold\"])\n-        for endpoint in endpoints:\n-            cprint(f\" {endpoint.method.upper()} {endpoint.route}\", \"white\")\n+def get_impl_api(p: Any) -> Api:\n+    return p.__provider_spec__.api\n+\n \n-    print(\"\")\n-    app.exception_handler(RequestValidationError)(global_exception_handler)\n-    app.exception_handler(Exception)(global_exception_handler)\n-    signal.signal(signal.SIGINT, functools.partial(handle_sigint, app))\n+async def register_object_with_provider(obj: RoutableObject, p: Any) -> None:\n+    api = get_impl_api(p)\n+    if api == Api.inference:\n+        await p.register_model(obj)\n+    elif api == Api.safety:\n+        await p.register_shield(obj)\n+    elif api == Api.memory:\n+        await p.register_memory_bank(obj)\n \n-    app.__llama_stack_impls__ = impls\n \n-    import uvicorn\n+Registry = Dict[str, List[RoutableObjectWithProvider]]\n \n-    # FYI this does not do hot-reloads\n-    listen_host = \"::\" if not disable_ipv6 else \"0.0.0.0\"\n-    print(f\"Listening on {listen_host}:{port}\")\n-    uvicorn.run(app, host=listen_host, port=port)\n \n+# TODO: this routing table maintains state in memory purely. We need to\n+# add persistence to it when we add dynamic registration of objects.\n+class CommonRoutingTableImpl(RoutingTable):\n+    def __init__(\n+        self,\n+        impls_by_provider_id: Dict[str, RoutedProtocol],\n+    ) -> None:\n+        self.impls_by_provider_id = impls_by_provider_id\n+\n+    async def initialize(self) -> None:\n+        self.registry: Registry = {}\n+\n+        def add_objects(objs: List[RoutableObjectWithProvider]) -> None:\n+            for obj in objs:\n+                if obj.identifier not in self.registry:\n+                    self.registry[obj.identifier] = []\n+\n+                self.registry[obj.identifier].append(obj)\n+\n+        for pid, p in self.impls_by_provider_id.items():\n+            api = get_impl_api(p)\n+            if api == Api.inference:\n+                p.model_store = self\n+                models = await p.list_models()\n+                add_objects(\n+                    [ModelDefWithProvider(**m.dict(), provider_id=pid) for m in models]\n+                )\n+\n+            elif api == Api.safety:\n+                p.shield_store = self\n+                shields = await p.list_shields()\n+                add_objects(\n+                    [\n+                        ShieldDefWithProvider(**s.dict(), provider_id=pid)\n+                        for s in shields\n+                    ]\n+                )\n \n-if __name__ == \"__main__\":\n-    fire.Fire(main)\n+            elif api == Api.memory:\n+                p.memory_bank_store = self\n+                memory_banks = await p.list_memory_banks()\n+\n+                # do in-memory updates due to pesky Annotated unions\n+                for m in memory_banks:\n+                    m.provider_id = pid\n+\n+                add_objects(memory_banks)\n+\n+    async def shutdown(self) -> None:\n+        for p in self.impls_by_provider_id.values():\n+            await p.shutdown()\n+\n+    def get_provider_impl(\n+        self, routing_key: str, provider_id: Optional[str] = None\n+    ) -> Any:\n+        if routing_key not in self.registry:\n+            raise ValueError(f\"`{routing_key}` not registered\")\n+\n+        objs = self.registry[routing_key]\n+        for obj in objs:\n+            if not provider_id or provider_id == obj.provider_id:\n+                return self.impls_by_provider_id[obj.provider_id]\n+\n+        raise ValueError(f\"Provider not found for `{routing_key}`\")\n+\n+    def get_object_by_identifier(\n+        self, identifier: str\n+    ) -> Optional[RoutableObjectWithProvider]:\n+        objs = self.registry.get(identifier, [])\n+        if not objs:\n+            return None\n+\n+        # kind of ill-defined behavior here, but we'll just return the first one\n+        return objs[0]\n+\n+    async def register_object(self, obj: RoutableObjectWithProvider):\n+        # if provider_id is not specified, we'll pick an arbitrary one from existing entries\n+        if not obj.provider_id and len(self.impls_by_provider_id) > 0:\n+            obj.provider_id = list(self.impls_by_provider_id.keys())[0]\n+\n+        if obj.provider_id not in self.impls_by_provider_id:\n+            raise ValueError(f\"Provider `{obj.provider_id}` not found\")\n+        \n+        p = self.impls_by_provider_id[obj.provider_id]\n+        await register_object_with_provider(obj, p)\n+        \n+        # Direct registry modification without checking current state\n+        if obj.identifier not in self.registry:\n+            self.registry[obj.identifier] = []\n+        self.registry[obj.identifier].append(obj)\n+        # TODO: persist this to a store\n+\n+\n+class ModelsRoutingTable(CommonRoutingTableImpl, Models):\n+    async def list_models(self) -> List[ModelDefWithProvider]:\n+        objects = []\n+        for objs in self.registry.values():\n+            objects.extend(objs)\n+        return objects\n+\n+    async def get_model(self, identifier: str) -> Optional[ModelDefWithProvider]:\n+        return self.get_object_by_identifier(identifier)\n+\n+    async def register_model(self, model: ModelDefWithProvider) -> None:\n+        await self.register_object(model)\n+\n+\n+class ShieldsRoutingTable(CommonRoutingTableImpl, Shields):\n+    async def list_shields(self) -> List[ShieldDef]:\n+        objects = []\n+        for objs in self.registry.values():\n+            objects.extend(objs)\n+        return objects\n+\n+    async def get_shield(self, shield_type: str) -> Optional[ShieldDefWithProvider]:\n+        return self.get_object_by_identifier(shield_type)\n+\n+    async def register_shield(self, shield: ShieldDefWithProvider) -> None:\n+        await self.register_object(shield)\n+\n+\n+class MemoryBanksRoutingTable(CommonRoutingTableImpl, MemoryBanks):\n+    async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]:\n+        objects = []\n+        for objs in self.registry.values():\n+            objects.extend(objs)\n+        return objects\n+\n+    async def get_memory_bank(\n+        self, identifier: str\n+    ) -> Optional[MemoryBankDefWithProvider]:\n+        return self.get_object_by_identifier(identifier)\n+\n+    async def register_memory_bank(\n+        self, memory_bank: MemoryBankDefWithProvider\n+    ) -> None:\n+        await self.register_object(memory_bank)\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "32",
        "type": "6",
        "number": 164,
        "base_branch": "main-copy",
        "head_branch": "error-040-api",
        "title": "Automated Test: @eval-allprompt-newsonnet",
        "user": "nehal-a2z",
        "state": "closed",
        "created_at": "2024-10-25T11:00:59+00:00",
        "closed_at": "2024-10-25T11:04:22+00:00",
        "merged_at": null,
        "merge_commit_sha": "72ec81118c374ad90e010bee04ce56459c8ae43e",
        "body": "This pull request was automatically created by [`@coderabbitai/e2e-reviewer`](https://github.com/coderabbitai/e2e-reviewer).",
        "labels": [],
        "commits": 1,
        "additions": 456,
        "deletions": 82,
        "changed_files": 1,
        "comments": [
            {
                "user": "coderabbitai[bot]",
                "created_at": "2024-10-25T11:01:06+00:00",
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes involve a complete replacement of a Together-based safety implementation with a new agent-based system architecture. The code introduces a comprehensive set of data models and protocols for managing agent interactions. The implementation includes data structures for various agent tools like SearchTool, WolframAlpha, Photogen, and CodeInterpreter. The memory system is structured to support multiple types including vector, key-value, keyword, and graph memories.\n\nThe architecture defines models for tracking different operation types through step tracking models that handle inference, tool execution, shield calls, and memory retrieval. Turn and Session models are implemented to manage agent interactions. The codebase establishes agent configuration models that incorporate sampling parameters, shields, and tools.\n\nThe implementation uses Pydantic models for type validation and employs discriminated unions for handling polymorphic types. A new Agents Protocol is defined with methods for agent creation, turn management, session management, and deletion operations. Multiple enums are introduced to define agent tools, search engine types, step types, turn response events, and memory query generators. The previous TogetherSafetyImpl class has been removed from the codebase.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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=\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T11:04:15+00:00",
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider adding validation for step timestamps**\n\nThe `StepCommon` class could benefit from validation to ensure:\n- `completed_at` is not earlier than `started_at`\n- Both timestamps are within reasonable bounds\n\n```diff\n class StepCommon(BaseModel):\n     turn_id: str\n     step_id: str\n     started_at: Optional[datetime] = None\n     completed_at: Optional[datetime] = None\n+    \n+    @field_validator('completed_at')\n+    def validate_completed_at(cls, v, values):\n+        if v and values.get('started_at'):\n+            if v < values['started_at']:\n+                raise ValueError('completed_at must be after started_at')\n+        return v\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "position": 257,
                "line_range": "Comment on lines +257 to +257"
            },
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T11:04:15+00:00",
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Add validation for max_infer_iters**\n\nConsider adding validation to ensure `max_infer_iters` is within reasonable bounds to prevent potential infinite loops or resource exhaustion.\n\n```diff\n class AgentConfigCommon(BaseModel):\n     sampling_params: Optional[SamplingParams] = SamplingParams()\n     input_shields: Optional[List[str]] = Field(default_factory=list)\n     output_shields: Optional[List[str]] = Field(default_factory=list)\n     tools: Optional[List[AgentToolDefinition]] = Field(default_factory=list)\n     tool_choice: Optional[ToolChoice] = Field(default=ToolChoice.auto)\n     tool_prompt_format: Optional[ToolPromptFormat] = Field(\n         default=ToolPromptFormat.json\n     )\n-    max_infer_iters: int = 10\n+    max_infer_iters: int = Field(default=10, ge=1, le=100)\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\nclass AgentConfigCommon(BaseModel):\n    sampling_params: Optional[SamplingParams] = SamplingParams()\n\n    input_shields: Optional[List[str]] = Field(default_factory=list)\n    output_shields: Optional[List[str]] = Field(default_factory=list)\n\n    tools: Optional[List[AgentToolDefinition]] = Field(default_factory=list)\n    tool_choice: Optional[ToolChoice] = Field(default=ToolChoice.auto)\n    tool_prompt_format: Optional[ToolPromptFormat] = Field(\n        default=ToolPromptFormat.json\n    )\n\n    max_infer_iters: int = Field(default=10, ge=1, le=100)\n\n\n@json_schema_type\nclass AgentConfig(AgentConfigCommon):\n    model: str\n    instructions: str\n    enable_session_persistence: bool\n\n\nclass AgentConfigOverridablePerTurn(AgentConfigCommon):\n    instructions: Optional[str] = None\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "position": 323,
                "line_range": "Comment on lines +323 to +323"
            }
        ],
        "reviews": [
            {
                "user": "nehal-a2z",
                "state": "COMMENTED",
                "submitted_at": "2024-10-25T11:04:16+00:00",
                "body": "**Actionable comments posted: 2**\n\n<details>\n<summary>\ud83e\uddf9 Outside diff range and nitpick comments (5)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/adapters/safety/together/together.py (5)</summary><blockquote>\n\n`58-66`: **Consider improving SearchToolDefinition documentation and defaults**\n\nThe TODO comment indicates a placeholder implementation. Additionally, consider:\n1. Adding docstring explaining the purpose and usage of this tool\n2. Making the default engine choice more explicit in the code\n\n---\n\n`97-127`: **Add documentation for memory bank configurations**\n\nThe memory bank configuration classes would benefit from docstrings explaining:\n- Purpose and use cases for each memory type\n- Expected format and constraints for the configuration parameters\n- Examples of typical usage\n\n---\n\n`392-395`: **Address TODO comment regarding ToolResponseMessage**\n\nThe TODO comment indicates uncertainty about the ToolResponseMessage usage. This should be clarified and documented.\n\nWould you like me to help analyze the ToolResponseMessage usage pattern in the codebase and propose a simplified approach?\n\n---\n\n`416-475`: **Document error handling for API endpoints**\n\nThe `Agents` protocol would benefit from documentation describing:\n- Expected error responses for each endpoint\n- Rate limiting behavior\n- Retry strategies\n\n---\n\nLine range hint `1-475`: **Consider implementing request/response logging middleware**\n\nGiven the complexity of the agent system and multiple steps involved in processing, consider adding a middleware layer for:\n- Request/response logging for debugging\n- Performance monitoring\n- Error tracking\nThis will help with system observability and debugging in production.\n\n</blockquote></details>\n\n</blockquote></details>\n\n<details>\n<summary>\ud83d\udcdc Review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and e9c2e74dd28d41b632942146e544aa897aa9e9db.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/providers/adapters/safety/together/together.py (1 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n<details>\n<summary>\ud83d\udd07 Additional comments (2)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/adapters/safety/together/together.py (2)</summary><blockquote>\n\n`7-30`: **LGTM: Well-organized imports with proper type hints**\n\nThe imports are clean, specific, and make good use of type hints and annotations.\n\n---\n\n`244-274`: **Consider validating memory bank configuration**\n\nThe `Session` model has an optional `memory_bank` field but there's no validation to ensure it matches the memory bank configurations defined in the tools. Consider adding validation to maintain consistency.\n\n```shell\n#!/bin/bash\n# Search for memory bank validation in the codebase\nrg -l \"memory_bank.*validate\" \n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit for review status -->"
            }
        ],
        "requested_reviewers": [],
        "hunks": [],
        "commits_data": [
            {
                "sha": "e9c2e74dd28d41b632942146e544aa897aa9e9db",
                "author": "nehal-a2z",
                "message": "Add error pattern 040 to multiple files",
                "date": "2024-10-23T19:26:59+00:00",
                "files_changed": [
                    {
                        "filename": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                        "additions": 456,
                        "deletions": 82,
                        "changes": 538,
                        "status": "modified"
                    }
                ]
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "hunks": [
                    {
                        "id": 1,
                        "old_start": 3,
                        "old_count": 99,
                        "new_start": 3,
                        "new_count": 473,
                        "content": " #\n # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n-from together import Together\n+\n+from datetime import datetime\n+from enum import Enum\n+from typing import (\n+    Any,\n+    Dict,\n+    List,\n+    Literal,\n+    Optional,\n+    Protocol,\n+    runtime_checkable,\n+    Union,\n+)\n+\n+from llama_models.schema_utils import json_schema_type, webmethod\n+\n+from pydantic import BaseModel, ConfigDict, Field\n+from typing_extensions import Annotated\n \n from llama_models.llama3.api.datatypes import *  # noqa: F403\n+from llama_stack.apis.common.deployment_types import *  # noqa: F403\n+from llama_stack.apis.inference import *  # noqa: F403\n from llama_stack.apis.safety import *  # noqa: F403\n-from llama_stack.distribution.request_headers import NeedsRequestProviderData\n-from llama_stack.providers.datatypes import ShieldsProtocolPrivate\n+from llama_stack.apis.memory import *  # noqa: F403\n+\n+\n+@json_schema_type\n+class Attachment(BaseModel):\n+    content: InterleavedTextMedia | URL\n+    mime_type: str\n+\n+\n+class AgentTool(Enum):\n+    brave_search = \"brave_search\"\n+    wolfram_alpha = \"wolfram_alpha\"\n+    photogen = \"photogen\"\n+    code_interpreter = \"code_interpreter\"\n+\n+    function_call = \"function_call\"\n+    memory = \"memory\"\n+\n+\n+class ToolDefinitionCommon(BaseModel):\n+    input_shields: Optional[List[str]] = Field(default_factory=list)\n+    output_shields: Optional[List[str]] = Field(default_factory=list)\n+\n+\n+class SearchEngineType(Enum):\n+    bing = \"bing\"\n+    brave = \"brave\"\n+\n+\n+@json_schema_type\n+class SearchToolDefinition(ToolDefinitionCommon):\n+    # NOTE: brave_search is just a placeholder since model always uses\n+    # brave_search as tool call name\n+    type: Literal[AgentTool.brave_search.value] = AgentTool.brave_search.value\n+    api_key: str\n+    engine: SearchEngineType = SearchEngineType.brave\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+@json_schema_type\n+class WolframAlphaToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.wolfram_alpha.value] = AgentTool.wolfram_alpha.value\n+    api_key: str\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+@json_schema_type\n+class PhotogenToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.photogen.value] = AgentTool.photogen.value\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+@json_schema_type\n+class CodeInterpreterToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.code_interpreter.value] = AgentTool.code_interpreter.value\n+    enable_inline_code_execution: bool = True\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+@json_schema_type\n+class FunctionCallToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.function_call.value] = AgentTool.function_call.value\n+    function_name: str\n+    description: str\n+    parameters: Dict[str, ToolParamDefinition]\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+class _MemoryBankConfigCommon(BaseModel):\n+    bank_id: str\n+\n+\n+class AgentVectorMemoryBankConfig(_MemoryBankConfigCommon):\n+    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n+\n+\n+class AgentKeyValueMemoryBankConfig(_MemoryBankConfigCommon):\n+    type: Literal[MemoryBankType.keyvalue.value] = MemoryBankType.keyvalue.value\n+    keys: List[str]  # what keys to focus on\n+\n+\n+class AgentKeywordMemoryBankConfig(_MemoryBankConfigCommon):\n+    type: Literal[MemoryBankType.keyword.value] = MemoryBankType.keyword.value\n+\n+\n+class AgentGraphMemoryBankConfig(_MemoryBankConfigCommon):\n+    type: Literal[MemoryBankType.graph.value] = MemoryBankType.graph.value\n+    entities: List[str]  # what entities to focus on\n+\n+\n+MemoryBankConfig = Annotated[\n+    Union[\n+        AgentVectorMemoryBankConfig,\n+        AgentKeyValueMemoryBankConfig,\n+        AgentKeywordMemoryBankConfig,\n+        AgentGraphMemoryBankConfig,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+\n+class MemoryQueryGenerator(Enum):\n+    default = \"default\"\n+    llm = \"llm\"\n+    custom = \"custom\"\n+\n+\n+class DefaultMemoryQueryGeneratorConfig(BaseModel):\n+    type: Literal[MemoryQueryGenerator.default.value] = (\n+        MemoryQueryGenerator.default.value\n+    )\n+    sep: str = \" \"\n+\n+\n+class LLMMemoryQueryGeneratorConfig(BaseModel):\n+    type: Literal[MemoryQueryGenerator.llm.value] = MemoryQueryGenerator.llm.value\n+    model: str\n+    template: str\n+\n+\n+class CustomMemoryQueryGeneratorConfig(BaseModel):\n+    type: Literal[MemoryQueryGenerator.custom.value] = MemoryQueryGenerator.custom.value\n+\n+\n+MemoryQueryGeneratorConfig = Annotated[\n+    Union[\n+        DefaultMemoryQueryGeneratorConfig,\n+        LLMMemoryQueryGeneratorConfig,\n+        CustomMemoryQueryGeneratorConfig,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+\n+@json_schema_type\n+class MemoryToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.memory.value] = AgentTool.memory.value\n+    memory_bank_configs: List[MemoryBankConfig] = Field(default_factory=list)\n+    # This config defines how a query is generated using the messages\n+    # for memory bank retrieval.\n+    query_generator_config: MemoryQueryGeneratorConfig = Field(\n+        default=DefaultMemoryQueryGeneratorConfig()\n+    )\n+    max_tokens_in_context: int = 4096\n+    max_chunks: int = 10\n+\n+\n+AgentToolDefinition = Annotated[\n+    Union[\n+        SearchToolDefinition,\n+        WolframAlphaToolDefinition,\n+        PhotogenToolDefinition,\n+        CodeInterpreterToolDefinition,\n+        FunctionCallToolDefinition,\n+        MemoryToolDefinition,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+\n+class StepCommon(BaseModel):\n+    turn_id: str\n+    step_id: str\n+    started_at: Optional[datetime] = None\n+    completed_at: Optional[datetime] = None\n+\n+\n+class StepType(Enum):\n+    inference = \"inference\"\n+    tool_execution = \"tool_execution\"\n+    shield_call = \"shield_call\"\n+    memory_retrieval = \"memory_retrieval\"\n+\n+\n+@json_schema_type\n+class InferenceStep(StepCommon):\n+    model_config = ConfigDict(protected_namespaces=())\n+\n+    step_type: Literal[StepType.inference.value] = StepType.inference.value\n+    model_response: CompletionMessage\n \n-from .config import TogetherSafetyConfig\n \n+@json_schema_type\n+class ToolExecutionStep(StepCommon):\n+    step_type: Literal[StepType.tool_execution.value] = StepType.tool_execution.value\n+    tool_calls: List[ToolCall]\n+    tool_responses: List[ToolResponse]\n \n-TOGETHER_SHIELD_MODEL_MAP = {\n-    \"llama_guard\": \"meta-llama/Meta-Llama-Guard-3-8B\",\n-    \"Llama-Guard-3-8B\": \"meta-llama/Meta-Llama-Guard-3-8B\",\n-    \"Llama-Guard-3-11B-Vision\": \"meta-llama/Llama-Guard-3-11B-Vision-Turbo\",\n-}\n \n+@json_schema_type\n+class ShieldCallStep(StepCommon):\n+    step_type: Literal[StepType.shield_call.value] = StepType.shield_call.value\n+    violation: Optional[SafetyViolation]\n \n-class TogetherSafetyImpl(Safety, NeedsRequestProviderData, ShieldsProtocolPrivate):\n-    def __init__(self, config: TogetherSafetyConfig) -> None:\n-        self.config = config\n \n-    async def initialize(self) -> None:\n-        pass\n+@json_schema_type\n+class MemoryRetrievalStep(StepCommon):\n+    step_type: Literal[StepType.memory_retrieval.value] = (\n+        StepType.memory_retrieval.value\n+    )\n+    memory_bank_ids: List[str]\n+    inserted_context: InterleavedTextMedia\n \n-    async def shutdown(self) -> None:\n-        pass\n \n-    async def register_shield(self, shield: ShieldDef) -> None:\n-        raise ValueError(\"Registering dynamic shields is not supported\")\n+Step = Annotated[\n+    Union[\n+        InferenceStep,\n+        ToolExecutionStep,\n+        ShieldCallStep,\n+        MemoryRetrievalStep,\n+    ],\n+    Field(discriminator=\"step_type\"),\n+]\n \n-    async def list_shields(self) -> List[ShieldDef]:\n-        return [\n-            ShieldDef(\n-                identifier=ShieldType.llama_guard.value,\n-                type=ShieldType.llama_guard.value,\n-                params={},\n-            )\n+\n+@json_schema_type\n+class Turn(BaseModel):\n+    \"\"\"A single turn in an interaction with an Agentic System.\"\"\"\n+\n+    turn_id: str\n+    session_id: str\n+    input_messages: List[\n+        Union[\n+            UserMessage,\n+            ToolResponseMessage,\n         ]\n+    ]\n+    steps: List[Step]\n+    output_message: CompletionMessage\n+    output_attachments: List[Attachment] = Field(default_factory=list)\n+\n+    started_at: datetime\n+    completed_at: Optional[datetime] = None\n+\n+\n+@json_schema_type\n+class Session(BaseModel):\n+    \"\"\"A single session of an interaction with an Agentic System.\"\"\"\n+\n+    session_id: str\n+    session_name: str\n+    turns: List[Turn]\n+    started_at: datetime\n+\n+    memory_bank: Optional[MemoryBankDef] = None\n+\n+\n+class AgentConfigCommon(BaseModel):\n+    sampling_params: Optional[SamplingParams] = SamplingParams()\n+\n+    input_shields: Optional[List[str]] = Field(default_factory=list)\n+    output_shields: Optional[List[str]] = Field(default_factory=list)\n+\n+    tools: Optional[List[AgentToolDefinition]] = Field(default_factory=list)\n+    tool_choice: Optional[ToolChoice] = Field(default=ToolChoice.auto)\n+    tool_prompt_format: Optional[ToolPromptFormat] = Field(\n+        default=ToolPromptFormat.json\n+    )\n+\n+    max_infer_iters: int = 10\n+\n+\n+@json_schema_type\n+class AgentConfig(AgentConfigCommon):\n+    model: str\n+    instructions: str\n+    enable_session_persistence: bool\n+\n+\n+class AgentConfigOverridablePerTurn(AgentConfigCommon):\n+    instructions: Optional[str] = None\n+\n+\n+class AgentTurnResponseEventType(Enum):\n+    step_start = \"step_start\"\n+    step_complete = \"step_complete\"\n+    step_progress = \"step_progress\"\n+\n+    turn_start = \"turn_start\"\n+    turn_complete = \"turn_complete\"\n+\n+\n+@json_schema_type\n+class AgentTurnResponseStepStartPayload(BaseModel):\n+    event_type: Literal[AgentTurnResponseEventType.step_start.value] = (\n+        AgentTurnResponseEventType.step_start.value\n+    )\n+    step_type: StepType\n+    step_id: str\n+    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)\n+\n+\n+@json_schema_type\n+class AgentTurnResponseStepCompletePayload(BaseModel):\n+    event_type: Literal[AgentTurnResponseEventType.step_complete.value] = (\n+        AgentTurnResponseEventType.step_complete.value\n+    )\n+    step_type: StepType\n+    step_details: Step\n+\n+\n+@json_schema_type\n+class AgentTurnResponseStepProgressPayload(BaseModel):\n+    model_config = ConfigDict(protected_namespaces=())\n+\n+    event_type: Literal[AgentTurnResponseEventType.step_progress.value] = (\n+        AgentTurnResponseEventType.step_progress.value\n+    )\n+    step_type: StepType\n+    step_id: str\n+\n+    model_response_text_delta: Optional[str] = None\n+    tool_call_delta: Optional[ToolCallDelta] = None\n+    tool_response_text_delta: Optional[str] = None\n+\n+\n+@json_schema_type\n+class AgentTurnResponseTurnStartPayload(BaseModel):\n+    event_type: Literal[AgentTurnResponseEventType.turn_start.value] = (\n+        AgentTurnResponseEventType.turn_start.value\n+    )\n+    turn_id: str\n+\n+\n+@json_schema_type\n+class AgentTurnResponseTurnCompletePayload(BaseModel):\n+    event_type: Literal[AgentTurnResponseEventType.turn_complete.value] = (\n+        AgentTurnResponseEventType.turn_complete.value\n+    )\n+    turn: Turn\n+\n+\n+@json_schema_type\n+class AgentTurnResponseEvent(BaseModel):\n+    \"\"\"Streamed agent execution response.\"\"\"\n+\n+    payload: Annotated[\n+        Union[\n+            AgentTurnResponseStepStartPayload,\n+            AgentTurnResponseStepProgressPayload,\n+            AgentTurnResponseStepCompletePayload,\n+            AgentTurnResponseTurnStartPayload,\n+            AgentTurnResponseTurnCompletePayload,\n+        ],\n+        Field(discriminator=\"event_type\"),\n+    ]\n+\n+\n+@json_schema_type\n+class AgentCreateResponse(BaseModel):\n+    agent_id: str\n+\n+\n+@json_schema_type\n+class AgentSessionCreateResponse(BaseModel):\n+    session_id: str\n+\n+\n+@json_schema_type\n+class AgentTurnCreateRequest(AgentConfigOverridablePerTurn):\n+    agent_id: str\n+    session_id: str\n+\n+    # TODO: figure out how we can simplify this and make why\n+    # ToolResponseMessage needs to be here (it is function call\n+    # execution from outside the system)\n+    messages: List[\n+        Union[\n+            UserMessage,\n+            ToolResponseMessage,\n+        ]\n+    ]\n+    attachments: Optional[List[Attachment]] = None\n+\n+    stream: Optional[bool] = False\n+\n+\n+@json_schema_type\n+class AgentTurnResponseStreamChunk(BaseModel):\n+    event: AgentTurnResponseEvent\n+\n+\n+@json_schema_type\n+class AgentStepResponse(BaseModel):\n+    step: Step\n+\n+\n+@runtime_checkable\n+class Agents(Protocol):\n+    @webmethod(route=\"/agents/create\")\n+    async def create_agent(\n+        self,\n+        agent_config: AgentConfig,\n+    ) -> AgentCreateResponse: ...\n+\n+    # This method is not `async def` because it can result in either an\n+    # `AsyncGenerator` or a `AgentTurnCreateResponse` depending on the value of `stream`.\n+    @webmethod(route=\"/agents/turn/create\")\n+    def create_agent_turn(\n+        self,\n+        agent_id: str,\n+        session_id: str,\n+        messages: List[\n+            Union[\n+                UserMessage,\n+                ToolResponseMessage,\n+            ]\n+        ],\n+        attachments: Optional[List[Attachment]] = None,\n+        stream: Optional[bool] = False,\n+    ) -> AgentTurnResponseStreamChunk: ...\n+\n+    @webmethod(route=\"/agents/turn/get\")\n+    async def get_agents_turn(\n+        self,\n+        agent_id: str,\n+        turn_id: str,\n+    ) -> Turn: ...\n+\n+    @webmethod(route=\"/agents/step/get\")\n+    async def get_agents_step(\n+        self, agent_id: str, turn_id: str, step_id: str\n+    ) -> AgentStepResponse: ...\n+\n+    @webmethod(route=\"/agents/session/create\")\n+    async def create_agent_session(\n+        self,\n+        agent_id: str,\n+        session_name: str,\n+    ) -> AgentSessionCreateResponse: ...\n+\n+    @webmethod(route=\"/agents/session/get\")\n+    async def get_agents_session(\n+        self,\n+        agent_id: str,\n+        session_id: str,\n+        turn_ids: Optional[List[str]] = None,\n+    ) -> Session: ...\n+\n+    @webmethod(route=\"/agents/session/delete\")\n+    async def delete_agents_session(self, agent_id: str, session_id: str) -> None: ...\n \n-    async def run_shield(\n-        self, shield_type: str, messages: List[Message], params: Dict[str, Any] = None\n-    ) -> RunShieldResponse:\n-        shield_def = await self.shield_store.get_shield(shield_type)\n-        if not shield_def:\n-            raise ValueError(f\"Unknown shield {shield_type}\")\n-\n-        model = shield_def.params.get(\"model\", \"llama_guard\")\n-        if model not in TOGETHER_SHIELD_MODEL_MAP:\n-            raise ValueError(f\"Unsupported safety model: {model}\")\n-\n-        together_api_key = None\n-        if self.config.api_key is not None:\n-            together_api_key = self.config.api_key\n-        else:\n-            provider_data = self.get_request_provider_data()\n-            if provider_data is None or not provider_data.together_api_key:\n-                raise ValueError(\n-                    'Pass Together API Key in the header X-LlamaStack-ProviderData as { \"together_api_key\": <your api key>}'\n-                )\n-            together_api_key = provider_data.together_api_key\n-\n-        # messages can have role assistant or user\n-        api_messages = []\n-        for message in messages:\n-            if message.role in (Role.user.value, Role.assistant.value):\n-                api_messages.append({\"role\": message.role, \"content\": message.content})\n-\n-        violation = await get_safety_response(\n-            together_api_key, TOGETHER_SHIELD_MODEL_MAP[model], api_messages\n-        )\n-        return RunShieldResponse(violation=violation)\n-\n-\n-async def get_safety_response(\n-    api_key: str, model_name: str, messages: List[Dict[str, str]]\n-) -> Optional[SafetyViolation]:\n-    client = Together(api_key=api_key)\n-    response = client.chat.completions.create(messages=messages, model=model_name)\n-    if len(response.choices) == 0:\n-        return None\n-\n-    response_text = response.choices[0].message.content\n-    if response_text == \"safe\":\n-        return None\n-\n-    parts = response_text.split(\"\\n\")\n-    if len(parts) != 2:\n-        return None\n-\n-    if parts[0] == \"unsafe\":\n-        return SafetyViolation(\n-            violation_level=ViolationLevel.ERROR,\n-            metadata={\"violation_type\": parts[1]},\n-        )\n-\n-    return None\n+    @webmethod(route=\"/agents/delete\")\n+    async def delete_agents(\n+        self,\n+        agent_id: str,\n+    ) -> None: ...\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "34",
        "type": "6",
        "number": 166,
        "base_branch": "main-copy",
        "head_branch": "error-037-api",
        "title": "Automated Test: @eval-allprompt-newsonnet",
        "user": "nehal-a2z",
        "state": "closed",
        "created_at": "2024-10-25T11:06:29+00:00",
        "closed_at": "2024-10-25T11:09:40+00:00",
        "merged_at": null,
        "merge_commit_sha": "92dde8c5942379db4bcbe571f859a6dd2c2ba452",
        "body": "This pull request was automatically created by [`@coderabbitai/e2e-reviewer`](https://github.com/coderabbitai/e2e-reviewer).",
        "labels": [],
        "commits": 1,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "comments": [
            {
                "user": "coderabbitai[bot]",
                "created_at": "2024-10-25T11:06:35+00:00",
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes focus on updating the API endpoint routes in the inference protocol interface. Three existing endpoints have been modified to follow a versioned path structure by changing from `/inference/*` to `/v1/*`. Specifically, the completion endpoint is now `/v1/completion`, chat completion is `/v1/chat_completion`, and embeddings endpoint is `/v1/embeddings`. The modifications only affect the route paths while maintaining all existing functionality, data models, and method signatures. The file continues to use Pydantic for defining data models related to quantization, chat completion events, tool calls, and request/response structures.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcVKkw4CzanR5sABsgyAoSAEdsaVxIAAozAEYANmSASkZw/3o5SAABclg0ILA0ACYAL0hxXCCAgCIAQTwWbJgYrjySCWLSkN4WblwwcgB3RHwMclx6jRhYEms7UbRkUnIqGnpcWAp8bCJYaoXIAAM8piUqVXVtAHoSMpIwcIl4ElHKU59WrchEWSIGjMOZwRbcYKhcJRGLobjcEguZAESACRZoaqw8JBNoo7rFbDUeAYIg8PbMIbIABm+D4GKYFMm7EYxTq9HqYwmUxIMzmADl8OhaLR1PBJsVIEpcNogsgVPtYjtFogEQx4FT4AxGEUSdJ+HwIRRuPhEItPuEyfg3kp6MTjoslIgGBR4EMxVg0bJJvQMPhxCSULg5S0/H8MNRsOENEZ9EYTFAyPR8FSfsQyMo/gy2BhOGT+MJROIpDJ5JdlDdNNpdGA9PH5p45Vg0C00xs2ln2FwqKN/k4/K5UaXFOW1JWdLHjOAwEYBiIxHc0Nx4HcQmg/AB9IFoBgAawXS8Qd2JVMoZAYJCPGBP4Qw5403FkHAM9RfBgskEaAElWxmAog+y48jJtqmCkIg7gnCKVIpswijqu8yCNFYn5hAq4LULAyB2kqKBXqet7gnsBBMKExI0BQVLbiQoK7CQiyJsaZGoXgeouIs2DcLQbRUuSZyXteZ4XgAVN8KKnHcEiJHcIlPlA4nHvh553Ay3B1OIkzfIASYR8ZJymDGp7qnAYcn8YpF4MEUuDripBkaZA2nibpFnUNZ+k8oZxl8QpN5KSQzBosKxJEIgWk6VJfkBSKJIhRBizhEC/ApjhGp1JKJAauQyA9C6+zIFx0qQLBSiyugGDbLICLINgiBBdYshcTmmoADS4QwQTYFFpJMFe8BEJGRKTNStKQFEmDiBUA0YC1zmxDZ7mTJA3TsIgLUEPgoQMKyK2lfQ0LRECdzxcaGCmv8uAUNgYiRtINGLEwFpUtgt7qeGQTqPImD0DVRDhrg13IMBOFsDsijIOEfh2k9zm6rQ0ZGG+liNEE5GTcigo4UobUuKjiWLQAHsaFB/MNEICG9WrsKK0gxpAACyPKwIozE0JAHH5XQXDnJ8AjA4ztBxHsLEALz1KZPnmW5L31GkoVcyQPMM4oAtoSLElSXNUsy7hlrEetZyfnh4tGVA9Mg7taGs5x2Sc3k3O80rgs0Kr3mCcplmuRStkYNLsu2/L9v847JCq057sa+6PvawMuuhKcBsCQRxt04r5ssZb7O0Dbdsp8rwuiy7BEPP5dCdYgkcOX7Ctm7nTui7pEUl0FZda3a0f4CR+uG4JRkvvUMY1nGU6LWVuPNgQP6bAEHY5l2aA9v+zgDrkZbXKOWjjjWdZZuo67wLQiDrq87yfLQm7SkT1aGJOECQAAHFSiR0FSADMADsAAsZQAAxv6/iQMIkV+DBaDJFoFSAAnO/FQ78GACGSAA8oVIv5/3oBOLeLBmA7z3gfI+Hw6DrkTJfIAA==\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T11:09:34+00:00",
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Document API versioning changes.**\n\nThe route change completes the consistent versioning pattern across all inference endpoints.\n\nConsider adding:\n1. API version documentation in the module docstring\n2. OpenAPI/Swagger annotations to describe the versioned endpoints\n3. Migration guide in the README\n\nLet's verify the usage of this endpoint:\n```shell\n#!/bin/bash\n# Search for direct usage of the old route to assess impact\nrg -l \"/inference/embeddings\" --type py\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/inference/inference.py",
                "position": 23,
                "line_range": "Comment on lines +23 to +23"
            }
        ],
        "reviews": [
            {
                "user": "nehal-a2z",
                "state": "COMMENTED",
                "submitted_at": "2024-10-25T11:09:34+00:00",
                "body": "**Actionable comments posted: 1**\n\n<details>\n<summary>\ud83e\uddf9 Outside diff range and nitpick comments (1)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/apis/inference/inference.py (1)</summary><blockquote>\n\nLine range hint `186-212`: **Consider comprehensive API versioning strategy.**\n\nThe consistent application of `/v1/` prefix across all endpoints is good practice, but requires additional considerations:\n\n1. Version Management:\n   - Consider adding version constants/enums\n   - Implement version negotiation mechanism\n   - Plan for future versions (v2, v3, etc.)\n\n2. Backward Compatibility:\n   - Consider supporting both old and new routes during transition\n   - Implement deprecation warnings for old routes\n   - Set up monitoring for route usage\n\n3. Documentation:\n   - Add API versioning policy\n   - Document breaking changes\n   - Provide migration guides\n\n\nWould you like assistance in implementing any of these suggestions?\n\n</blockquote></details>\n\n</blockquote></details>\n\n<details>\n<summary>\ud83d\udcdc Review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and 8f1edf374203771c17cd6df94ab4cb6c1a2f071d.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/apis/inference/inference.py (3 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n<details>\n<summary>\ud83d\udd07 Additional comments (2)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/apis/inference/inference.py (2)</summary><blockquote>\n\n`186-186`: **Breaking change: API endpoint route modification requires careful transition.**\n\nThe route change from `/inference/completion` to `/v1/completion` is a breaking change that will affect existing clients.\n\n\nConsider implementing one of these transition strategies:\n1. Temporarily support both routes using route aliasing\n2. Add a deprecation period for the old route\n3. Document the migration path for clients\n\nLet's verify the usage of this endpoint:\n```shell\n#!/bin/bash\n# Search for direct usage of the old route to assess impact\nrg -l \"/inference/completion\" --type py\n```\n\n---\n\n`198-198`: **Consistent versioning strategy applied.**\n\nThe route change aligns with the versioning strategy applied to other endpoints.\n\n\nLet's verify the usage of this endpoint:\n```shell\n#!/bin/bash\n# Search for direct usage of the old route to assess impact\nrg -l \"/inference/chat_completion\" --type py\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit for review status -->"
            }
        ],
        "requested_reviewers": [],
        "hunks": [],
        "commits_data": [
            {
                "sha": "8f1edf374203771c17cd6df94ab4cb6c1a2f071d",
                "author": "nehal-a2z",
                "message": "Add error pattern 037 in inference.py",
                "date": "2024-10-23T19:23:10+00:00",
                "files_changed": [
                    {
                        "filename": "project/api/llama_stack/apis/inference/inference.py",
                        "additions": 3,
                        "deletions": 3,
                        "changes": 6,
                        "status": "modified"
                    }
                ]
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/apis/inference/inference.py",
                "hunks": [
                    {
                        "id": 1,
                        "old_start": 183,
                        "old_count": 7,
                        "new_start": 183,
                        "new_count": 7,
                        "content": " \n     # This method is not `async def` because it can result in either an\n     # `AsyncGenerator` or a `CompletionResponse` depending on the value of `stream`.\n-    @webmethod(route=\"/inference/completion\")\n+    @webmethod(route=\"/v1/completion\")\n     def completion(\n         self,\n         model: str,\n"
                    },
                    {
                        "id": 2,
                        "old_start": 195,
                        "old_count": 7,
                        "new_start": 195,
                        "new_count": 7,
                        "content": " \n     # This method is not `async def` because it can result in either an\n     # `AsyncGenerator` or a `ChatCompletionResponse` depending on the value of `stream`.\n-    @webmethod(route=\"/inference/chat_completion\")\n+    @webmethod(route=\"/v1/chat_completion\")\n     def chat_completion(\n         self,\n         model: str,\n"
                    },
                    {
                        "id": 3,
                        "old_start": 209,
                        "old_count": 7,
                        "new_start": 209,
                        "new_count": 7,
                        "content": "         logprobs: Optional[LogProbConfig] = None,\n     ) -> Union[ChatCompletionResponse, ChatCompletionResponseStreamChunk]: ...\n \n-    @webmethod(route=\"/inference/embeddings\")\n+    @webmethod(route=\"/v1/embeddings\")\n     async def embeddings(\n         self,\n         model: str,\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "35",
        "type": "6",
        "number": 167,
        "base_branch": "main-copy",
        "head_branch": "error-005-api",
        "title": "Automated Test: @eval-allprompt-newsonnet",
        "user": "nehal-a2z",
        "state": "closed",
        "created_at": "2024-10-25T11:09:41+00:00",
        "closed_at": "2024-10-25T11:11:27+00:00",
        "merged_at": null,
        "merge_commit_sha": "987d604e6c24e6fd6bbc4a7ee12f78693c19923f",
        "body": "This pull request was automatically created by [`@coderabbitai/e2e-reviewer`](https://github.com/coderabbitai/e2e-reviewer).",
        "labels": [],
        "commits": 1,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "comments": [
            {
                "user": "coderabbitai[bot]",
                "created_at": "2024-10-25T11:09:47+00:00",
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes focus on modifying the agent turn creation logic within the MetaReferenceAgentsImpl class. The implementation updates how streaming and attachments are handled during agent turn creation. The code now evaluates both the streaming status and attachment presence when determining the flow of agent turn creation. The streaming agent turn creation method (_create_agent_turn_streaming) has been modified to conditionally initialize the agent based on whether the request is streaming and contains attachments. The core method signatures and public interfaces remain unchanged, with all modifications contained to the internal implementation logic.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcVKkw4CzanR5sABsgyAoSAEdsaVxIAAozAEYANgB2AEpGcP96OUhcWBJIAAMAASYlKlV1bQB6EgAmEjBwiXgSAHdKYshEWUQaZkgO9Vh8wvz1IKKAIgBBPBYcmBiuUpIJNCCwLaDeFm5cMHIOxHwMclwZjRgJ22G0ZERsAWZ1Glz5bERKSHJYLY7BoAL3y+HGRQA4vggkoMGBbGAACLUR4kWLhbj4RDqfCuG5wIpKRAMCjwQ7wc4oDC0eAMfzIAqeZDeNCLPwffIxQIhMKRaIDSCkchUTkFCj4bBEMYAYUUJBsKjUuAA5MgyLQwAQwBq+W1Or8+gMSMwNEZ9EYTFBdfgAGY+QjC5ScpisdhcXj8YSicRSGTyCrKaqabS6MB6K23ZnoLBs7VO0UBV1sDCcMJoDq9Jx+VyQPKBqrKrQ6C3GcBgIz7ERiWpobjwWohNB+AD6AzQDAA1rV9m1KohavBmNwggO2Lg0C3wrbKGQGCRa8LcAO0EvEBpuLIOAYZruDBZIHMAJLEMjOgLPZy5u2MAEYUiIdwTWm2+3MRTwW3tRkTACy6LQGwSBncIMHnOY1yPYdQgYIJHkQNV0CXfJsAoLBSRIahKSwIJ8CIOkCQmPx4HQu8H3QcJtwMRJCKKJgaXUbDIFtPFelwbI3nvJD2BQtDkFQbBuFoZZbQlIYGEKLsSKIfgMCCeRigGDiegIW9RE7aTIBIUZfkU9jMOYHpWM/EpqAnCSU2XHpUAAOXOEgDAaG4jywYoWww/wW1XdgW1wVCMDbfTm2k4oABpuNTalGK2eBgSwqlUAwfBM3o2lxHOLZdBQWJznkoV0R/IpvMikyCiKcIom5RL8FiJSDM0zB6ABFlcHM2BLMfABmWjIHHWBFF6eAiAwahUOkPliKwbAwLIgIBDwCFqRoNDMtw/CGGY1i71oIJNLq4KuMa9BWo7dr2H45B3xfdpaCMcxLDmIJlvijBGXBMrICUWCXBe5AbxIAAPLEKE5VjuBeXaNvYRjpCMOzb0wciunCXq0CUMFFq+uDRWwv77UB4HQb4cGBEhrTUxh9dbiKK7PzpX7ZLytBXx9RaSOWkbQiHEcTXYF7IDWul+HxgGkHELi+sUdcdz3AxS0jG17TjR0z0Tehk3ddNM0vHN5HzeVC3UYswwjcsFFYdQW3gWhECnDZ2i6WhApcWJ5bNxJgIABlSAAOVJEltH2AE4fYAVlDoPQ4aBhOs9oPOtoW1E7QBoGloWgBCDxIGFoBpUnzk3LTN5NLet23WgdugW11fQgA\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [],
        "reviews": [
            {
                "user": "nehal-a2z",
                "state": "COMMENTED",
                "submitted_at": "2024-10-25T11:11:20+00:00",
                "body": "**Actionable comments posted: 0**\n\n<details>\n<summary>\ud83e\uddf9 Outside diff range and nitpick comments (1)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/impls/meta_reference/agents/agents.py (1)</summary><blockquote>\n\nLine range hint `143-167`: **Consider implementing critical management methods**\n\nSeveral core methods are marked as `NotImplementedError`:\n- `get_agents_turn`\n- `get_agents_step`\n- `get_agents_session`\n- `delete_agents_session`\n- `delete_agents`\n\nThis limits the ability to:\n1. Monitor and debug agent interactions\n2. Clean up resources\n3. Manage agent lifecycles\n\n\nConsider implementing these methods to improve:\n- Observability: Ability to inspect turn and session states\n- Resource management: Proper cleanup of unused agents and sessions\n- Debugging capabilities: Detailed step inspection\n\n\nWould you like me to help prioritize these methods and provide implementation suggestions?\n\n</blockquote></details>\n\n</blockquote></details>\n\n<details>\n<summary>\ud83d\udcdc Review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and 1ef07871f89855952c3093dfdfa22ddb91cd2777.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/providers/impls/meta_reference/agents/agents.py (2 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n<details>\n<summary>\ud83d\udd07 Additional comments (2)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/impls/meta_reference/agents/agents.py (2)</summary><blockquote>\n\n`128-131`: **Review the streaming condition logic**\n\nThe condition `stream or attachments is None` forces streaming behavior in all cases where attachments are not provided, regardless of the `stream` parameter value. This might lead to unexpected behavior:\n\n1. When `stream=False` and `attachments=None`, it will still use streaming\n2. The `else` clause raises `NotImplementedError`, making it impossible to handle non-streaming cases without attachments\n\n\nLet's check if there are any existing non-streaming uses without attachments that might break:\n\n```shell\n#!/bin/bash\n# Search for create_agent_turn calls without attachments parameter\nast-grep --pattern 'create_agent_turn($$$, stream=false)'\n```\n\n---\n\n`137-140`: _:warning: Potential issue_\n\n**Fix critical NameError in agent initialization**\n\nThere's a critical error in the agent initialization logic:\n1. The `agent` variable is only initialized if `not request.stream and request.attachments`\n2. However, it's used unconditionally in the `async for` loop\n3. This will raise `NameError: name 'agent' is not defined` when the condition is false\n\n\nApply this fix to ensure agent is always initialized:\n\n```diff\n    async def _create_agent_turn_streaming(\n        self,\n        request: AgentTurnCreateRequest,\n    ) -> AsyncGenerator:\n-        if not request.stream and request.attachments:\n-            agent = await self.get_agent(request.agent_id)\n+        agent = await self.get_agent(request.agent_id)\n        async for event in agent.create_and_execute_turn(request):\n            yield event\n```\n\n\nLet's verify if this method is called directly anywhere else in the codebase:\n\n```shell\n#!/bin/bash\n# Search for direct calls to _create_agent_turn_streaming\nrg \"_create_agent_turn_streaming\" -A 2\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit for review status -->"
            }
        ],
        "requested_reviewers": [],
        "hunks": [],
        "commits_data": [
            {
                "sha": "1ef07871f89855952c3093dfdfa22ddb91cd2777",
                "author": "nehal-a2z",
                "message": "Add error pattern 005 in agents.py",
                "date": "2024-10-23T19:22:48+00:00",
                "files_changed": [
                    {
                        "filename": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                        "additions": 3,
                        "deletions": 2,
                        "changes": 5,
                        "status": "modified"
                    }
                ]
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "hunks": [
                    {
                        "id": 1,
                        "old_start": 125,
                        "old_count": 7,
                        "new_start": 125,
                        "new_count": 7,
                        "content": "             attachments=attachments,\n             stream=True,\n         )\n-        if stream:\n+        if stream or attachments is None:\n             return self._create_agent_turn_streaming(request)\n         else:\n             raise NotImplementedError(\"Non-streaming agent turns not yet implemented\")\n"
                    },
                    {
                        "id": 2,
                        "old_start": 134,
                        "old_count": 7,
                        "new_start": 134,
                        "new_count": 8,
                        "content": "         self,\n         request: AgentTurnCreateRequest,\n     ) -> AsyncGenerator:\n-        agent = await self.get_agent(request.agent_id)\n+        if not request.stream and request.attachments:\n+            agent = await self.get_agent(request.agent_id)\n         async for event in agent.create_and_execute_turn(request):\n             yield event\n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "36",
        "type": "6",
        "number": 168,
        "base_branch": "main-copy",
        "head_branch": "error-003-api",
        "title": "Automated Test: @eval-allprompt-newsonnet",
        "user": "nehal-a2z",
        "state": "closed",
        "created_at": "2024-10-25T11:11:28+00:00",
        "closed_at": "2024-10-25T11:13:23+00:00",
        "merged_at": null,
        "merge_commit_sha": "30fb32daf116081ab9cba6e13497369ba9ee3757",
        "body": "This pull request was automatically created by [`@coderabbitai/e2e-reviewer`](https://github.com/coderabbitai/e2e-reviewer).",
        "labels": [],
        "commits": 1,
        "additions": 6,
        "deletions": 8,
        "changed_files": 1,
        "comments": [
            {
                "user": "coderabbitai[bot]",
                "created_at": "2024-10-25T11:11:35+00:00",
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes focus on modifying the vector store implementation, specifically refactoring the `get_embedding_model` function in the project's API stack. The function's internal logic has been streamlined to implement a more efficient caching mechanism for sentence transformer models. The new implementation first verifies if a model exists in the cache before attempting to load it, and only loads new models when they're not found in the cache. The function maintains its original signature and external behavior while optimizing its internal operations. The rest of the codebase, including functionality for data URL handling, content extraction, chunk creation, and document querying using embedding indices, remains unchanged.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcVKkw4CzanR5sABsgyAoSAEdsaVxIAAozAEYANgAOAEpGcP96OUhcWBJIAAMAASYlKlV1bQB6EgAmEjBwiXgSAHdKYshEWUQaZkgO9Vh8wvz1IKKAIgBBPBYcmBiuUpIJNCCwLaDeFm5cMHIOxHwMclwZjRgJ22G0ZERsAWZ1Glz5bERKSHJYLY7BoAL3y+HGRQA4vggkoMGBbGAACLUR4kWLhbj4RDqfCuG5wIpKRAMCjwQ7wc4oDC0eAMfzIAqeZDeNCLPwffIxQIhMKRaIDSCkchUTkFCj4bBEMYAYUUJBsKjUuAA5MgyLQwAQwBq+W1Or8+gMSMwNEZ9EYTFBdfgAGY+QjC5ScpisdhcXj8YSicRSGTyCrKaqabS6MB6K23ZnoLBs7VO0UBV1sDCcMJoDq9Jx+VyQPKBqrKrQ6C3GcBgIz7ERiWpobjwWohNB+AD6AzQDAA1rV9m1Kohanh4EEB2xmHjZLUpGI8W2COENNxZBwDDM1wYLJA5gBJYhkZ0BZ7OXN2xgAjCkRDuCa02328e39qMibT+e9edFeDMbjTFO4aiUlgtp4pAtD4AwTjsO+eJoKQMb0OEuBkhsWwEhMfjwFgDDnnBnhYAURTFKQuAtiaAh0LSF4tg+JBBD0trYBgYiAQANMMCDYSgyDhLaHYfvQBCQN8RRoJA47hAoTFIEU3DUDQFBASBQT4GglFEPBjAdggF69OwZAMEUSGYIgwEUGwfA0SOaFFAxTHiFSGD4Jm2GiJ2yDwPaBFifKoQkAAHkguDufhEz0i5kC2vAFADGxymqe5sQeWJSA4hebGYAh6LYApz5FGFhT0Jh7ZMSQ1l8oKp5eZF0ySf+RVCUxOEBF+P4muwAHnMgpmQOetBBJh6m0KikAAKo2AAMogbFMKmUH+UZzFUraEpDGBEF/lNZ6MZ2WQkB1GDpTSoHgZBqaQFElCyANEWMYtGBbOoXypepZEUddmG0gZiBmkYG6WHMQTyftjLgl5SgMEELjA/w9r+ViFCciB3AvP1DCQOw6hPkYABy4LYZgl7DJQRR+EoYIQqBoiQ6KgHIKecN4ojfDIwIqPo6mmPSGVD4eXS0PnEE8hoHePoU5h8n3aELW/u19lYJVExEeipHMORtBqdRPn0bdcvscOMnhD8FBtDp6hPPARD3bg2UiUd5EAm0eJmmuMzmuGlrluz9CnnGjr7om9DJu66aZkeObyPm8qFuoxZhhGnvJuoLbwLQiAtq07RdLQc4uLEpaRmgAAMaAAOxoAALKkDSJLaDSF1XyQAJwkIXjcdokqTlxXiS0LXyQCAArA0TQNI3yQl3HHsQAorBJynacZwa2e6voQA==\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T11:13:18+00:00",
                "body": "_:warning: Potential issue_\n\n**Consider adding error handling for model loading.**\n\nWhile the implementation is functional, it lacks error handling for potential model loading failures. This could lead to unhandled exceptions in production.\n\nConsider wrapping the model loading in a try-except block:\n\n```diff\n     if model not in EMBEDDING_MODELS:\n         print(f\"Loading sentence transformer for {model}...\")\n-        from sentence_transformers import SentenceTransformer\n-        loaded_model = SentenceTransformer(model)\n-        EMBEDDING_MODELS[model] = loaded_model\n+        try:\n+            from sentence_transformers import SentenceTransformer\n+            loaded_model = SentenceTransformer(model)\n+            EMBEDDING_MODELS[model] = loaded_model\n+        except Exception as e:\n+            raise RuntimeError(f\"Failed to load embedding model {model}: {str(e)}\")\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    if model not in EMBEDDING_MODELS:\n        print(f\"Loading sentence transformer for {model}...\")\n        try:\n            from sentence_transformers import SentenceTransformer\n            loaded_model = SentenceTransformer(model)\n            EMBEDDING_MODELS[model] = loaded_model\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load embedding model {model}: {str(e)}\")\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "position": 11,
                "line_range": "Comment on lines +11 to +11"
            }
        ],
        "reviews": [
            {
                "user": "nehal-a2z",
                "state": "COMMENTED",
                "submitted_at": "2024-10-25T11:13:18+00:00",
                "body": "**Actionable comments posted: 1**\n\n<details>\n<summary>\ud83e\uddf9 Outside diff range and nitpick comments (1)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/utils/memory/vector_store.py (1)</summary><blockquote>\n\n`34-40`: **Consider thread-safety and memory management improvements.**\n\nThe current implementation has potential thread-safety issues with the global cache and might benefit from memory management considerations.\n\nConsider these improvements:\n1. Use a thread-safe cache implementation (e.g., `threading.Lock` or concurrent dictionary)\n2. Implement a cache size limit or LRU eviction policy\n3. Remove the redundant assignment on line 40\n\nExample implementation:\n\n```python\nfrom threading import Lock\nfrom functools import lru_cache\n\n_model_lock = Lock()\n_MAX_CACHE_SIZE = 5  # Adjust based on memory constraints\n\ndef get_embedding_model(model: str) -> \"SentenceTransformer\":\n    global EMBEDDING_MODELS\n    \n    with _model_lock:\n        if model not in EMBEDDING_MODELS:\n            if len(EMBEDDING_MODELS) >= _MAX_CACHE_SIZE:\n                # Implement cache eviction policy if needed\n                oldest_model = next(iter(EMBEDDING_MODELS))\n                del EMBEDDING_MODELS[oldest_model]\n            \n            try:\n                from sentence_transformers import SentenceTransformer\n                EMBEDDING_MODELS[model] = SentenceTransformer(model)\n            except Exception as e:\n                raise RuntimeError(f\"Failed to load embedding model {model}: {str(e)}\")\n        \n        return EMBEDDING_MODELS[model]\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n<details>\n<summary>\ud83d\udcdc Review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and a0a7a4821f208269e09ac184a41df26b522e2967.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/providers/utils/memory/vector_store.py (1 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n<details>\n<summary>\ud83d\udd07 Additional comments (1)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/utils/memory/vector_store.py (1)</summary><blockquote>\n\n`34-40`: **Integration looks good!**\n\nThe refactored implementation maintains backward compatibility while improving code organization. The function continues to serve its purpose effectively in both document insertion and querying operations.\n\n\nLet's verify the usage pattern:\n\n```shell\n#!/bin/bash\n# Description: Verify the embedding model usage patterns\n# Look for potential issues in error handling or performance bottlenecks\n\n# Check for any direct access to EMBEDDING_MODELS\nrg \"EMBEDDING_MODELS\\[\" --type py\n\n# Look for parallel/concurrent usage patterns\nrg \"get_embedding_model.*asyncio|concurrent|threading\" --type py\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit for review status -->"
            }
        ],
        "requested_reviewers": [],
        "hunks": [],
        "commits_data": [
            {
                "sha": "a0a7a4821f208269e09ac184a41df26b522e2967",
                "author": "nehal-a2z",
                "message": "Add error pattern 003 to multiple files",
                "date": "2024-10-23T19:05:24+00:00",
                "files_changed": [
                    {
                        "filename": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                        "additions": 6,
                        "deletions": 8,
                        "changes": 14,
                        "status": "modified"
                    }
                ]
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "hunks": [
                    {
                        "id": 1,
                        "old_start": 31,
                        "old_count": 15,
                        "new_start": 31,
                        "new_count": 13,
                        "content": " def get_embedding_model(model: str) -> \"SentenceTransformer\":\n     global EMBEDDING_MODELS\n \n-    loaded_model = EMBEDDING_MODELS.get(model)\n-    if loaded_model is not None:\n-        return loaded_model\n+    if model not in EMBEDDING_MODELS:\n+        print(f\"Loading sentence transformer for {model}...\")\n+        from sentence_transformers import SentenceTransformer\n+        loaded_model = SentenceTransformer(model)\n+        EMBEDDING_MODELS[model] = loaded_model\n \n-    print(f\"Loading sentence transformer for {model}...\")\n-    from sentence_transformers import SentenceTransformer\n-\n-    loaded_model = SentenceTransformer(model)\n-    EMBEDDING_MODELS[model] = loaded_model\n+    loaded_model = EMBEDDING_MODELS[model]\n     return loaded_model\n \n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "37",
        "type": "6",
        "number": 169,
        "base_branch": "main-copy",
        "head_branch": "error-004-api",
        "title": "Automated Test: @eval-allprompt-newsonnet",
        "user": "nehal-a2z",
        "state": "closed",
        "created_at": "2024-10-25T11:13:25+00:00",
        "closed_at": "2024-10-25T11:15:57+00:00",
        "merged_at": null,
        "merge_commit_sha": "fc8b5c81442ef957cc41cd87d4e3e0cf07cafc21",
        "body": "This pull request was automatically created by [`@coderabbitai/e2e-reviewer`](https://github.com/coderabbitai/e2e-reviewer).",
        "labels": [],
        "commits": 1,
        "additions": 12,
        "deletions": 2,
        "changed_files": 1,
        "comments": [
            {
                "user": "coderabbitai[bot]",
                "created_at": "2024-10-25T11:13:31+00:00",
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes introduce a new `score_threshold` parameter to the reward scoring functionality. This parameter is added with a default value of 0.0 to both the `RewardScoringRequest` model and the `reward_score` method in the `RewardScoring` Protocol. The implementation includes error handling logic where it returns an empty response when dialog_generations is empty, and returns a response with empty dialog and scored_generations when the score_threshold is non-negative. The method signature for `reward_score` is updated to accommodate the new parameter, changing from accepting just dialog_generations and model to now including score_threshold as an optional parameter.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcVKkw4CzanR5sABsgyAoSAEdsaVxIAAozAEYANgBOAEpGcP96OUhcWBJIAAMAASYlKlV1bQB6EgAmEjBwiXgSAHdKYshEWUQaZkgO9Vh8wvz1IKKAIgBBPBYcmBiuUpIJNCCwLaDeFm5cMHIOxHwMclwZjRgJ22G0ZERsAWZ1Glz5bERKSHJYLY7BoAL3y+HGRQA4vggkoMGBbGAACLUR4kWLhbj4RDqfCuG5wIpKRAMCjwQ7wc4oDC0eAMfzIAqeZDeNCLPwffIxQIhMKRaIDSCkchUTkFCj4bBEMYAYUUJBsKjUuAA5MgyLQwAQwBq+W1Or8+gMSMwNEZ9EYTFBdfgAGY+QjC5ScpisdhcXj8YSicRSGTyCrKaqabS6MB6K23ZnoLBs7VO0UBV1sDCcMJoDq9Jx+VyQPKBqrKrQ6C3GcBgIz7ERiWpobjwWohNB+AD6AzQDAA1rX64hauEOi5aG2mGSMER+50hyO8fBxxpuLIOAYZquDBZIHMAJLEMjOgLPZy5u2MAHj6TuCa02325iKeC29qMiYDoe9UdzoiQW3YDBiSkYFs6jyHkaC0LS47oH8nQlCSeIkC24rSLAMK0D03AuM26K/CMBRQUotpskEsSbEE0T8PaAAMGiUWCeb4HhxQ2FOFC0PYH7jsxUQxD0d5KKEmD0MUr6sTO4S8eiKH0HOEIlMxg6sexs7jj0VgSgQTBBASEzwMw3DTCmuDUABFElCJw5weJKDINg3C0MsBDUgwZFKHmjx0pAlASnwZ60EEn4xvQ4SIFiGA/EKe6igBXDqHyuDYBQYUxp5em4PIwWheFHSFFgBRFHO3B4JAtJbPgRAtgmxnnMgqAmocsgADSBXFCVJWgfIhdVRS4WMdVpcV8ClV+gnvvBw6VeI1XDDlo3hIhsDBShsLWX85zHCQRDGVIZpGOulhzMRzoAYy4J5cVohBJhk1JSeJAAB5YhQnJ4oEAj+QwnmpuoT7mtYmFsDQfBgUotBcMUlkIUhiBLaD35BPg1CQAAvJA1GUT0MnOY8yBMSxbEcUQXECrgGNYMUVY+j2DZNq27ZdlTfbmWJn6TgpFkEwusjFAYUAALKSYovTwEQgHxeEkC2fZHxgwRfJs2JJBxD8QS2k1JXw+VE3HVwAAySC4AA2kig0a5CkVVWFAC6TV8SQQRcAMFDpD0gBJhCUstMxDSt26rA1DRV5vXYguv60bJtlWbIoW4g1uQLb9u9LgFBNRD82LahXC2vDiMo2jzvUtY6n4Jpcl40pY5EKTJQUzWdbU5dtNGfTdeM3jzPjqz06WZ+nPc6uMzmuGlrlp99AnnGjqB0mLCGVwVCZoeOYgQG8qFuoxZhhGI/JuoLbwLQiAtq07RdBZRlPZvw8QJAiQAMyUaIDSJA0N4NAwAAcKgkLftAAOypKkRItBEi2mSO/WgABWEgto34kFSL/AQv8wKXzLNfHeuA94HyPhsE+dAWy6n0EAA=\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T11:15:52+00:00",
                "body": "_:warning: Potential issue_\n\n**Critical: Incorrect threshold logic implementation**\n\nThe current implementation returns empty results for ALL requests since:\n1. The default score_threshold is 0.0\n2. The condition `if score_threshold >= 0` will always be true for the default case\n3. This results in always returning empty dialog and scored_generations\n\nThis appears to be the opposite of the intended behavior. The threshold should likely be used to filter scored generations, not to return empty results.\n\nConsider implementing proper threshold filtering:\n\n```diff\n-        if score_threshold >= 0:\n-            return RewardScoringResponse(\n-                scored_generations=[\n-                    ScoredDialogGenerations(dialog=[], scored_generations=[])\n-                ])\n+        # Process dialog_generations and apply scoring\n+        scored_results = []\n+        for dg in dialog_generations:\n+            scored_msgs = [\n+                ScoredMessage(message=msg, score=compute_score(msg))\n+                for msg in dg.sampled_generations\n+            ]\n+            # Filter based on threshold\n+            filtered_msgs = [\n+                sm for sm in scored_msgs\n+                if sm.score >= score_threshold\n+            ]\n+            scored_results.append(\n+                ScoredDialogGenerations(\n+                    dialog=dg.dialog,\n+                    scored_generations=filtered_msgs\n+                )\n+            )\n+        return RewardScoringResponse(scored_generations=scored_results)\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/reward_scoring/reward_scoring.py",
                "position": 24,
                "line_range": "Comment on lines +24 to +24"
            }
        ],
        "reviews": [
            {
                "user": "nehal-a2z",
                "state": "COMMENTED",
                "submitted_at": "2024-10-25T11:15:53+00:00",
                "body": "**Actionable comments posted: 1**\n\n<details>\n<summary>\ud83e\uddf9 Outside diff range and nitpick comments (2)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/apis/reward_scoring/reward_scoring.py (2)</summary><blockquote>\n\n`57-57`: **Simplify return type annotation**\n\nThe `Union` type hint with a single type is redundant.\n\n```diff\n-    ) -> Union[RewardScoringResponse]:\n+    ) -> RewardScoringResponse:\n```\n\n---\n\n`54-56`: **Fix method signature formatting**\n\nThe parameter formatting is inconsistent. Remove the trailing whitespace and align the parameters.\n\n```diff\n-        model: str, \n-        score_threshold: float = 0.0,\n+        model: str,\n+        score_threshold: float = 0.0\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n<details>\n<summary>\ud83d\udcdc Review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and 130ec212ff2c8abe3d7991d1f68d5ef2ce97b7ad.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/apis/reward_scoring/reward_scoring.py (2 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n<details>\n<summary>\ud83d\udd07 Additional comments (3)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/apis/reward_scoring/reward_scoring.py (3)</summary><blockquote>\n\n`40-40`: **LGTM: Clean addition of score_threshold parameter**\n\nThe parameter is well-typed and maintains backward compatibility with its default value.\n\n---\n\n`58-60`: **LGTM: Proper handling of empty input**\n\nGood defensive programming by handling empty dialog_generations case.\n\n---\n\n`54-57`: **Verify protocol implementations**\n\nThis change modifies the RewardScoring protocol interface. Please ensure all implementations of this protocol are updated to handle the new score_threshold parameter correctly.\n\n```shell\n#!/bin/bash\n# Search for all implementations of RewardScoring protocol\nast-grep --pattern 'class $_($$_) implements RewardScoring:\n  $$$'\n\n# Search for all classes that might be implementing the reward_score method\nrg -l \"def reward_score.*dialog_generations.*model\"\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit for review status -->"
            }
        ],
        "requested_reviewers": [],
        "hunks": [],
        "commits_data": [
            {
                "sha": "130ec212ff2c8abe3d7991d1f68d5ef2ce97b7ad",
                "author": "nehal-a2z",
                "message": "Add error pattern 004 in reward_scoring.py",
                "date": "2024-10-23T19:03:28+00:00",
                "files_changed": [
                    {
                        "filename": "project/api/llama_stack/apis/reward_scoring/reward_scoring.py",
                        "additions": 12,
                        "deletions": 2,
                        "changes": 14,
                        "status": "modified"
                    }
                ]
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/apis/reward_scoring/reward_scoring.py",
                "hunks": [
                    {
                        "id": 1,
                        "old_start": 37,
                        "old_count": 6,
                        "new_start": 37,
                        "new_count": 7,
                        "content": " \n     dialog_generations: List[DialogGenerations]\n     model: str\n+    score_threshold: float = 0.0\n \n \n @json_schema_type\n"
                    },
                    {
                        "id": 2,
                        "old_start": 51,
                        "old_count": 5,
                        "new_start": 52,
                        "new_count": 14,
                        "content": "     def reward_score(\n         self,\n         dialog_generations: List[DialogGenerations],\n-        model: str,\n-    ) -> Union[RewardScoringResponse]: ...\n+        model: str, \n+        score_threshold: float = 0.0,\n+    ) -> Union[RewardScoringResponse]:\n+        if not dialog_generations:\n+            return RewardScoringResponse(scored_generations=[])\n+        \n+        if score_threshold >= 0:\n+            return RewardScoringResponse(\n+                scored_generations=[\n+                    ScoredDialogGenerations(dialog=[], scored_generations=[])\n+                ])\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "38",
        "type": "6",
        "number": 170,
        "base_branch": "main-copy",
        "head_branch": "error-018-api",
        "title": "Automated Test: @eval-allprompt-newsonnet",
        "user": "nehal-a2z",
        "state": "closed",
        "created_at": "2024-10-25T11:15:59+00:00",
        "closed_at": "2024-10-25T12:47:45+00:00",
        "merged_at": null,
        "merge_commit_sha": "f0d2e761cd0d67f13161e3ca5f0a1806acc46c04",
        "body": "This pull request was automatically created by [`@coderabbitai/e2e-reviewer`](https://github.com/coderabbitai/e2e-reviewer).",
        "labels": [],
        "commits": 1,
        "additions": 6,
        "deletions": 3,
        "changed_files": 1,
        "comments": [
            {
                "user": "coderabbitai[bot]",
                "created_at": "2024-10-25T11:16:06+00:00",
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes focus on implementing a metadata caching mechanism in the PGVectorMemoryAdapter class. A new `_bank_metadata_cache` dictionary was added to store memory bank metadata at the class instance level. The implementation modifies several methods including `register_memory_bank`, `list_memory_banks`, `insert_documents`, and `query_documents` to utilize this new caching layer. The register_memory_bank method now stores bank metadata in the cache while committing changes to the database. The list_memory_banks method was updated to retrieve banks from the metadata cache when available. Additionally, the bank lookup logic in insert_documents and query_documents methods was modified to first check the metadata cache before accessing the main cache. The core functionality remains unchanged, with the addition of this new caching layer for memory bank metadata.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcVKkw4CzanR5sABsgyAoSAEdsaVxIAAozAEYAdgAGAEpGcP96OUgAAXJYNCCwNAAmAC9IcVwggIAiAEE8FhyYGK58kgkSspDeFm5cMHIAd0R8DHJcBo0YWBJrOzG0ZFJyKhp6XFgKfGwiWBrFyAADfKYlKlV1bQB6EnKSMHCJeBIxyjOfNu3IRCyRA0ZjzOBLbjBULhKIxFDIIL4BjteBYXZLADi+CCSgwYFsYAAItQ1iRYuFuPhEOp8K5INgMNcTksAMKKEg2FRqWJNACS/AoREw8Eq1HgU3mADl8OhaLR1OKMCVIEpcNogsgVAdYuiAdxRPAAGbwBiMYoYUjIWmBCiUxBLL7hHj7d5Keio5kq6QMCjwYaKyACEiyKb0DD4cQWlC4TWtPz/JW4bDhDRGfRGExQMj0fCG37EMjKf5MVjsLi8fjCUTiKQyeRXZS3TTaXRgPSZhaeTVYNCtAubdoltgYThhNBjAFOPx0vINm7crQ6dPGcBgIyDERie5objwe4hNB+AD6wLQDAA1vdBq7KIht7QdzQKHe2MxabIr0QpGJaZ/vwQKA0bhZA4AwGnAgwLEgPl+yLAJECnFx5FzM1MEtdxTnlQ08zfLCPmQXUrAxAA1ataQAWRIN9XCaB9hkoRggjWZB4GYbh6mHNVxCmQN5FRXB9lobAGFRIh0EgcZziPARMHPI82DVB81SPZEGEWH55TERUkJqGVgVpJZX3fQNZMgBS0CUtAwVONS0OkdBsOrSBYHwCcjJnWTNSdVTFnoTB6HCASPikWhQIMRJ5lot0pJkjA5PMyyVLPdT0E1RjmJQDBTwwBgll6X0VHqAxynmcjFCND56DOcIiCQJ95Ko99pNkn4CABAClli88zLJCySUyz1JJ8pZ/IUVh1BVEkZPtVCLWkAwAGZ5gAVW4JSAjOII6oa6jZGauLEFamVAuTLAuuQQ19mYT0Ev64bIDGRYe16eAmIEIqABZ5hZc1SFyUzEXwc9sG4SBEVq00PTOVF7QoXAj1oJEnHYQ70EZc5YVcBGkc41G2uNZ9YjU0Rut1W61UYZLOpIQ0DPQBhcsQakozJ7QsGGjCllsubkD8fi2c9RBDy5unDQZLSphKdR5Ee16ln4wThNEtHZXlbilVCHzlaY2QGNpvh3PkLqesUklUyMSDLCaIInzFKYCJlXUlAYJitkVK08xIAAPSk4YCa1IXek1IHYBV5oMaVZstB7KEMiyljap3RFdu2sv4T2fdpf4A+wIPTVD8RpGswzyuNZF1atDAgnkNBHLET1+MoDWUDYjj2FTr01Vej3PW9urlZd5j7NGhSXNoRBU3Aho0zbDNVxD9GUN7AhYK2AIhzLMcJwQ5wZ3rdl53URdW3beeh3UI94HHo83g+L5aBPNU4ZPueIBVAAOABOd/DQWgRUgEIkEgJAABszxygMH/rQBgC0GCJA+iA1IDBDTv3KB9AQIDaAgLQKkRIICX4rjfufeGV9EA3x6HfOgR5swnyAA===\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T11:18:15+00:00",
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Refactor duplicated bank lookup logic.**\n\nThe bank lookup logic is duplicated between `insert_documents` and `query_documents`. Consider extracting this into a private helper method for better maintainability and clarity.\n\nConsider this refactoring:\n\n```diff\n+    def _get_bank_index(self, bank_id: str) -> BankWithIndex:\n+        \"\"\"Get bank index from cache if bank exists in metadata cache.\"\"\"\n+        if bank_id not in self._bank_metadata_cache:\n+            raise ValueError(f\"Bank '{bank_id}' is not registered\")\n+        index = self.cache.get(bank_id)\n+        if not index:\n+            raise ValueError(f\"Bank '{bank_id}' exists in metadata but missing from index cache\")\n+        return index\n\n     async def insert_documents(self, bank_id: str, documents: List[MemoryBankDocument],\n                              ttl_seconds: Optional[int] = None) -> None:\n-        index = self.cache.get(bank_id) if bank_id in self._bank_metadata_cache else None\n-        if not index:\n-            raise ValueError(f\"Bank {bank_id} not found\")\n+        index = self._get_bank_index(bank_id)\n         await index.insert_documents(documents)\n\n     async def query_documents(self, bank_id: str, query: InterleavedTextMedia,\n                             params: Optional[Dict[str, Any]] = None) -> QueryDocumentsResponse:\n-        index = self.cache.get(bank_id) if bank_id in self._bank_metadata_cache else None\n-        if not index:\n-            raise ValueError(f\"Bank {bank_id} not found\")\n+        index = self._get_bank_index(bank_id)\n         return await index.query_documents(query, params)\n```\n\n\nAlso applies to: 210-213\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/memory/pgvector/pgvector.py",
                "position": 40,
                "line_range": "Comment on lines +40 to +40"
            },
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T11:18:16+00:00",
                "body": "_:warning: Potential issue_\n\n**Add error handling for cache updates.**\n\nThe cache update should be wrapped in a try-except block to ensure consistency between the database and cache state. If index creation fails, the cache entry should be removed.\n\nConsider this approach:\n\n```diff\n-        self._bank_metadata_cache[memory_bank.identifier] = memory_bank\n-\n-        index = BankWithIndex(\n-            bank=memory_bank,\n-            index=PGVectorIndex(memory_bank, ALL_MINILM_L6_V2_DIMENSION, self.cursor),\n-        )\n-        self.cache[memory_bank.identifier] = index\n-        self.conn.commit()\n+        try:\n+            self._bank_metadata_cache[memory_bank.identifier] = memory_bank\n+            index = BankWithIndex(\n+                bank=memory_bank,\n+                index=PGVectorIndex(memory_bank, ALL_MINILM_L6_V2_DIMENSION, self.cursor),\n+            )\n+            self.cache[memory_bank.identifier] = index\n+            self.conn.commit()\n+        except Exception:\n+            self._bank_metadata_cache.pop(memory_bank.identifier, None)\n+            self.conn.rollback()\n+            raise\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n        try:\n            self._bank_metadata_cache[memory_bank.identifier] = memory_bank\n            index = BankWithIndex(\n                bank=memory_bank,\n                index=PGVectorIndex(memory_bank, ALL_MINILM_L6_V2_DIMENSION, self.cursor),\n            )\n            self.cache[memory_bank.identifier] = index\n            self.conn.commit()\n        except Exception:\n            self._bank_metadata_cache.pop(memory_bank.identifier, None)\n            self.conn.rollback()\n            raise\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/memory/pgvector/pgvector.py",
                "position": 19,
                "line_range": "Comment on lines +19 to +19"
            },
            {
                "user": "nehal-a2z",
                "created_at": "2024-10-25T11:18:16+00:00",
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider optimizing the bank metadata retrieval.**\n\nThe current implementation always loads banks from the database even if they're already cached. Consider checking the cache first and only loading from the database for missing entries.\n\nHere's a more efficient approach:\n\n```diff\n-        return [self._bank_metadata_cache.get(bank.identifier, bank) for bank in banks]\n+        # First check if we have all banks in cache\n+        cached_banks = {id: bank for id, bank in self._bank_metadata_cache.items()}\n+        if all(bank.identifier in cached_banks for bank in banks):\n+            return list(cached_banks.values())\n+        # Fall back to database lookup for missing banks\n+        return [self._bank_metadata_cache.get(bank.identifier, bank) for bank in banks]\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/memory/pgvector/pgvector.py",
                "position": 28,
                "line_range": "Comment on lines +28 to +28"
            }
        ],
        "reviews": [
            {
                "user": "nehal-a2z",
                "state": "COMMENTED",
                "submitted_at": "2024-10-25T11:18:16+00:00",
                "body": "**Actionable comments posted: 3**\n\n<details>\n<summary>\ud83e\uddf9 Outside diff range and nitpick comments (1)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/adapters/memory/pgvector/pgvector.py (1)</summary><blockquote>\n\n`122-122`: **Consider thread safety for the metadata cache.**\n\nThe `_bank_metadata_cache` dictionary might need thread-safety mechanisms if the adapter is used in a multi-threaded environment. Consider using a thread-safe dictionary implementation or adding appropriate locks.\n\n</blockquote></details>\n\n</blockquote></details>\n\n<details>\n<summary>\ud83d\udcdc Review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and d898f3b0b1ee62e2cb0dc3c1460cf824b6d6a016.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/providers/adapters/memory/pgvector/pgvector.py (4 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit for review status -->"
            }
        ],
        "requested_reviewers": [],
        "hunks": [],
        "commits_data": [
            {
                "sha": "d898f3b0b1ee62e2cb0dc3c1460cf824b6d6a016",
                "author": "nehal-a2z",
                "message": "Add error pattern 018 in pgvector.py",
                "date": "2024-10-23T17:48:25+00:00",
                "files_changed": [
                    {
                        "filename": "project/api/llama_stack/providers/adapters/memory/pgvector/pgvector.py",
                        "additions": 6,
                        "deletions": 3,
                        "changes": 9,
                        "status": "modified"
                    }
                ]
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/memory/pgvector/pgvector.py",
                "hunks": [
                    {
                        "id": 1,
                        "old_start": 119,
                        "old_count": 6,
                        "new_start": 119,
                        "new_count": 7,
                        "content": "         self.cursor = None\n         self.conn = None\n         self.cache = {}\n+        self._bank_metadata_cache = {}\n \n     async def initialize(self) -> None:\n         try:\n"
                    },
                    {
                        "id": 2,
                        "old_start": 168,
                        "old_count": 12,
                        "new_start": 169,
                        "new_count": 14,
                        "content": "                 (memory_bank.identifier, memory_bank),\n             ],\n         )\n+        self._bank_metadata_cache[memory_bank.identifier] = memory_bank\n \n         index = BankWithIndex(\n             bank=memory_bank,\n             index=PGVectorIndex(memory_bank, ALL_MINILM_L6_V2_DIMENSION, self.cursor),\n         )\n         self.cache[memory_bank.identifier] = index\n+        self.conn.commit()\n \n     async def list_memory_banks(self) -> List[MemoryBankDef]:\n         banks = load_models(self.cursor, MemoryBankDef)\n"
                    },
                    {
                        "id": 3,
                        "old_start": 184,
                        "old_count": 7,
                        "new_start": 187,
                        "new_count": 7,
                        "content": "                     index=PGVectorIndex(bank, ALL_MINILM_L6_V2_DIMENSION, self.cursor),\n                 )\n                 self.cache[bank.identifier] = index\n-        return banks\n+        return [self._bank_metadata_cache.get(bank.identifier, bank) for bank in banks]\n \n     async def insert_documents(\n         self,\n"
                    },
                    {
                        "id": 4,
                        "old_start": 192,
                        "old_count": 7,
                        "new_start": 195,
                        "new_count": 7,
                        "content": "         documents: List[MemoryBankDocument],\n         ttl_seconds: Optional[int] = None,\n     ) -> None:\n-        index = self.cache.get(bank_id, None)\n+        index = self.cache.get(bank_id) if bank_id in self._bank_metadata_cache else None\n         if not index:\n             raise ValueError(f\"Bank {bank_id} not found\")\n \n"
                    },
                    {
                        "id": 5,
                        "old_start": 204,
                        "old_count": 7,
                        "new_start": 207,
                        "new_count": 7,
                        "content": "         query: InterleavedTextMedia,\n         params: Optional[Dict[str, Any]] = None,\n     ) -> QueryDocumentsResponse:\n-        index = self.cache.get(bank_id, None)\n+        index = self.cache.get(bank_id) if bank_id in self._bank_metadata_cache else None\n         if not index:\n             raise ValueError(f\"Bank {bank_id} not found\")\n \n"
                    }
                ]
            }
        ]
    }
]