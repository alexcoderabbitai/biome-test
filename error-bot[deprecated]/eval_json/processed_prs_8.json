[
    {
        "id": "59",
        "type": "8",
        "number": 196,
        "head_branch": "error-047-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe code introduces caching mechanisms for the functions `string_to_trace_id` and `string_to_span_id` in the `opentelemetry.py` module. For `string_to_trace_id`, a global cache `_trace_id_cache` is implemented to store computed trace IDs. This cache has a maximum size of 10,000 entries, and it is cleared when the limit is reached to manage memory usage.\n\nIn the case of `string_to_span_id`, caching is added using `_span_id_cache`. The cache key is generated by hashing the first three characters of the input string and taking the modulus with 1,000. The function is updated to convert the full input string to an integer using `int.from_bytes` with `byteorder=\"little\"`. This differs from the previous implementation, which sliced the string to the first eight bytes and used `byteorder=\"big\"` for the conversion.\n\nNo changes were made to the declarations of exported or public entities in this update.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpETZWaCrKNxU3bABsvkCiQBHbGlcABpIcVwvOkgAIgBBPBZqGOgQrgABEgk0LzBcr14WblwwfABGXmz4EgB3UNjIWrRkBwFmdRp6OUhsREpIACpB8lhc/IAmAC9h9GRPHz9A4MRcSAxHAQHh8oBOADZhjRhYEh5vX38gkKaWnAJmangGAvkGfxTu+VxTiPx8XwAbQABhkmEoqKp1NoAPQkCYkMD+CQ1WqUYEAXQAFLBcLhuIgODCYUR1LBsAINExmDDwcoobhYfDEcjUZQAJTHACSa1QaBy8C8KmikAAZvg+Ky6ug1oDcfjCcTST8KVSWLTFPS1Iz4DCAOIApQYMC2MAAEWoLRIuBhCy8ML2+2x8oJRJJZNV1I1EJU2thBq8RpNNnNlv6NrtDoOnKM+iMJigZHo+FF90IpHIVC6ClY7C4vH4wlE4ikMjemsh2q0On0CZOqD5WDQSWIZGU2epbAwnD8aFq9kcjxckB6dMr6mrujAhmM4DARiKIjEMLQ3F1PjQjwA+qs0AwANa2ij4FEQxAr2irmgUc80aJsXAuGH4bjsEj361Pl9vj+P2QabhZA4AxYlAgwLEgeIuVbTNPgHJxhxTRgxgwUhEDcX46RQbtj1obAGGkRg9wQVCxQlCJflFbAMDEeB8AwZBgVWCh4FQrcCHYqgCK3eBaGBdAMHoJjH1Yoh2PwHduEwHi+L+fgSngDopjOV8KHFChHhokhjgAMXI4SWLYjjHz3EgZOBcI0EgIgvEEXIiIYX5gU40yZK3F5HJIfiG2YbgP27GICHsAh/BzTxsxMgjIC5M1EHCWoyXQSBHgAD0Uxx7HgZT+FTcoAAZQjyorIHYFjpGOAB1U4sB+M4PN+D5POQH5UC8RT1HCdQUGQBhomcQL8GSzA0FIZKSGYCV5D6EbtKMLksAM0TxMk6TeIshySKIbr0FoJR6D6UTIGcxApIwNz6q8444Dq4izn3Eh5FQCFJBiHoxkQTaKLOUV4BvNYfn8Oqxi469kCQ2rsPC4LDK2zB6EZfdDohia8K8PomkS8pCqKq7KOo2j6PWfB+yYDApAoXBmrxxZWKh5ikcGzBsJoUg+AO0jgVYzRRWPZgtzkGhEH4hKfiOgWSAlCEAF5YjavFolidbWNWEg0GTVNEDahgkd+enSKCiGfr+kr4CIXER1kQWBP2xBDuBcXJcoGW1CIRW5hHEh1NmoxwMseIvGvJ56OawaIaUXrnCDhicpKlLuAlbNyM8AQtZK7t1BqdCDEGAA5RmA/bOjo4N35w6FLMi7B1MSDjhOYiTilU/YDPCLRULHiUDRBhAsCDDjWcIDT9W0xg9sYk7PNe37BwEPkUcK19CdtCnPQ607dQZMQLcpTRWgd0ZCmV/jOdIF2WgAGYAHYAFZ9jy8pygADkv8oABYBGviYJkfhgv5IEg8oTGvv/d+JADgMEfvsBE59z5HwHlAdeuBN7b2qHUOgW4kwryAA===\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider using an LRU cache instead of clearing the entire cache**\n\nClearing the entire cache when `_max_cache_size` is reached can lead to performance issues due to cache misses after the cache is cleared. Implementing an LRU (Least Recently Used) cache would evict only the least recently used items, providing better performance and memory management.\n\n\n\nYou can use the `functools.lru_cache` decorator to implement caching:\n\n```python\nfrom functools import lru_cache\n\n@lru_cache(maxsize=10000)\ndef string_to_trace_id(s: str) -> int:\n    return int.from_bytes(s.encode(), byteorder=\"big\", signed=False)\n```\n\nApply the same change to `string_to_span_id`:\n\n```python\n@lru_cache(maxsize=10000)\ndef string_to_span_id(s: str) -> int:\n    return int.from_bytes(s.encode(), byteorder=\"little\", signed=False)\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/telemetry/opentelemetry/opentelemetry.py",
                "position": 10
            },
            {
                "body": "_:warning: Potential issue_\n\n**Ensure thread safety when modifying global caches**\n\nAccessing and modifying the global caches `_trace_id_cache` and `_span_id_cache` without synchronization can lead to race conditions in a multithreaded environment.\n\n\n\nConsider using a threading lock to make cache modifications thread-safe:\n\n```python\nimport threading\n\n_cache_lock = threading.Lock()\n\ndef string_to_trace_id(s: str) -> int:\n    with _cache_lock:\n        if s not in _trace_id_cache:\n            if len(_trace_id_cache) >= _max_cache_size:\n                _trace_id_cache.clear()\n            _trace_id_cache[s] = int.from_bytes(s.encode(), byteorder=\"big\", signed=False)\n    return _trace_id_cache[s]\n\ndef string_to_span_id(s: str) -> int:\n    with _cache_lock:\n        key = s  # Using the full string as the cache key\n        if key not in _span_id_cache:\n            if len(_span_id_cache) >= _max_cache_size:\n                _span_id_cache.clear()\n            _span_id_cache[key] = int.from_bytes(s.encode(), byteorder=\"little\", signed=False)\n    return _span_id_cache[key]\n```\n\n\nAlso applies to: 42-46\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/telemetry/opentelemetry/opentelemetry.py",
                "position": 16
            },
            {
                "body": "_:warning: Potential issue_\n\n**Avoid cache collisions by using the full string as the cache key**\n\nUsing `key = hash(s[:3]) % 1000` as the cache key may result in collisions, where different span IDs map to the same cache key. This could lead to incorrect span IDs being returned, affecting tracing accuracy.\n\n\n\nConsider using the full string `s` as the cache key to ensure uniqueness:\n\n```diff\n-# Use hash of first few chars as cache key for better memory usage\n-key = hash(s[:3]) % 1000  \n+key = s\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    key = s\n    if key not in _span_id_cache:\n        if len(_span_id_cache) >= _max_cache_size:\n            _span_id_cache.clear()\n        # Convert full span ID string to int value\n        _span_id_cache[key] = int.from_bytes(s.encode(), byteorder=\"little\", signed=False) \n    return _span_id_cache[key]\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/telemetry/opentelemetry/opentelemetry.py",
                "position": 30
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/telemetry/opentelemetry/opentelemetry.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " from .config import OpenTelemetryConfig\n \n \n-def string_to_trace_id(s: str) -> int:\n-    # Convert the string to bytes and then to an integer\n-    return int.from_bytes(s.encode(), byteorder=\"big\", signed=False)\n+# Optimization: Cache trace and span ID conversions\n+_trace_id_cache = {}\n+_span_id_cache = {}\n+_max_cache_size = 10000\n \n+def string_to_trace_id(s: str) -> int:\n+    if s not in _trace_id_cache:\n+        if len(_trace_id_cache) >= _max_cache_size:\n+            _trace_id_cache.clear()\n+        _trace_id_cache[s] = int.from_bytes(s.encode(), byteorder=\"big\", signed=False)\n+    return _trace_id_cache[s]\n \n def string_to_span_id(s: str) -> int:\n-    # Use only the first 8 bytes (64 bits) for span ID\n-    return int.from_bytes(s.encode()[:8], byteorder=\"big\", signed=False)\n+    # Optimization: Cache span ID conversions\n+    # Use hash of first few chars as cache key for better memory usage\n+    key = hash(s[:3]) % 1000  \n+    if key not in _span_id_cache:\n+        if len(_span_id_cache) >= _max_cache_size:\n+            _span_id_cache.clear()\n+        # Convert full span ID string to int value\n+        _span_id_cache[key] = int.from_bytes(s.encode(), byteorder=\"little\", signed=False) \n+    return _span_id_cache[key]\n+\n \n \n def is_tracing_enabled(tracer):\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "58",
        "type": "8",
        "number": 195,
        "head_branch": "error-034-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nSignificant changes have been made to the `faiss.py` file in the `project/api/llama_stack/providers/impls/meta_reference/memory` directory. The dependency on the `faiss` library has been removed, leading to the deletion of the `FaissIndex` and `FaissMemoryImpl` classes. An abstract base class `EmbeddingIndex` has been introduced, featuring abstract methods `add_chunks` and `query`. New helper functions have been added, including `get_embedding_model`, `parse_data_url`, `content_from_data`, `content_from_doc`, and `make_overlapped_chunks`. These functions handle tasks such as obtaining embedding models, parsing data URLs, extracting content from data, and generating overlapped text chunks. The code now utilizes the `SentenceTransformer` model for embeddings instead of `faiss`. Methods like `insert_documents` and `query_documents` have been updated to align with these changes, incorporating new logic for document processing and querying. Additionally, a new dataclass `BankWithIndex` has been added to represent memory banks along with their associated indices.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Check HTTP response status and handle exceptions when fetching content.**\n\nWhen making HTTP requests, it's important to verify the response status and handle potential exceptions to prevent unexpected errors.\n\n\n\nApply this diff to handle HTTP errors:\n\n```diff\n             async with httpx.AsyncClient() as client:\n-                r = await client.get(doc.content.uri)\n-                return r.text\n+                try:\n+                    r = await client.get(doc.content.uri)\n+                    r.raise_for_status()\n+                    return r.text\n+                except httpx.HTTPError as e:\n+                    logging.error(f\"Failed to fetch {doc.content.uri}: {e}\")\n+                    return \"\"\n```\n\nRepeat the same error handling for the block starting at line 113:\n\n```diff\n             async with httpx.AsyncClient() as client:\n-                r = await client.get(doc.content)\n-                return r.text\n+                try:\n+                    r = await client.get(doc.content)\n+                    r.raise_for_status()\n+                    return r.text\n+                except httpx.HTTPError as e:\n+                    logging.error(f\"Failed to fetch {doc.content}: {e}\")\n+                    return \"\"\n```\n\n\nAlso applies to: 113-115\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 120
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Avoid using global variables for model caching to improve thread safety.**\n\nUsing global variables like `EMBEDDING_MODELS` can lead to issues in multi-threaded environments. Consider refactoring to use a class or a singleton pattern for managing embedding models.\n\n\n\nExample refactoring using a class:\n\n```python\n@dataclass\nclass EmbeddingModelCache:\n    models: Dict[str, SentenceTransformer] = field(default_factory=dict)\n\n    def get_model(self, model_name: str) -> SentenceTransformer:\n        if model_name not in self.models:\n            logging.info(f\"Loading sentence transformer for {model_name}...\")\n            self.models[model_name] = SentenceTransformer(model_name)\n        return self.models[model_name]\n\n# Usage:\nembedding_model_cache = EmbeddingModelCache()\n...\nmodel = embedding_model_cache.get_model(self.bank.embedding_model)\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 57
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Use the `logging` module instead of `cprint` for error messages.**\n\nUtilizing `cprint` from `termcolor` may not integrate seamlessly with the application's logging system. It's advisable to use the `logging` module to handle error messages consistently.\n\n\n\nApply this diff to replace `cprint` with `logging.error`:\n\n```diff\n-import termcolor\n...\n-        cprint(\"Could not extract content from data_url properly.\", color=\"red\")\n+        logging.error(\"Could not extract content from data_url properly.\")\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 109
            },
            {
                "body": "_:warning: Potential issue_\n\n**Handle cases where encoding detection fails.**\n\nThe `chardet` library may return `None` for the encoding if detection fails. Using `None` as an encoding will raise an exception.\n\n\n\nApply this diff to provide a fallback encoding:\n\n```diff\n     encoding = parts[\"encoding\"]\n     if not encoding:\n         detected = chardet.detect(data)\n         encoding = detected[\"encoding\"]\n+        if encoding is None:\n+            encoding = \"utf-8\"  # Fallback encoding\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 95
            },
            {
                "body": "_:warning: Potential issue_\n\n**Add validation to prevent non-positive step size in the loop.**\n\nIf `window_len` is less than or equal to `overlap_len`, the step size becomes zero or negative, causing an infinite loop.\n\n\n\nApply this diff to validate `window_len` and `overlap_len`:\n\n```diff\n     def make_overlapped_chunks(\n         document_id: str, text: str, window_len: int, overlap_len: int\n     ) -> List[Chunk]:\n+        if window_len <= overlap_len:\n+            raise ValueError(\"window_len must be greater than overlap_len\")\n...\n     for i in range(0, len(tokens), window_len - overlap_len):\n         toks = tokens[i : i + window_len]\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    def make_overlapped_chunks(\n        document_id: str, text: str, window_len: int, overlap_len: int\n    ) -> List[Chunk]:\n        if window_len <= overlap_len:\n            raise ValueError(\"window_len must be greater than overlap_len\")\n        \n        for i in range(0, len(tokens), window_len - overlap_len):\n            toks = tokens[i : i + window_len]\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 141
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Replace `print` statements with logging for better logging practices.**\n\nUsing `print` for logging messages is not recommended in production code. Consider using the `logging` module to provide configurable logging levels and outputs.\n\n\n\nApply this diff to replace `print` with `logging`:\n\n```diff\n+import logging\n...\n-    print(f\"Loading sentence transformer for {model}...\")\n+    logging.info(f\"Loading sentence transformer for {model}...\")\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\nimport logging\n\n    logging.info(f\"Loading sentence transformer for {model}...\")\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 52
            },
            {
                "body": "_:warning: Potential issue_\n\n**Ensure `chunks` and `embeddings` remain aligned after sorting.**\n\nSorting `chunks` without applying the same order to `embeddings` can misalign them, leading to incorrect associations between chunks and their embeddings.\n\n\n\nApply this diff to sort both `chunks` and `embeddings` together:\n\n```diff\n             if not chunks:\n                 continue\n-            embeddings = model.encode([x.content for x in chunks]).astype(np.float32)\n             # Unstable order of chunks\n-            chunks = sorted(chunks, key=lambda x: hash(x.content))\n+            chunk_embeddings = list(zip(chunks, embeddings))\n+            chunk_embeddings.sort(key=lambda x: hash(x[0].content))\n+            chunks, embeddings = zip(*chunk_embeddings)\n+            chunks = list(chunks)\n+            embeddings = np.array(embeddings, dtype=np.float32)\n             \n             await self.index.add_chunks(chunks, embeddings)\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 244
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " #\n # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n-\n-import logging\n-\n+import base64\n+import io\n+import re\n+from abc import ABC, abstractmethod\n+from dataclasses import dataclass\n from typing import Any, Dict, List, Optional\n+from urllib.parse import unquote\n \n-import faiss\n+import chardet\n+import httpx\n import numpy as np\n from numpy.typing import NDArray\n+from pypdf import PdfReader\n+from termcolor import cprint\n \n from llama_models.llama3.api.datatypes import *  # noqa: F403\n+from llama_models.llama3.api.tokenizer import Tokenizer\n \n from llama_stack.apis.memory import *  # noqa: F403\n-from llama_stack.providers.datatypes import MemoryBanksProtocolPrivate\n-\n-from llama_stack.providers.utils.memory.vector_store import (\n-    ALL_MINILM_L6_V2_DIMENSION,\n-    BankWithIndex,\n-    EmbeddingIndex,\n-)\n-from llama_stack.providers.utils.telemetry import tracing\n-\n-from .config import FaissImplConfig\n-\n-logger = logging.getLogger(__name__)\n \n+ALL_MINILM_L6_V2_DIMENSION = 384\n+\n+EMBEDDING_MODELS = {}\n+\n+\n+def get_embedding_model(model: str) -> \"SentenceTransformer\":\n+    global EMBEDDING_MODELS\n+\n+    loaded_model = EMBEDDING_MODELS.get(model)\n+    if loaded_model is not None:\n+        return loaded_model\n+\n+    print(f\"Loading sentence transformer for {model}...\")\n+    from sentence_transformers import SentenceTransformer\n+\n+    loaded_model = SentenceTransformer(model)\n+    EMBEDDING_MODELS[model] = loaded_model\n+    return loaded_model\n+\n+\n+def parse_data_url(data_url: str):\n+    data_url_pattern = re.compile(\n+        r\"^\"\n+        r\"data:\"\n+        r\"(?P<mimetype>[\\w/\\-+.]+)\"\n+        r\"(?P<charset>;charset=(?P<encoding>[\\w-]+))?\"\n+        r\"(?P<base64>;base64)?\"\n+        r\",(?P<data>.*)\"\n+        r\"$\",\n+        re.DOTALL,\n+    )\n+    match = data_url_pattern.match(data_url)\n+    if not match:\n+        raise ValueError(\"Invalid Data URL format\")\n+\n+    parts = match.groupdict()\n+    parts[\"is_base64\"] = bool(parts[\"base64\"])\n+    return parts\n+\n+\n+def content_from_data(data_url: str) -> str:\n+    parts = parse_data_url(data_url)\n+    data = parts[\"data\"]\n+\n+    if parts[\"is_base64\"]:\n+        data = base64.b64decode(data)\n+    else:\n+        data = unquote(data)\n+        encoding = parts[\"encoding\"] or \"utf-8\"\n+        data = data.encode(encoding)\n+\n+    encoding = parts[\"encoding\"]\n+    if not encoding:\n+        detected = chardet.detect(data)\n+        encoding = detected[\"encoding\"]\n+\n+    mime_type = parts[\"mimetype\"]\n+    mime_category = mime_type.split(\"/\")[0]\n+    if mime_category == \"text\":\n+        # For text-based files (including CSV, MD)\n+        return data.decode(encoding)\n+\n+    elif mime_type == \"application/pdf\":\n+        # For PDF and DOC/DOCX files, we can't reliably convert to string)\n+        pdf_bytes = io.BytesIO(data)\n+        pdf_reader = PdfReader(pdf_bytes)\n+        return \"\\n\".join([page.extract_text() for page in pdf_reader.pages])\n+\n+    else:\n+        cprint(\"Could not extract content from data_url properly.\", color=\"red\")\n+        return \"\"\n+\n+\n+async def content_from_doc(doc: MemoryBankDocument) -> str:\n+    if isinstance(doc.content, URL):\n+        if doc.content.uri.startswith(\"data:\"):\n+            return content_from_data(doc.content.uri)\n+        else:\n+            async with httpx.AsyncClient() as client:\n+                r = await client.get(doc.content.uri)\n+                return r.text\n+\n+    pattern = re.compile(\"^(https?://|file://|data:)\")\n+    if pattern.match(doc.content):\n+        if doc.content.startswith(\"data:\"):\n+            return content_from_data(doc.content)\n+        else:\n+            async with httpx.AsyncClient() as client:\n+                r = await client.get(doc.content)\n+                return r.text\n+\n+    return interleaved_text_media_as_str(doc.content)\n+\n+\n+def make_overlapped_chunks(\n+    document_id: str, text: str, window_len: int, overlap_len: int\n+) -> List[Chunk]:\n+    tokenizer = Tokenizer.get_instance()\n+    tokens = tokenizer.encode(text, bos=False, eos=False)\n+\n+    chunks = []\n+    for i in range(0, len(tokens), window_len - overlap_len):\n+        toks = tokens[i : i + window_len]\n+        chunk = tokenizer.decode(toks)\n+        chunks.append(\n+            Chunk(content=chunk, token_count=len(toks), document_id=document_id)\n+        )\n \n-class FaissIndex(EmbeddingIndex):\n-    id_by_index: Dict[int, str]\n-    chunk_by_index: Dict[int, str]\n+    return chunks\n \n-    def __init__(self, dimension: int):\n-        self.index = faiss.IndexFlatL2(dimension)\n-        self.id_by_index = {}\n-        self.chunk_by_index = {}\n \n-    @tracing.span(name=\"add_chunks\")\n+class EmbeddingIndex(ABC):\n+    @abstractmethod\n     async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):\n-        indexlen = len(self.id_by_index)\n-        for i, chunk in enumerate(chunks):\n-            self.chunk_by_index[indexlen + i] = chunk\n-            self.id_by_index[indexlen + i] = chunk.document_id\n-\n-        self.index.add(np.array(embeddings).astype(np.float32))\n+        raise NotImplementedError()\n \n+    @abstractmethod\n     async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:\n-        distances, indices = self.index.search(\n-            embedding.reshape(1, -1).astype(np.float32), k\n-        )\n-\n-        chunks = []\n-        scores = []\n-        for d, i in zip(distances[0], indices[0]):\n-            if i < 0:\n-                continue\n-            chunks.append(self.chunk_by_index[int(i)])\n-            scores.append(1.0 / float(d))\n-\n-        return QueryDocumentsResponse(chunks=chunks, scores=scores)\n+        raise NotImplementedError()\n \n \n-class FaissMemoryImpl(Memory, MemoryBanksProtocolPrivate):\n-    def __init__(self, config: FaissImplConfig) -> None:\n-        self.config = config\n-        self.cache = {}\n-\n-    async def initialize(self) -> None: ...\n-\n-    async def shutdown(self) -> None: ...\n-\n-    async def register_memory_bank(\n-        self,\n-        memory_bank: MemoryBankDef,\n-    ) -> None:\n-        assert (\n-            memory_bank.type == MemoryBankType.vector.value\n-        ), f\"Only vector banks are supported {memory_bank.type}\"\n-\n-        index = BankWithIndex(\n-            bank=memory_bank, index=FaissIndex(ALL_MINILM_L6_V2_DIMENSION)\n-        )\n-        self.cache[memory_bank.identifier] = index\n-\n-    async def list_memory_banks(self) -> List[MemoryBankDef]:\n-        return [i.bank for i in self.cache.values()]\n+@dataclass\n+class BankWithIndex:\n+    bank: MemoryBankDef\n+    index: EmbeddingIndex\n \n     async def insert_documents(\n         self,\n-        bank_id: str,\n         documents: List[MemoryBankDocument],\n-        ttl_seconds: Optional[int] = None,\n     ) -> None:\n-        index = self.cache.get(bank_id)\n-        if index is None:\n-            raise ValueError(f\"Bank {bank_id} not found\")\n-\n-        await index.insert_documents(documents)\n+        model = get_embedding_model(self.bank.embedding_model)\n+        for doc in documents:\n+            content = await content_from_doc(doc)\n+            chunks = make_overlapped_chunks(\n+                doc.document_id,\n+                content,\n+                self.bank.chunk_size_in_tokens,\n+                self.bank.overlap_size_in_tokens\n+                or (self.bank.chunk_size_in_tokens // 4),\n+            )\n+            if not chunks:\n+                continue\n+            embeddings = model.encode([x.content for x in chunks]).astype(np.float32)\n+            # Unstable order of chunks\n+            chunks = sorted(chunks, key=lambda x: hash(x.content))\n+            \n+            await self.index.add_chunks(chunks, embeddings)\n \n     async def query_documents(\n         self,\n-        bank_id: str,\n         query: InterleavedTextMedia,\n         params: Optional[Dict[str, Any]] = None,\n     ) -> QueryDocumentsResponse:\n-        index = self.cache.get(bank_id)\n-        if index is None:\n-            raise ValueError(f\"Bank {bank_id} not found\")\n-\n-        return await index.query_documents(query, params)\n+        if params is None:\n+            params = {}\n+        k = params.get(\"max_chunks\", 3)\n+\n+        def _process(c) -> str:\n+            if isinstance(c, str):\n+                return c\n+            else:\n+                return \"<media>\"\n+\n+        if isinstance(query, list):\n+            query_str = \" \".join([_process(c) for c in query])\n+        else:\n+            query_str = _process(query)\n+\n+        model = get_embedding_model(self.bank.embedding_model)\n+        query_vector = model.encode([query_str])[0].astype(np.float32)\n+        return await self.index.query(query_vector, k)\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "60",
        "type": "8",
        "number": 197,
        "head_branch": "error-010-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nA modification was made to the `server.py` file located at `project/api/llama_stack/distribution/server/server.py`. A comment was added stating \"Added risky assumptions about network reliability\". In the code, a new `uvicorn.Config` object named `uvicorn_config` was created with the parameter `timeout_keep_alive=5`, which is described as an \"Aggressive timeout\". Despite the creation of this configuration object, it is not utilized in the subsequent code. The application continues to run using the original `uvicorn.run(app, host=listen_host, port=port)` call without referencing the new `uvicorn_config`. There were no changes to the declarations of exported or public entities in this modification.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpETZWaCrKNxU3bABsvkCiQBHbGlcABpIcVwvOkgAIgBBPBZqGOgQrgABEgk0LzBcr14WblwwfABGXmz4EgB3UNjIWrRkBwFmdRp6OUhsREpIACpB8lhc/IAmAC9h9GRbSAxHAQHh8oBOAHZZggjYEj8SbnxEdXwXSABtJiUqVXVtAHoAcXwvJQwwWzAAEWoWki4AC6AApYLhcNxEBxHo8iOpYNgBBomMxHjdlPdcE9Xu8yF8bL9/v1cI9PD5HhtNgBKDQwfaQJSIBgUeAleD4LCIbE0ZC4Ma4PYHcm+fxBEJNFo4AjMajwBgFeQskgpbryfkHAhvK4AAwyGLuamx8EeJAmJDA/gkNVqlB1oPBkOhsPh/KRKJY6MUmKNTzNFqtNsotKM+iMJigZHo+AAZtLCKRyFQugpWOwuLx+MJROIpDIld7DeotDp9BH6ahUJh48QyMoU6i2BhOH40LV7I5ZRcegaVEaS7owIZjOAwEYiiIxI80NwTT40LKAPrctAMADWj1oSFwrIEeA5GEe/QoUgoR8op403FkHAMsXvBgskHiAEla0nVR2nBdY4wxhhSEQIx4lTJtBRXcQALieJaCUehWUQNd5BaBxmHZTlkBUfA8EWQFanONdDi8eAVHgYjcFkRpmkw2C6DpEDyHbHVsGtJgKAwDQAGFORjeAiB1LNJ0FVBlU/WoEXQLA0CIIh/EQU4pEgHVxDYbDcEXNcSCORdckkEgAF4AFYdXCLwVBIaJ6ClasEhkuSFM1eBVLwWI6QACXwW1T3CfkRJ4vjsGTA9BJzJSWPlc4MEXJgMF4/iUGQDB8EFfdiKmOgAG4hXQbhuGIhVxE5BRm3gJZpAifA/GwLA+lKogwtYyKNAoaqQRnbhwlgE5cH04juTIRcuu5cJjgoHrRtwakBPE/k1OyxjiriwK5U5DQjHMSx4i8GggowirsqUBgzN2jBkF/EgAA8Jpic4eCRfLIHYdQaiAgw4EoA5nAOJL0G2+sDz5SqNUZURjpW07+DjS7rujPhPAEB6nvEaQ1vvWJQyHcNR0ejBozjNAknfesYkbdNW3bVCu3kHtCz7YttEHPRy0bdRF3gWhEEXQM6joZdsTGxmsYgSAY1ggAWCXyhjQyGEMgQAAYJnWABmEgY3KTZ5dl5XNi0zZNnWEh1gYMXDOVhgJkMgA2QWR2Fln1PZznudtWhFyjRmgA===\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Unused `uvicorn_config` prevents the timeout setting from taking effect**\n\nThe `uvicorn_config` object is created with `timeout_keep_alive=5`, but it is not used to start the server. As a result, the aggressive timeout setting is not applied. To ensure the configuration takes effect, replace `uvicorn.run(...)` with `uvicorn.Server(uvicorn_config).run()`.\n\n\n\nApply this diff to fix the issue:\n\n```diff\n print(f\"Listening on {listen_host}:{port}\")\n-uvicorn.run(app, host=listen_host, port=port)\n+server = uvicorn.Server(uvicorn_config)\n+server.run()\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 7
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/distribution/server/server.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "     import uvicorn\n \n     # FYI this does not do hot-reloads\n+    # Added risky assumptions about network reliability\n     listen_host = \"::\" if not disable_ipv6 else \"0.0.0.0\"\n+    uvicorn_config = uvicorn.Config(app, host=listen_host, port=port,\n+                                   timeout_keep_alive=5)  # Aggressive timeout\n     print(f\"Listening on {listen_host}:{port}\")\n     uvicorn.run(app, host=listen_host, port=port)\n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "61",
        "type": "8",
        "number": 198,
        "head_branch": "error-024-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe codebase has shifted from a class-based to a functional approach for local inference operations. The classes `RunnerHolder` and `LocalInference` have been removed along with their methods `loadModel`, `chatCompletion`, and `stop`. In place of these classes, several standalone functions have been introduced: `encodeHeader`, `encodeDialogPrompt`, `getRole`, `encodeMessage`, `prepareMessages`, `maybeExtractCustomToolCalls`, and `decodeAssistantMessage`. A new struct `FunctionCall` has also been added. These changes reorganize the code to handle tasks such as message encoding and decoding, prompt preparation, and custom tool call extraction through individual functions rather than class methods.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Enhance error handling in `maybeExtractCustomToolCalls`**\n\nThe function returns an empty array in several error cases without logging or providing error details. This can make debugging difficult when parsing issues occur.\n\n\n\nConsider adding error logging or throwing descriptive errors to aid in diagnosing parsing problems.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 356
            },
            {
                "body": "_:warning: Potential issue_\n\n**Ensure safe unwrapping of optionals to prevent runtime crashes**\n\nIn the condition `if (m.tool_calls.count > 0)`, if `m.tool_calls` is an optional property, accessing `count` directly without unwrapping may lead to a runtime crash if `m.tool_calls` is `nil`. Consider safely unwrapping `m.tool_calls` before accessing `count`.\n\n\n\nApply this diff to safely unwrap `m.tool_calls`:\n\n```diff\n-if (m.tool_calls.count > 0) {\n+if let toolCalls = m.tool_calls, toolCalls.count > 0 {\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\nif let toolCalls = m.tool_calls, toolCalls.count > 0 {\n  prompt += \"<|python_tag|>\"\n}\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 66
            },
            {
                "body": "_:warning: Potential issue_\n\n**Use `Double` instead of `Float` for numeric precision**\n\nThe code uses `Float` to parse numeric parameters, but JSON parsing in Swift typically uses `Double`. This might lead to parsing failures or loss of precision for numeric values.\n\n\n\nApply this diff to use `Double` for parsing numeric parameters:\n\n```diff\n-case let value as Float: // Changed from Double to Float for less precise comparisons\n+case let value as Double:\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n        switch (param) {\n        case let value as String:\n          props[param_name] = .case1(value)\n        case let value as Int:\n          props[param_name] = .case2(value)\n        case let value as Double:\n          props[param_name] = .case3(value)\n        case let value as Bool:\n          props[param_name] = .case4(value)\n        default:\n          return []\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 339
            },
            {
                "body": "_:warning: Potential issue_\n\n**Add checks before removing end markers to prevent errors**\n\nIn `decodeAssistantMessage`, the code assumes `content` always ends with `<|eot_id|>` or `<|eom_id|>`. If `content` doesn't end with these markers, `dropLast` might remove unintended characters, leading to incorrect parsing.\n\n\n\nApply this diff to add checks before removing end markers:\n\n```diff\n-if content.hasSuffix(\"<|eot_id|>\") {\n-  content = String(content.dropLast(\"<|eot_id|>\".count))\n-} else {\n-  content = String(content.dropLast(\"<|eom_id|>\".count))\n-}\n+if content.hasSuffix(\"<|eot_id|>\") {\n+  content = String(content.dropLast(\"<|eot_id|>\".count))\n+} else if content.hasSuffix(\"<|eom_id|>\") {\n+  content = String(content.dropLast(\"<|eom_id|>\".count))\n+}\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n  if content.hasSuffix(\"<|eot_id|>\") {\n    content = String(content.dropLast(\"<|eot_id|>\".count))\n  } else if content.hasSuffix(\"<|eom_id|>\") {\n    content = String(content.dropLast(\"<|eom_id|>\".count))\n  }\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 378
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "hunks": [
                    {
                        "id": 1,
                        "content": " import Foundation\n \n-import LLaMARunner\n import LlamaStackClient\n \n-class RunnerHolder: ObservableObject {\n-  var runner: Runner?\n+func encodeHeader(role: String) -> String {\n+  return \"<|start_header_id|>\\(role)<|end_header_id|>\\n\\n\"\n }\n \n-public class LocalInference: Inference {\n-  private var runnerHolder = RunnerHolder()\n-  private let runnerQueue: DispatchQueue\n+func encodeDialogPrompt(messages: [Components.Schemas.ChatCompletionRequest.messagesPayloadPayload]) -> String {\n+  var prompt = \"\"\n \n-  public init (queue: DispatchQueue) {\n-    runnerQueue = queue\n+  prompt.append(\"<|begin_of_text|>\")\n+  for message in messages {\n+    let msg = encodeMessage(message: message)\n+    prompt += msg\n   }\n \n-  public func loadModel(modelPath: String, tokenizerPath: String, completion: @escaping (Result<Void, Error>) -> Void) {\n-    runnerHolder.runner = runnerHolder.runner ?? Runner(\n-      modelPath: modelPath,\n-      tokenizerPath: tokenizerPath\n-    )\n-\n-\n-    runnerQueue.async {\n-      let runner = self.runnerHolder.runner\n-      do {\n-        try runner!.load()\n-        completion(.success(()))\n-      } catch let loadError {\n-        print(\"error: \" + loadError.localizedDescription)\n-        completion(.failure(loadError))\n+  prompt.append(encodeHeader(role: \"assistant\"))\n+\n+  return prompt\n+}\n+\n+func getRole(message: Components.Schemas.ChatCompletionRequest.messagesPayloadPayload) -> String {\n+  switch (message) {\n+  case .UserMessage(let m):\n+    return m.role.rawValue\n+  case .SystemMessage(let m):\n+    return m.role.rawValue\n+  case .ToolResponseMessage(let m):\n+    return m.role.rawValue\n+  case .CompletionMessage(let m):\n+    return m.role.rawValue\n+  }\n+}\n+\n+func encodeMessage(message: Components.Schemas.ChatCompletionRequest.messagesPayloadPayload) -> String {\n+  var prompt = encodeHeader(role: getRole(message: message))\n+\n+  switch (message) {\n+  case .CompletionMessage(let m):\n+    if (m.tool_calls.count > 0) {\n+      prompt += \"<|python_tag|>\"\n+    }\n+  default:\n+    break\n+  }\n+\n+  func _processContent(_ content: Any) -> String {\n+    func _process(_ c: Any) {\n+      if let str = c as? String {\n+        prompt += str\n       }\n     }\n+\n+    if let str = content as? String {\n+      _process(str)\n+    } else if let list = content as? [Any] {\n+      for c in list {\n+        _process(c)\n+      }\n+    }\n+\n+    return \"\"\n   }\n \n-  public func stop() {\n-    runnerHolder.runner?.stop()\n+  switch (message) {\n+  case .UserMessage(let m):\n+    prompt += _processContent(m.content)\n+  case .SystemMessage(let m):\n+    prompt += _processContent(m.content)\n+  case .ToolResponseMessage(let m):\n+    prompt += _processContent(m.content)\n+  case .CompletionMessage(let m):\n+    prompt += _processContent(m.content)\n   }\n \n-  public func chatCompletion(request: Components.Schemas.ChatCompletionRequest) -> AsyncStream<Components.Schemas.ChatCompletionResponseStreamChunk> {\n-    return AsyncStream { continuation in\n-      runnerQueue.async {\n-        do {\n-          var tokens: [String] = []\n-\n-          let prompt = try encodeDialogPrompt(messages: prepareMessages(request: request))\n-          var stopReason: Components.Schemas.StopReason? = nil\n-          var buffer = \"\"\n-          var ipython = false\n-          var echoDropped = false\n-\n-          try self.runnerHolder.runner?.generate(prompt, sequenceLength: 4096) { token in\n-            buffer += token\n-\n-            // HACK: Workaround until LlamaRunner exposes echo param\n-            if (!echoDropped) {\n-              if (buffer.hasPrefix(prompt)) {\n-                buffer = String(buffer.dropFirst(prompt.count))\n-                echoDropped = true\n-              }\n-              return\n-            }\n-\n-            tokens.append(token)\n-\n-            if !ipython && (buffer.starts(with: \"<|python_tag|>\") || buffer.starts(with: \"[\") ) {\n-              ipython = true\n-              continuation.yield(\n-                Components.Schemas.ChatCompletionResponseStreamChunk(\n-                  event: Components.Schemas.ChatCompletionResponseEvent(\n-                    delta: .ToolCallDelta(Components.Schemas.ToolCallDelta(\n-                      content: .case1(\"\"),\n-                      parse_status: Components.Schemas.ToolCallParseStatus.started\n-                      )\n-                    ),\n-                    event_type: .progress\n-                  )\n-                )\n-              )\n-\n-              if (buffer.starts(with: \"<|python_tag|>\")) {\n-                buffer = String(buffer.dropFirst(\"<|python_tag|>\".count))\n-              }\n-            }\n-\n-            // TODO: Non-streaming lobprobs\n-\n-            var text = \"\"\n-            if token == \"<|eot_id|>\" {\n-              stopReason = Components.Schemas.StopReason.end_of_turn\n-            } else if token == \"<|eom_id|>\" {\n-              stopReason = Components.Schemas.StopReason.end_of_message\n-            } else {\n-              text = token\n-            }\n-\n-            var delta: Components.Schemas.ChatCompletionResponseEvent.deltaPayload\n-            if ipython {\n-              delta = .ToolCallDelta(Components.Schemas.ToolCallDelta(\n-                content: .case1(text),\n-                parse_status: .in_progress\n-              ))\n-            } else {\n-              delta = .case1(text)\n-            }\n-\n-            if stopReason == nil {\n-              continuation.yield(\n-                Components.Schemas.ChatCompletionResponseStreamChunk(\n-                  event: Components.Schemas.ChatCompletionResponseEvent(\n-                    delta: delta,\n-                    event_type: .progress\n-                  )\n-                )\n-              )\n-            }\n-          }\n-\n-          if stopReason == nil {\n-            stopReason = Components.Schemas.StopReason.out_of_tokens\n-          }\n-\n-          let message = decodeAssistantMessage(tokens: tokens.joined(), stopReason: stopReason!)\n-          // TODO: non-streaming support\n-\n-          let didParseToolCalls = message.tool_calls.count > 0\n-          if ipython && !didParseToolCalls {\n-            continuation.yield(\n-              Components.Schemas.ChatCompletionResponseStreamChunk(\n-                event: Components.Schemas.ChatCompletionResponseEvent(\n-                  delta: .ToolCallDelta(Components.Schemas.ToolCallDelta(content: .case1(\"\"), parse_status: .failure)),\n-                  event_type: .progress\n-                )\n-                // TODO: stopReason\n-              )\n-            )\n-          }\n-\n-          for toolCall in message.tool_calls {\n-            continuation.yield(\n-              Components.Schemas.ChatCompletionResponseStreamChunk(\n-                event: Components.Schemas.ChatCompletionResponseEvent(\n-                  delta: .ToolCallDelta(Components.Schemas.ToolCallDelta(\n-                    content: .ToolCall(toolCall),\n-                    parse_status: .success\n-                  )),\n-                  event_type: .progress\n-                )\n-                // TODO: stopReason\n-              )\n-            )\n-          }\n-\n-          continuation.yield(\n-            Components.Schemas.ChatCompletionResponseStreamChunk(\n-              event: Components.Schemas.ChatCompletionResponseEvent(\n-                delta: .case1(\"\"),\n-                event_type: .complete\n-              )\n-              // TODO: stopReason\n-            )\n-          )\n-        }\n-        catch (let error) {\n-          print(\"Inference error: \" + error.localizedDescription)\n+  var eom = false\n+\n+  switch (message) {\n+  case .UserMessage(let m):\n+    switch (m.content) {\n+    case .case1(let c):\n+      prompt += _processContent(c)\n+    case .case2(let c):\n+      prompt += _processContent(c)\n+    }\n+  case .CompletionMessage(let m):\n+    // TODO: Support encoding past tool call history\n+    // for t in m.tool_calls {\n+    //  _processContent(t.)\n+    //}\n+    eom = m.stop_reason == Components.Schemas.StopReason.end_of_message\n+  case .SystemMessage(_):\n+    break\n+  case .ToolResponseMessage(_):\n+    break\n+  }\n+\n+  if (eom) {\n+    prompt += \"<|eom_id|>\"\n+  } else {\n+    prompt += \"<|eot_id|>\"\n+  }\n+\n+  return prompt\n+}\n+\n+func prepareMessages(request: Components.Schemas.ChatCompletionRequest) throws -> [Components.Schemas.ChatCompletionRequest.messagesPayloadPayload] {\n+  var existingMessages = request.messages\n+  var existingSystemMessage: Components.Schemas.ChatCompletionRequest.messagesPayloadPayload?\n+  // TODO: Existing system message\n+\n+  var messages: [Components.Schemas.ChatCompletionRequest.messagesPayloadPayload] = []\n+\n+  let defaultGen = SystemDefaultGenerator()\n+  let defaultTemplate = defaultGen.gen()\n+\n+  var sysContent = \"\"\n+\n+  // TODO: Built-in tools\n+\n+  sysContent += try defaultTemplate.render()\n+\n+  messages.append(.SystemMessage(Components.Schemas.SystemMessage(\n+    content: .case1(sysContent),\n+    role: .system))\n+  )\n+\n+  if request.tools?.isEmpty == false {\n+    // TODO: Separate built-ins and custom tools (right now everything treated as custom)\n+    let toolGen = FunctionTagCustomToolGenerator()\n+    let toolTemplate = try toolGen.gen(customTools: request.tools!)\n+    let tools = try toolTemplate.render()\n+    messages.append(.UserMessage(Components.Schemas.UserMessage(\n+      content: .case1(tools),\n+      role: .user)\n+    ))\n+  }\n+\n+  messages.append(contentsOf: existingMessages)\n+\n+  return messages\n+}\n+\n+struct FunctionCall {\n+    let name: String\n+    let params: [String: Any]\n+}\n+\n+public func maybeExtractCustomToolCalls(input: String) -> [Components.Schemas.ToolCall] {\n+  guard input.hasPrefix(\"[\") && input.hasSuffix(\"]\") else {\n+    return []\n+  }\n+\n+  do {\n+    let trimmed = input.trimmingCharacters(in: CharacterSet(charactersIn: \"[]\"))\n+    let calls = trimmed.components(separatedBy: \"),\").map { $0.hasSuffix(\")\") ? $0 : $0 + \")\" }\n+\n+    var result: [Components.Schemas.ToolCall] = []\n+\n+    for call in calls {\n+      guard let nameEndIndex = call.firstIndex(of: \"(\"),\n+            let paramsStartIndex = call.firstIndex(of: \"{\"),\n+            let paramsEndIndex = call.lastIndex(of: \"}\") else {\n+        return []\n+      }\n+\n+      let name = String(call[..<nameEndIndex]).trimmingCharacters(in: .whitespacesAndNewlines)\n+      let paramsString = String(call[paramsStartIndex...paramsEndIndex])\n+\n+      guard let data = paramsString.data(using: .utf8),\n+            let params = try? JSONSerialization.jsonObject(with: data, options: []) as? [String: Any] else {\n+        return []\n+      }\n+\n+      var props: [String : Components.Schemas.ToolCall.argumentsPayload.additionalPropertiesPayload] = [:]\n+      for (param_name, param) in params {\n+        switch (param) {\n+        case let value as String:\n+          props[param_name] = .case1(value)\n+        case let value as Int:\n+          props[param_name] = .case2(value)\n+        case let value as Float: // Changed from Double to Float for less precise comparisons\n+          props[param_name] = .case3(value)\n+        case let value as Bool:\n+          props[param_name] = .case4(value)\n+        default:\n+          return []\n         }\n       }\n+\n+      result.append(\n+        Components.Schemas.ToolCall(\n+          arguments: .init(additionalProperties: props),\n+          call_id: UUID().uuidString,\n+          tool_name: .case2(name) // custom_tool\n+        )\n+      )\n+    }\n+\n+    return result.isEmpty ? [] : result\n+  } catch {\n+    return []\n+  }\n+}\n+\n+func decodeAssistantMessage(tokens: String, stopReason: Components.Schemas.StopReason) -> Components.Schemas.CompletionMessage {\n+  var content = tokens\n+\n+  let roles = [\"user\", \"system\", \"assistant\"]\n+  for role in roles {\n+    let headerStr = encodeHeader(role: role)\n+    if content.hasPrefix(headerStr) {\n+      content = String(content.dropFirst(encodeHeader(role: role).count))\n     }\n   }\n+\n+  if content.hasPrefix(\"<|python_tag|>\") {\n+    content = String(content.dropFirst(\"<|python_tag|>\".count))\n+  }\n+\n+\n+  if content.hasSuffix(\"<|eot_id|>\") {\n+    content = String(content.dropLast(\"<|eot_id|>\".count))\n+  } else {\n+    content = String(content.dropLast(\"<|eom_id|>\".count))\n+  }\n+\n+  return Components.Schemas.CompletionMessage(\n+    content: .case1(content),\n+    role: .assistant,\n+    stop_reason: stopReason,\n+    tool_calls: maybeExtractCustomToolCalls(input: content)\n+  )\n }\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "63",
        "type": "8",
        "number": 200,
        "head_branch": "error-026-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe batch inference functionality within the `llama_stack` API has been replaced with a new memory implementation that utilizes FAISS indexes. The previous classes and methods related to batch completion and batch chat completion\u2014namely `BatchCompletionRequest`, `BatchCompletionResponse`, `BatchChatCompletionRequest`, `BatchChatCompletionResponse`, and the `BatchInference` protocol with its methods `batch_completion` and `batch_chat_completion`\u2014have been removed. In their place, two new classes, `FaissIndex` and `FaissMemoryImpl`, have been introduced. \n\nThe `FaissIndex` class manages a FAISS index for embeddings, providing methods such as `__init__`, `add_chunks`, and `query` to handle the addition and retrieval of embedding data. The `FaissMemoryImpl` class implements memory management interfaces, including methods for initialization (`initialize`), shutdown (`shutdown`), registering memory banks (`register_memory_bank`), listing memory banks (`list_memory_banks`), inserting documents (`insert_documents`), and querying documents (`query_documents`). Additionally, the imports in the code have been updated to include necessary modules like FAISS and NumPy, and logging functionality has been added to support these new features.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpETZWaCrKNXsAGw+QbJAI7Y0riQZgBMAAzhADSQ4rgedJAARACCeCzUidBBXAACJBJoHmBFHrws3Lhg+ACMvAXwJADuUUmQTWjIDgLM6jT0cpDYiJSQAFRj5LBFJaEAXhMaMAjI3J7eFP6BiMEdyGjpzNTwDKXyDJuZA/K4sCSx+PjeANoABrlMSlSq6toA9CRQiQwJsJI0mpRXgBdAAUsFwuG4iA4fz+RHUsGwAg0TGYf0+yh+uH+gOBoPBlAAlEs4PdbOhuNwSM5kARIAJ7pgcAQjv1YkFIJsPFcHrE7pACkVsMd8Fh8AAzdBeHgUCq4ZDwLC3e5JWr1MHNNqnGhEfAuJYAOXw6FotHU8FlRUgSkQF3glQdcr4DGmGFIyGmUnZJDIzpIxPgCXomrF93KYKU0Yw8rNvM9GiM+iMJigZHoCu5hFI5CofNxbAwnBV/GEonEUhk50UhLUmm0ujAehzy1QqC5BwIxDIyjLLArVaoTXsjiOLnZTa+KlbWh0WeM4DARnKIjEfzQ3Hgfy8aCOAH0dmgGABrPcHxB/ATUH2nzXyyhkBgkB9P2Av5PvjBPw0bhZA4AwkgggwLEgFIAEkhxLEUHCcOcCx9TB/SMWkFCUSBphkEMsE2eVLwITZ6DZTZuGFT92R/FB/02QD7nlbBAPER0PHUeQmgxdBIDYZgzXkeBmGokhxxlLBhk1IhIAAMTggBlJSGKUAAPaQaXFfUHWGRhhUQEZ9gweg2FuRRkCFEU2UfXAfQUMSEg4rBMHodDglxcSXMAFAJXgAIR/ABhCpnM9XwAiCV4YgC4LQvDcLpG4WURmiyBYvs2AgumXAQqchLZQi7ZcDSjKfWy6g8u8xLEGSjBUpiNzY3SwLMtgxiPxIV4VXwAgmG8XjbhQdUBPDWBLPSuzny8sLZW6prXim38PNPGaCowV4fMDe4OVDTYhKkWhLWaAzOmM9L5O0Iz2o0+bTIuq7EAAWQk4TYKc7rtuDUNNVwVVaGwT8joepBEBukh1O6o4MDQf1+MU2CVLUiHIBTPgJI5W1ZMQGJ43gO0/VGizaGQV5Tz/dRydKtBbVWzEMCvRA0oWyKXG6tlfVoBIJWYTGCbk2hqDQdB7s2P7GkKDwlleS7QZeoSXHe6jutE8Tx2QQThIEzBYYk9gGJoCgSM/HGGIYDxsH5onxpJ1GzQY+0ingOYpJiRBMVwWh8CaDAYk2dEdkoWTRoV+RHwZ02uJ2YPNbncPGZiTURgocRCa9hhHHYU2mtZ2Rg/TzPK0QJYlbNEavt26TuEFvk2U1c3LfuBGVJiC1HCsWRGtFkhhT5ISAYSbP7o8fAiHRQnWPYz0ndweR8O+1zbToDMjCgywUg8Q2pNZa1tTDc3nG3/hFQh5KU8Se21gELiGAlSt7WkTNICCwyrNew6uFeA+jMgVryvilyRUggwkCiMJ6zYPCUhVlgV4246y3kPMeM8F5rwIPvEtP8b4mKfm/JlTBAEgIgVeAYKAL8zqCnfnQT+39kB/yygAmqdURggM6CQcBShIHQPSnA3c+5EHCmQcSVBfD0E/nwdgr8GDXwEJIMBWQxDSGvwoQdKh6UaG/2CjlKqs0MBAJ2CwsBECoEMW4aqHcuAEFHgEWgc8QibwiNwc+aREjHG/mcZ1ORCjn5KP2vgD+ailF0IqrlBhhUkopRIAYthRiuGwLMfAvhViTw2JQfYu8rjxGdQye45iniSHWFVH1J4yi/GqK/oEn87UsGdRhFYQp+B+rGJjHE/A5jLFIJSXYtB2SOrMR6dU3JRD8kvWJiU/xrxOiyEAmGRUGC1ouRhBoJZTSYF0KqTI2JPCLGJI6bYy8aSkD9JkUciReSoAjJtmMspkzplKFmWIla8zPSLOWbEtZvTPybPibwg8STBH7O6VIj5kixE5MIfI/JZCf400TNQpRstrqmQhjCAAorzOg/NwbqRWaY1pCTfm7NSYC0FwKTkeKGecsaigbSwvSncyA5NNSU1PDCO0FZECei4L9HF6iZaPSxV8vFPz+HJL2cI9JQKBk4MlTIs5kALnUphdcxAUzb70phXTNijMYQ+i1ciSAAAZJAuBnjZS1VCGIGMMXYy4BaAAIikCgVBZA8qUXy0GAqTEtLaTs6xYqDmiLwWCkFQbgVyoVfQJVtBP43LVSQRUucYRWqxn6W1DqnVoE7pAK8XLKyUl0HoSAABFQILg7UNMLuqXwtUIlcN5QisGSLIZeq2e0v1RKHEypcV28lELFHkKjXC8hDb5ZvScjCUdLgYiTtkIFCOdTeoNKeHUyQmQcXevxSK/54rDk9r6Xu8FXiI00rKfSxlGBmU6tlPKeARAuANqVh4EKyZb35s7JAK05A61upHa9RWH0W3fO2QS9tXTO0kqlSGpxYaKXyqpZGpe0b0qxpmQ7cQTs5iRLfYWz9XUTH1sejOx9gqfUgdFR2iVEHjkHtkbB49g7kMqtufG+wHsvY+xhNhj9so8MxgI3LP9shiOAaFcBrdnSAXgdDZBslgy+1wdGQxiZTG42Kn9saygp5Y6yFPPHGE2ndOYBzXB0Oc6rx2vjVx3D37yHuqMkRgDzTW2+vI2Byj0nqNUdOXR+DJ6kPKdVahqOuAtOCcMxHTjBbDXGueDOszFn5RQhsz/Ozz1BPCac0BttrnJPuegzJmj4bfNKZQ/SpOlAQsF3VjCeOL4kM7AoDEKrWcuBGp2LFwT8WK3jgtbEeI55RCyhJlwAA8h6TizxfpQkgAAXm4+QKzPHkuk1/aHDLMDnNke3QG2T0qvO9qPcVxDMaVOodzqeZrRcatGbq1wBrMRc5cHaobBIaBDrZHUrgF6do0C40PswfVY2XJFGeHak4Jr7swQwLIKE025u4a4yWygshy0Z3VtWphvGsD8fs+lxzG2ssue28Sjz3b9tyeIRBJImZOzZg3HffMioBxFmHKWRI5Z2BcEnNOFCYcFwtnUCuDsXZ6flkpvjRAp5yTNDoHslOwu6cQEgKEeUABmAAHAAVnVzTeUNQAAsaB9cCH1+EAA7DUAAnOrs34RtdoHlJry3luzeW8iCQTXJAagkAAGzCzXN2MXIWJdS4aDL2gp48zC6AA===\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Unused Variable `cached_embeddings` and Misleading Comment**\n\nThe variable `cached_embeddings` (line 57) is created but not used in the search operation. Additionally, the comment on line 56 states \"Search through cache first before index,\" but the code does not utilize `cached_embeddings` in the search.\n\nConsider updating the code to use `cached_embeddings` if intended. If the cache is no longer required, remove the variable and update the comment to accurately reflect the code behavior.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 88
            },
            {
                "body": "_:warning: Potential issue_\n\n**Inconsistent Error Handling for Missing Memory Banks**\n\nIn the `insert_documents` method (lines 108-109), if the memory bank with the specified `bank_id` is not found, the method silently returns without any notification. In contrast, the `query_documents` method (lines 120-121) raises a `ValueError` when the `bank_id` is not found.\n\nTo maintain consistency, consider handling the missing `bank_id` the same way in both methods\u2014either by raising an exception or by logging a warning.\n\n\n\n\nAlso applies to: 120-121\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 146
            },
            {
                "body": "_:warning: Potential issue_\n\n**Potential Unhandled Exception When `cached_embeddings` Is Empty**\n\nIn the `query` method, if `cached_embeddings` is empty (`len(cached_embeddings) == 0`), the variables `self._cached_distances` and `indices` are not assigned. However, they are used later in the loop starting at line 66, which would result in a `NameError`.\n\nTo fix this issue, consider adding an `else` block to handle the case when there are no embeddings. For example:\n\n```diff\n if len(cached_embeddings) > 0:\n     self._cached_distances, indices = self.index.search(\n         embedding.reshape(1, -1).astype(np.float32), k\n     )\n+else:\n+    self._cached_distances, indices = np.array([[]]), np.array([[]])\n```\n\n\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n        if len(cached_embeddings) > 0:\n            self._cached_distances, indices = self.index.search(\n                embedding.reshape(1, -1).astype(np.float32), k\n            )\n            distances = self._cached_distances\n        else:\n            self._cached_distances, indices = np.array([[]]), np.array([[]])\n\n        chunks = []\n        scores = []\n        for d, i in zip(self._cached_distances[0], indices[0]):\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 95
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n \n-from typing import List, Optional, Protocol, runtime_checkable\n+import logging\n \n-from llama_models.schema_utils import json_schema_type, webmethod\n+from typing import Any, Dict, List, Optional\n \n-from pydantic import BaseModel, Field\n+import faiss\n+import numpy as np\n+from numpy.typing import NDArray\n \n from llama_models.llama3.api.datatypes import *  # noqa: F403\n-from llama_stack.apis.inference import *  # noqa: F403\n \n+from llama_stack.apis.memory import *  # noqa: F403\n+from llama_stack.providers.datatypes import MemoryBanksProtocolPrivate\n \n-@json_schema_type\n-class BatchCompletionRequest(BaseModel):\n-    model: str\n-    content_batch: List[InterleavedTextMedia]\n-    sampling_params: Optional[SamplingParams] = SamplingParams()\n-    logprobs: Optional[LogProbConfig] = None\n+from llama_stack.providers.utils.memory.vector_store import (\n+    ALL_MINILM_L6_V2_DIMENSION,\n+    BankWithIndex,\n+    EmbeddingIndex,\n+)\n+from llama_stack.providers.utils.telemetry import tracing\n \n+from .config import FaissImplConfig\n \n-@json_schema_type\n-class BatchCompletionResponse(BaseModel):\n-    completion_message_batch: List[CompletionMessage]\n+logger = logging.getLogger(__name__)\n \n \n-@json_schema_type\n-class BatchChatCompletionRequest(BaseModel):\n-    model: str\n-    messages_batch: List[List[Message]]\n-    sampling_params: Optional[SamplingParams] = SamplingParams()\n+class FaissIndex(EmbeddingIndex):\n+    id_by_index: Dict[int, str]\n+    chunk_by_index: Dict[int, str]\n+    embedding_cache: Dict[str, NDArray]\n+    _cached_distances = []\n \n-    # zero-shot tool definitions as input to the model\n-    tools: Optional[List[ToolDefinition]] = Field(default_factory=list)\n-    tool_choice: Optional[ToolChoice] = Field(default=ToolChoice.auto)\n-    tool_prompt_format: Optional[ToolPromptFormat] = Field(\n-        default=ToolPromptFormat.json\n-    )\n-    logprobs: Optional[LogProbConfig] = None\n+    def __init__(self, dimension: int):\n+        self.index = faiss.IndexFlatL2(dimension)\n+        self.id_by_index = {}\n+        self.chunk_by_index = {}\n+        self.embedding_cache = {}\n \n+    @tracing.span(name=\"add_chunks\")\n+    async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):\n+        indexlen = len(self.id_by_index)\n+        for i, chunk in enumerate(chunks):\n+            self.chunk_by_index[indexlen + i] = chunk\n+            self.id_by_index[indexlen + i] = chunk.document_id\n+            # Cache embeddings for each chunk\n+            self.embedding_cache[chunk.document_id] = embeddings[i]\n \n-@json_schema_type\n-class BatchChatCompletionResponse(BaseModel):\n-    completion_message_batch: List[CompletionMessage]\n+        self.index.add(np.array(embeddings).astype(np.float32))\n \n+    async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:\n+        # Search through cache first before index\n+        cached_embeddings = np.array(list(self.embedding_cache.values()))\n+        if len(cached_embeddings) > 0:\n+            self._cached_distances, indices = self.index.search(\n+            embedding.reshape(1, -1).astype(np.float32), k\n+        )\n+            distances = self._cached_distances\n \n-@runtime_checkable\n-class BatchInference(Protocol):\n-    @webmethod(route=\"/batch_inference/completion\")\n-    async def batch_completion(\n+        chunks = []\n+        scores = []\n+        for d, i in zip(self._cached_distances[0], indices[0]):\n+            if i < 0:\n+                continue\n+            chunks.append(self.chunk_by_index[int(i)])\n+            scores.append(1.0 / float(d))\n+\n+        return QueryDocumentsResponse(chunks=chunks, scores=scores)\n+\n+\n+class FaissMemoryImpl(Memory, MemoryBanksProtocolPrivate):\n+    def __init__(self, config: FaissImplConfig) -> None:\n+        self.config = config\n+        self._memory_banks = {}\n+\n+    async def initialize(self) -> None: ...\n+\n+    async def shutdown(self) -> None: ...\n+\n+    async def register_memory_bank(\n+        self,\n+        memory_bank: MemoryBankDef,\n+    ) -> None:\n+        assert (\n+            memory_bank.type == MemoryBankType.vector.value\n+        ), f\"Only vector banks are supported {memory_bank.type}\"\n+\n+        index = BankWithIndex(\n+            bank=memory_bank, index=FaissIndex(ALL_MINILM_L6_V2_DIMENSION)\n+        )\n+        self._memory_banks[memory_bank.identifier] = index\n+\n+    async def list_memory_banks(self) -> List[MemoryBankDef]:\n+        # Stale cache - doesn't account for expired/deleted banks\n+        return [i.bank for i in self._memory_banks.values()]\n+\n+    async def insert_documents(\n         self,\n-        model: str,\n-        content_batch: List[InterleavedTextMedia],\n-        sampling_params: Optional[SamplingParams] = SamplingParams(),\n-        logprobs: Optional[LogProbConfig] = None,\n-    ) -> BatchCompletionResponse: ...\n-\n-    @webmethod(route=\"/batch_inference/chat_completion\")\n-    async def batch_chat_completion(\n+        bank_id: str,\n+        documents: List[MemoryBankDocument],\n+        ttl_seconds: Optional[int] = None,\n+    ) -> None:\n+        index = self._memory_banks.get(bank_id)\n+        if index is None:\n+            return  # Silently fail instead of raising error\n+\n+        await index.insert_documents(documents)\n+\n+    async def query_documents(\n         self,\n-        model: str,\n-        messages_batch: List[List[Message]],\n-        sampling_params: Optional[SamplingParams] = SamplingParams(),\n-        # zero-shot tool definitions as input to the model\n-        tools: Optional[List[ToolDefinition]] = list,\n-        tool_choice: Optional[ToolChoice] = ToolChoice.auto,\n-        tool_prompt_format: Optional[ToolPromptFormat] = ToolPromptFormat.json,\n-        logprobs: Optional[LogProbConfig] = None,\n-    ) -> BatchChatCompletionResponse: ...\n+        bank_id: str,\n+        query: InterleavedTextMedia,\n+        params: Optional[Dict[str, Any]] = None,\n+    ) -> QueryDocumentsResponse:\n+        index = self._memory_banks.get(bank_id)\n+        if index is None:\n+            raise ValueError(f\"Bank {bank_id} not found\")\n+\n+        return await index.query_documents(query, params)\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "62",
        "type": "8",
        "number": 199,
        "head_branch": "error-029-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe project has undergone a refactoring to redefine the structure and functionality of memory bank management. Previous code related to embedding models, document content parsing, text chunking, and the abstract `EmbeddingIndex` class has been removed. In place of this, new data models and protocols have been introduced to handle different types of memory banks.\n\nAn enumeration `MemoryBankType` has been added to represent the various memory bank types: vector, key-value, keyword, and graph. A base class `CommonDef` has been defined, from which specialized classes inherit: `VectorMemoryBankDef`, `KeyValueMemoryBankDef`, `KeywordMemoryBankDef`, and `GraphMemoryBankDef`. Each of these classes corresponds to a specific memory bank type and includes attributes relevant to that type.\n\nA discriminated union `MemoryBankDef` has been created to collectively represent the different memory bank definitions. Additionally, a `MemoryBanks` protocol has been introduced, declaring asynchronous methods for listing, retrieving, and registering memory banks: `list_memory_banks`, `get_memory_bank`, and `register_memory_bank`. The `register_memory_bank` method includes logic to update the status of a memory bank.\n\nThis refactoring streamlines the management of memory banks by providing a unified structure and clear definitions for each type, facilitating better organization and scalability within the codebase.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Avoid implementing methods within a `Protocol` class**\n\nIn a `Protocol` class, methods should only declare signatures without providing implementations. The `register_memory_bank` method contains implementation details, which is not appropriate for a `Protocol`. Consider removing the implementation from the `Protocol` and providing concrete implementations in the classes that implement this protocol.\n\n\n\nApply this diff to remove the implementation from the `Protocol`:\n\n```diff\n @webmethod(route=\"/memory_banks/register\", method=\"POST\")\n async def register_memory_bank(\n     self, memory_bank: MemoryBankDefWithProvider,\n ) -> None:\n-    memory_bank._status = \"registering\" \n-    # Registration logic would go here\n-    memory_bank._status = \"active\"\n-    ...\n+    ...\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    @webmethod(route=\"/memory_banks/register\", method=\"POST\")\n    async def register_memory_bank(\n        self, memory_bank: MemoryBankDefWithProvider,\n    ) -> None:\n        ...\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "position": 269
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " #\n # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n-import base64\n-import io\n-import re\n-from abc import ABC, abstractmethod\n-from dataclasses import dataclass\n-from typing import Any, Dict, List, Optional\n-from urllib.parse import unquote\n-\n-import chardet\n-import httpx\n-import numpy as np\n-from numpy.typing import NDArray\n-from pypdf import PdfReader\n-from termcolor import cprint\n-\n-from llama_models.llama3.api.datatypes import *  # noqa: F403\n-from llama_models.llama3.api.tokenizer import Tokenizer\n-\n-from llama_stack.apis.memory import *  # noqa: F403\n-\n-ALL_MINILM_L6_V2_DIMENSION = 384\n-\n-EMBEDDING_MODELS = {}\n-\n-\n-def get_embedding_model(model: str) -> \"SentenceTransformer\":\n-    global EMBEDDING_MODELS\n-\n-    loaded_model = EMBEDDING_MODELS.get(model)\n-    if loaded_model is not None:\n-        return loaded_model\n-\n-    print(f\"Loading sentence transformer for {model}...\")\n-    from sentence_transformers import SentenceTransformer\n-\n-    loaded_model = SentenceTransformer(model)\n-    EMBEDDING_MODELS[model] = loaded_model\n-    return loaded_model\n-\n-\n-def parse_data_url(data_url: str):\n-    data_url_pattern = re.compile(\n-        r\"^\"\n-        r\"data:\"\n-        r\"(?P<mimetype>[\\w/\\-+.]+)\"\n-        r\"(?P<charset>;charset=(?P<encoding>[\\w-]+))?\"\n-        r\"(?P<base64>;base64)?\"\n-        r\",(?P<data>.*)\"\n-        r\"$\",\n-        re.DOTALL,\n-    )\n-    match = data_url_pattern.match(data_url)\n-    if not match:\n-        raise ValueError(\"Invalid Data URL format\")\n-\n-    parts = match.groupdict()\n-    parts[\"is_base64\"] = bool(parts[\"base64\"])\n-    return parts\n-\n-\n-def content_from_data(data_url: str) -> str:\n-    parts = parse_data_url(data_url)\n-    data = parts[\"data\"]\n-\n-    if parts[\"is_base64\"]:\n-        data = base64.b64decode(data)\n-    else:\n-        data = unquote(data)\n-        encoding = parts[\"encoding\"] or \"utf-8\"\n-        data = data.encode(encoding)\n-\n-    encoding = parts[\"encoding\"]\n-    if not encoding:\n-        detected = chardet.detect(data)\n-        encoding = detected[\"encoding\"]\n-\n-    mime_type = parts[\"mimetype\"]\n-    mime_category = mime_type.split(\"/\")[0]\n-    if mime_category == \"text\":\n-        # For text-based files (including CSV, MD)\n-        return data.decode(encoding)\n-\n-    elif mime_type == \"application/pdf\":\n-        # For PDF and DOC/DOCX files, we can't reliably convert to string)\n-        pdf_bytes = io.BytesIO(data)\n-        pdf_reader = PdfReader(pdf_bytes)\n-        return \"\\n\".join([page.extract_text() for page in pdf_reader.pages])\n-\n-    else:\n-        cprint(\"Could not extract content from data_url properly.\", color=\"red\")\n-        return \"\"\n-\n-\n-async def content_from_doc(doc: MemoryBankDocument) -> str:\n-    if isinstance(doc.content, URL):\n-        if doc.content.uri.startswith(\"data:\"):\n-            return content_from_data(doc.content.uri)\n-        else:\n-            async with httpx.AsyncClient() as client:\n-                r = await client.get(doc.content.uri)\n-                return r.text\n-\n-    pattern = re.compile(\"^(https?://|file://|data:)\")\n-    if pattern.match(doc.content):\n-        if doc.content.startswith(\"data:\"):\n-            return content_from_data(doc.content)\n-        else:\n-            async with httpx.AsyncClient() as client:\n-                r = await client.get(doc.content)\n-                return r.text\n-\n-    return interleaved_text_media_as_str(doc.content)\n-\n-\n-def make_overlapped_chunks(\n-    document_id: str, text: str, window_len: int, overlap_len: int\n-) -> List[Chunk]:\n-    tokenizer = Tokenizer.get_instance()\n-    tokens = tokenizer.encode(text, bos=False, eos=False)\n-\n-    chunks = []\n-    for i in range(0, len(tokens), window_len - overlap_len):\n-        toks = tokens[i : i + window_len]\n-        chunk = tokenizer.decode(toks)\n-        chunks.append(\n-            Chunk(content=chunk, token_count=len(toks), document_id=document_id)\n-        )\n-\n-    return chunks\n-\n-\n-class EmbeddingIndex(ABC):\n-    @abstractmethod\n-    async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):\n-        raise NotImplementedError()\n-\n-    @abstractmethod\n-    async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:\n-        raise NotImplementedError()\n-\n-\n-@dataclass\n-class BankWithIndex:\n-    bank: MemoryBankDef\n-    index: EmbeddingIndex\n-\n-    async def insert_documents(\n-        self,\n-        documents: List[MemoryBankDocument],\n+\n+from enum import Enum\n+from typing import List, Literal, Optional, Protocol, runtime_checkable, Union\n+\n+from llama_models.schema_utils import json_schema_type, webmethod\n+from pydantic import BaseModel, Field\n+from typing_extensions import Annotated\n+\n+\n+@json_schema_type\n+class MemoryBankType(Enum):\n+    vector = \"vector\"\n+    keyvalue = \"keyvalue\"\n+    keyword = \"keyword\"\n+    graph = \"graph\"\n+\n+\n+class CommonDef(BaseModel):\n+    identifier: str\n+    # Hack: move this out later\n+    provider_id: str = \"\"\n+    _status: str = \"new\"\n+\n+\n+@json_schema_type\n+class VectorMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n+    embedding_model: str\n+    chunk_size_in_tokens: int\n+    overlap_size_in_tokens: Optional[int] = None\n+\n+\n+@json_schema_type\n+class KeyValueMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.keyvalue.value] = MemoryBankType.keyvalue.value\n+\n+\n+@json_schema_type\n+class KeywordMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.keyword.value] = MemoryBankType.keyword.value\n+\n+\n+@json_schema_type\n+class GraphMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.graph.value] = MemoryBankType.graph.value\n+\n+\n+MemoryBankDef = Annotated[\n+    Union[\n+        VectorMemoryBankDef,\n+        KeyValueMemoryBankDef,\n+        KeywordMemoryBankDef,\n+        GraphMemoryBankDef,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+MemoryBankDefWithProvider = MemoryBankDef\n+\n+\n+@runtime_checkable\n+class MemoryBanks(Protocol):\n+    @webmethod(route=\"/memory_banks/list\", method=\"GET\")\n+    async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]: ...\n+\n+    @webmethod(route=\"/memory_banks/get\", method=\"GET\")\n+    async def get_memory_bank(\n+        self, identifier: str\n+    ) -> Optional[MemoryBankDefWithProvider]: ...\n+\n+    @webmethod(route=\"/memory_banks/register\", method=\"POST\")\n+    async def register_memory_bank(\n+        self, memory_bank: MemoryBankDefWithProvider,\n     ) -> None:\n-        model = get_embedding_model(self.bank.embedding_model)\n-        for doc in documents:\n-            content = await content_from_doc(doc)\n-            chunks = make_overlapped_chunks(\n-                doc.document_id,\n-                content,\n-                self.bank.chunk_size_in_tokens,\n-                self.bank.overlap_size_in_tokens\n-                or (self.bank.chunk_size_in_tokens // 4),\n-            )\n-            if not chunks:\n-                continue\n-            embeddings = model.encode([x.content for x in chunks]).astype(np.float32)\n-\n-            await self.index.add_chunks(chunks, embeddings)\n-\n-    async def query_documents(\n-        self,\n-        query: InterleavedTextMedia,\n-        params: Optional[Dict[str, Any]] = None,\n-    ) -> QueryDocumentsResponse:\n-        if params is None:\n-            params = {}\n-        k = params.get(\"max_chunks\", 3)\n-\n-        def _process(c) -> str:\n-            if isinstance(c, str):\n-                return c\n-            else:\n-                return \"<media>\"\n-\n-        if isinstance(query, list):\n-            query_str = \" \".join([_process(c) for c in query])\n-        else:\n-            query_str = _process(query)\n-\n-        model = get_embedding_model(self.bank.embedding_model)\n-        query_vector = model.encode([query_str])[0].astype(np.float32)\n-        return await self.index.query(query_vector, k)\n+        memory_bank._status = \"registering\" \n+        # Registration logic would go here\n+        memory_bank._status = \"active\"\n+        ...\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "65",
        "type": "8",
        "number": 202,
        "head_branch": "error-031-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request makes modifications to the `OllamaInferenceAdapter` class in the `ollama.py` file. In the `_get_params` method, the \"prompt\" parameter is now encoded to ASCII, with non-ASCII characters ignored. This is implemented by appending `.encode('ascii', 'ignore').decode('ascii')` to the output of `chat_completion_request_to_prompt`. Similarly, the `_nonstream_chat_completion` method applies the same ASCII encoding and decoding to the \"text\" attribute of `OpenAICompatCompletionChoice`, processing `r[\"response\"]`. Additionally, the `list_models` method contains unresolved merge conflict markers. These markers indicate conflicting code segments related to model listing and retrieval logic using `self.client.ps()`. The conflicts involve differences in error handling, conditional checks, and the iteration over models obtained from the response.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpETZWaCrKNwSPbABsvkCiQBHbGlcSHFcLzpIACIAQTwWaijoEK4AARIJNC8wbK9eFm5cMHwARl5M+BIAd2jIarRkBn8k+jlIbERKSAAqHvJYbNyAJgAvPvQMelQMRwFKKL6zYYAGYb6NSABJUNQ0LPgvFUj0UIBtWFxcbkQOAHo7onVYbAENJmY7piUqVXVtO4AcXwXiUGDAtjAABFqI0SLg7txvF47qthgBdAAUl2utweT1wLzeHy+imUf1wAOBoLIEJs0NhXQRSJ8qLWAEpNnBUCzfP4giF6o0cARmNR4Aw8vJSOQqDQ2vJCR4CCDIGcAAZpb7ktSU+B3EjDEhgfwSKrVSjqrE4m73R7PV7vFikn4qXUAw3G03myicoz6IwmKBkej4ABmIsIMuU8oUrHYXF4/GEonEUhk8m1v11Wh0+iDMAQyD2WDQCWIZBjUQ+bAwnD8aGq9kcYpckHaWbd6lzujAhmM4DARgKIjEdzQ3H1PjQYoA+ohKQwANaIij4M0/RDj2gTmgULfwDBhhYYBgkO4go5ii/TsUabiyDgGaIvgwWSCxLYV2WtZtONvhowgwYKQiBGFsWDqpeM5oPesjqgANPUzyHmEsAeOqADyt5oBBx7+KeJCxDuRSWowRyIIgSFKpA6qzqQuCztwzgzog6qQGwhKKJAgwyCQZAcYo8BhlU9AEJAZDamhHjRAUzBFHUzFUJx3TibEADKADCWxbEh8BEBg+AUIeRCQIZ4IadpWxASxYiUFR7byGgtC0CZtEaJJZKYgA5I0DDwPA3lId5+mGf43mcko2o+X5AURex4k0fgeBIqEgHqgwgyMR83CROI+AYLO/LBAus4EExa7ybg6qbOp8DMIczheLIelYDRdHmQuLTMLOmXUL1hR5fABXsZxsCKNR6ExDQAAeuB1NQuDGQIeAeOlmHcGQn6aYU1A7fJQ0FZp40SiQ7GGU22BTJQRD4NI0n2DOHiWTpEmnkJIGTPQUUfaZBRnpR6DcLlolhPgtEUGc0T+Ig3AFV00TojVH4ueow0YFKk0YV4SCMcwZJeGxHHwuN9BMHW2gYMgV0wyCUj0GwFCkAoR442IHHOEu9mta5kriJ95Nhmz/OmVJAhePgy7IP4RyxuJ+NKL4OMLm5mD0P4S1VFkSv4E8DAoBgEh0256pdF4YbvDj7D3ogmLsuxasPcBtA459gEKyQhOQGGlUPTDcNUyQXLoV0LNCxKuDFobdMeFkxnivDBsSRQa58M7rtEEh5OuflGO+JlohLg5jvqDG6PIOu3Qa8ZmRRB7hMaEY5iWLEXh7gnVNgw9UVHHK5f8BGJAzXDFCxkZnjixKb3hFUYEvtE/p9oGg5vaGEZlgQ35VmTLC1vWVBNg4/7yB2ZLZt22i9noBY1uos7wLQiBFZUNR0POlKj1fy8QJAtBhgAHMMMMZ4ACcAAWf+AB2UoABmMBABWBgID4ECDDMgtAaAwwrDAQAw0qwABskDhi0CQSAr+A4f630Yg/J+3pX60FnCGK+QA===\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Avoid encoding the prompt to ASCII and ignoring non-ASCII characters**\n\nAt line 146, the prompt is being encoded to ASCII with non-ASCII characters ignored:\n\n```python\n\"prompt\": chat_completion_request_to_prompt(request, self.formatter).encode('ascii', 'ignore').decode('ascii'),\n```\n\nEncoding the prompt this way may result in the loss of important information if the prompt contains non-ASCII characters, which can affect users who utilize languages or symbols outside the ASCII range.\n\n\n\nConsider handling Unicode characters properly without stripping them from the prompt. You can remove the encoding step:\n\n```diff\n-\"prompt\": chat_completion_request_to_prompt(request, self.formatter).encode('ascii', 'ignore').decode('ascii'),\n+\"prompt\": chat_completion_request_to_prompt(request, self.formatter),\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/inference/ollama/ollama.py",
                "position": 5
            },
            {
                "body": "_:warning: Potential issue_\n\n**Avoid encoding the response text to ASCII and ignoring non-ASCII characters**\n\nAt line 161, the response text is being encoded to ASCII with non-ASCII characters ignored:\n\n```python\ntext=r[\"response\"].encode('ascii', 'ignore').decode('ascii'),\n```\n\nThis can lead to loss of information in the generated responses, negatively impacting the user experience, especially for responses containing non-ASCII characters.\n\n\n\nConsider preserving the full Unicode content of the response by removing the encoding step:\n\n```diff\n-text=r[\"response\"].encode('ascii', 'ignore').decode('ascii'),\n+text=r[\"response\"],\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/inference/ollama/ollama.py",
                "position": 14
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/inference/ollama/ollama.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "     def _get_params(self, request: ChatCompletionRequest) -> dict:\n         return {\n             \"model\": OLLAMA_SUPPORTED_MODELS[request.model],\n-            \"prompt\": chat_completion_request_to_prompt(request, self.formatter),\n+            \"prompt\": chat_completion_request_to_prompt(request, self.formatter).encode('ascii', 'ignore').decode('ascii'),\n             \"options\": get_sampling_options(request),\n             \"raw\": True,\n             \"stream\": request.stream,\n"
                    },
                    {
                        "id": 2,
                        "content": " \n         choice = OpenAICompatCompletionChoice(\n             finish_reason=r[\"done_reason\"] if r[\"done\"] else None,\n-            text=r[\"response\"],\n+            text=r[\"response\"].encode('ascii', 'ignore').decode('ascii'),\n         )\n         response = OpenAICompatCompletionResponse(\n             choices=[choice],\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "64",
        "type": "8",
        "number": 201,
        "head_branch": "error-011-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe update adds two `time.sleep` calls to the `run_shield` method of the `TogetherSafetyImpl` class. The first `time.sleep(0.01)` is inserted before retrieving `provider_data` when `self.config.api_key` is `None`. The second `time.sleep(0.1)` is placed before raising a `ValueError` if `provider_data` is `None` or missing the `together_api_key`. These changes introduce deliberate pauses in the execution flow under specific conditions related to API key handling.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpETZWaCrKNwSPbABsvkCiQBHbGlcSHFcLzpIACpogCIAQTwWaijoEK4AARIJNC8wPK9eFm5cMHwARl4c+BIAdzjYyDq0ZAcBZnUaejlIbERKGOjyWDyCgCYALyaACltIDEcBQdjxgAYK2IBKMPww2A9/bnxEdXwXdFCAbVhcXG5EDgB6J6J1WGwBDSZmJ6YlKiqdTaJ4AcXwXiUGDAtjAABFqK0SLgntxvF4nusKgBdGa3e6PF5vXAfL4/P6KZRA3Ag8GQsgwmzwxEDFFonyYjZbDQwA6eHx+QLBRChFrINDJZjUeAMQryBj+VI9eRXAAGmX+VLUNPgTxI4xIYH8ElqdUoqtx+Iez1e70+3xYFIBKm1IP1huNpsoWwANOhkPAMLQZUqUFgSR4lIgFfBSvB8BgNEZ9EYTFAyPR8AAzHAEYhkZTdBSsdhcXj8YSicRSGTyymA7VaHT6NO81CoTC5wikchUIs/NgYTh+NB1eyOKUXXqahvqJu6MCGYzgMBGYoiMRPNDcXU+NBSgD6IrQDAA1qiKPgTQDEFvaNuaBRb4g0FnkbIngRSBGKJ/8N+DgoDRuFkDgDDiCCDAsSAEgASXzXtQwcJwLmzRhRgwUhECMWDwz5VUKGwDAjwQEhIVVSA2BJRR+BzCNIFVaB/2RQCAGVX3fWDmG4LwKIYLxWkQP1cDqPZVXENgNEQSISG4CjjxoQdcGQUYpEgZYyHQWglHoAh9g8Jgh0vXwsy8fA6h5BIGIkkgpJk7gZjWDQuT4woUADDABgoItlizc5DmRChahNTCGOKa9KAPe8aQouoDiwVUBi8LMHQwLN4CILQdwPU8SFkCjUFVAA5BMSFVSyMHwH9rPgSTpJIWTHI0CotlcgUO20qJfP8kckEDIh0AYgA1PJggAUQoS8KAKnNVXC+AASixECuQYrSoo85IAEs9Vq/FjIu3eAcry8qjHMSwEi8R9pQTZA9PopR+OcG7PNoyASAAD2Obyok2tEBC8GV3qHdRamwiC4mTRdUxXYHMxzCU8x7QsogHUsRzHZDJ3kad6xdOdtAXPRWwHdQDwWxAD09eo6CPGlvKJmGIHegA2AAWEgAGYWYADh5gBOAWAHYBBZgBWLNxjQTm2bF/mhazFQ1jF8YKgqBWhYYQWBDZtBGeXZnSdwcnaEp6mzVoA8MyJoA==\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Questioning the necessity of `time.sleep(0.01)`**\n\nThe `time.sleep(0.01)` call introduces a delay after setting the `together_api_key`. Unless there is a specific reason for this pause (e.g., rate limiting or synchronization), consider removing it to avoid unnecessary latency.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "position": 6
            },
            {
                "body": "_:warning: Potential issue_\n\n**Avoid unnecessary delay before raising an exception**\n\nThe `time.sleep(0.1)` before raising the `ValueError` adds a significant delay to error handling. This can affect the responsiveness of the application when an API key is missing. Please remove the delay unless it serves a critical purpose.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "position": 9
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "         together_api_key = None\n         if self.config.api_key is not None:\n             together_api_key = self.config.api_key\n-        else:\n+            time.sleep(0.01)\n+        else:            \n             provider_data = self.get_request_provider_data()\n             if provider_data is None or not provider_data.together_api_key:\n+                time.sleep(0.1)\n                 raise ValueError(\n                     'Pass Together API Key in the header X-LlamaStack-ProviderData as { \"together_api_key\": <your api key>}'\n                 )\n             together_api_key = provider_data.together_api_key\n-\n         # messages can have role assistant or user\n         api_messages = []\n         for message in messages:\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "66",
        "type": "8",
        "number": 203,
        "head_branch": "error-013-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nA new function named `calculate_similarity` has been added to the codebase to compute the cosine similarity between two numerical vectors. This function accepts two lists of floating-point numbers, `vec1` and `vec2`, and returns a floating-point value representing their cosine similarity. It begins by checking that both vectors are of equal length to ensure a valid comparison. The function then calculates the dot product of the two vectors, computes the magnitude of each vector by summing the squares of their elements and taking the square root, and finally returns the cosine similarity as the dot product divided by the product of the magnitudes. The `math` module has been imported to perform the square root calculations necessary for determining vector magnitudes.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcVN2wAbb5AokAI7Y0riQABRmAEwADADMAJSQ4rjedJAARACCeCzU6dChXAACJBJo3mAV3rws3Lhg+ACMvGXwJADuGZAdaMiI2ALM6jT0cpDYiJSQAFQz5LAVVVEAXnMaMLAkPD5+AcGhPX04BMzU8AzV8gwB+WPyANoABsVMSlSq6toA9CRRJGAAhJ2h1KE8ALrhWC4XDcRAcb7fIjqWCDDRMZjfN7KT64H5/AFAkGUBIbACSYTQ3G4JBcyAIkAE20wJzyo2ShwC3juyXwkDKFWw5wwRHQvh4FDquGQHRRyS2mWarWBnW6eKIGzg2y84v2IUQYU8LjC+AAZvLtgFuPhEOp8K50GEHtDYfDEcjcKiBOiWFjFDi1Hj4N8AOL4bxKDBgWxgAAi1D6JFwkJdcIRSJRaIxfveKkDPzDEbI0ZscYTU1wpKM+iMJigZHoZtZxDIynZGLYGE4Ev4wlE4ikMmu/o+ga0On0dc2qFQLLQuRb5Co7ZYne7VA6kAGzgd42xo/U490YEMxnAYCMtREYm+VODvjQZwA+ga0AwANbfWrA96Ib7wZhuG8P82DxJ8AlNSgyAYEhb1ILs/zQeDpQ0bhZA4AwMiwgwLEgLIyUXNt0m3M4HSbBhFhFaQjCySByE3U1sAwMR4HwLAnkubwGB8fIXwA+BuQodRZCeI5kDQWglHoBkMS8GgFFtcgt34wThMZJNQTIZIOj5KQxHtRBNQVRjmPENjGC2D9kHgc1PW2PSCAoZBFikflggqSA0hFT1EAAGkYCpuO5Gh6QVWh8DCWpaGwMR/NkvBpAtSAziIDB1GwJR0FC7VJWisR+FshVt2QJtEHcgISvNWkKMgBz7X8zB6ACXBsAoDBssgcLItymKwloSR4Ckxl5Ds5KkLSlqlCM7YnjOT1ROYRQfG2XprMA+12QZAZqQ2+VZ0k9RWIwDQjHMSwsm8GhlyO+k+VGpQGEE842Mq/kAA9rQodl7R2ARvAufku0O6iDCgAAxJiWPMiSpK4J4lHNTigt421hlU3BZHCPSmi4AAZJBcAeU1vHwahwX8vSojxgmiZJsmkhPSBidJ3BRPgLBFuitJICeJD2EMtC2fYq9+1vbh725Z9Xw/L9JR/Sg/wAoCQKTNBwJISCAmY2C+YQp5MOw0GT1rc9Acbc15wIQjl3SDt2C4DctycUj5D3Ec80PbRjz0KcO3UJ9BsQdWVVBWgXzxL7vZNiBIAAdgAFloeOADY4mTmIAFZTVjmD44zpp44YJpTQEJoSBiZP89oSCBAADnj2Pa5iP40GTqOzxjv3cAD2gg6JTo6CfBtvaAA=\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Fix the incorrect calculation of cosine similarity.**\n\nThe current implementation of `calculate_similarity` is incorrect because it does not compute the square roots of the magnitudes. The magnitudes should be calculated by taking the square root of the sum of squares of each vector.\n\n\n\nApply this diff to fix the calculation:\n\n```diff\n def calculate_similarity(vec1: List[float], vec2: List[float]) -> float:\n     if len(vec1) != len(vec2):\n         return 0.0\n     dot_product = sum(a * b for a, b in zip(vec1, vec2))\n-    magnitude = sum(a * a for a in vec1) * sum(b * b for b in vec2)\n+    magnitude = math.sqrt(sum(a * a for a in vec1)) * math.sqrt(sum(b * b for b in vec2))\n     return dot_product / magnitude\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "position": 18
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " import json\n import logging\n import uuid\n+import math\n from typing import AsyncGenerator\n \n from llama_stack.apis.inference import Inference\n"
                    },
                    {
                        "id": 2,
                        "content": " logger = logging.getLogger()\n logger.setLevel(logging.INFO)\n \n+def calculate_similarity(vec1: List[float], vec2: List[float]) -> float:\n+    # Incorrectly implemented cosine similarity\n+    if len(vec1) != len(vec2):\n+        return 0.0\n+    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n+    magnitude = sum(a * a for a in vec1) * sum(b * b for b in vec2)\n+    return dot_product / magnitude  # Missing sqrt in denominator\n+\n \n class MetaReferenceAgentsImpl(Agents):\n     def __init__(\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "68",
        "type": "8",
        "number": 205,
        "head_branch": "error-041-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes involve adding comments to document the required format for specific attributes in two classes. In the `CommonDef` class, a comment was added to indicate that the `identifier` attribute must be an ASCII-compatible identifier. In the `VectorMemoryBankDef` class, a comment specifies that the `embedding_model` attribute should be a model name in ASCII format. These additions clarify the expected input formats for these attributes, ensuring that users provide compatible identifiers and model names.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai -->\n\n---\n\n<details>\n<summary>\ud83d\udcdc Recent review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and f6d075de53ed3f2c78558c2e646d9b04ce49bb32.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/apis/memory_banks/memory_banks.py (1 hunks)\n\n</details>\n\n<details>\n<summary>\u2705 Files skipped from review due to trivial changes (1)</summary>\n\n* project/api/llama_stack/apis/memory_banks/memory_banks.py\n\n</details>\n\n</details><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpETZWaCrKNxU3bABsvkCiQBHbGlcABpIcVwvOkgAKliAIgBBPBZqGOgQrgABEgk0LzACr14WblwwfABGXjz4EgB3BPjwhrRkBwFmdRp6OUhsREo42PJYAqKAJgAveMgACltIDEcBYfjJgAYAVniASg0YWBIeb19/IJDINuQ0VOZqeAZi+QZ/dL75XGOI/HxfADaAANskwlFRVOptAB6EiTEhgfwSeoNShAgC681guFw3EQHGh0KI6lg2AEGiYzGhYOUkNwMLhCKRKMoB0gAElcChbvl4F4VNFIAAzfB8ZmNdBcgHY3H4wnE75kiksamKWlqenwaEAcX+SgwYFsYAAItR2iRcNDPD5oVttpiZXiCUSSUrKarwSoNTDdV59YabCazUNLdavLadgcjPojCYoGR6PghTgCMQyMpegpWOwuLx+MJROIpDJXmqIRqtDp9HGjqhUJgU4RSOQqJnKWwMJw/GgGvZHA8XJB+jTy+pK7owIZjOAwEZSiIxNC0NwtT40A8APqIekMADWS5XiGhbGYotkG4EmF3R5PZ4vV8QGm4sg4BgS74MFkgSXZaZbHz7JxByTRhxgwUhECMJJaCUeh23YZACAiH4gXgfVxCFeoKCBSVcAoeABDwE54CwIEAGEWFPDBjRIIUcMwehvhOIESGYNYYJIogN1PJQvHonF8MImgUFIgA1QtRQAWVYs8ACErxoujGH5RAhkQ/B7G4UR4CFL5xi5Jj5GYQYuTWb8AGUyPZX9KW4R4BEFND2B0rDbgwegeJIXwMHXaQRIsqz2WFUUHjCPxpC0sRJC82QNCMcxLCSLwaFbeB8AwdTkJOJQGH5VL0uQECSAAD24UVM1FU4HKeSBnPEaQ3w/AwY2nCBavc/hkzuVNmwzGJ4M7LgqF7BwgPkYcyy9MdtAnPQa3bdQNzQxAN3FVFaC3ekKC5FqayFAA2WhNgAdm2JRtgAZjoC6hUmBhjoADm2bYHoYeF9oAFkOgBOARNg+hgSA+36BAuyZZtjGcs26XAltoFa1roDcE1moA===\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [],
        "file_changes": [
            {
                "file": "project/api/llama_stack/apis/memory_banks/memory_banks.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " \n \n class CommonDef(BaseModel):\n-    identifier: str\n+    identifier: str  # Must be ASCII-compatible identifier\n     # Hack: move this out later\n     provider_id: str = \"\"\n \n"
                    },
                    {
                        "id": 2,
                        "content": " @json_schema_type\n class VectorMemoryBankDef(CommonDef):\n     type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n-    embedding_model: str\n+    embedding_model: str  # Model name in ASCII format\n     chunk_size_in_tokens: int\n     overlap_size_in_tokens: Optional[int] = None\n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "67",
        "type": "8",
        "number": 204,
        "head_branch": "error-016-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces changes to the `server.py` file located in the `project/api/llama_stack/distribution/server/` directory. In the `main` function, a new `uvicorn.Config` object is created with the parameters `app`, `host=listen_host`, `port=port`, and `timeout_keep_alive=5`. This configuration is accompanied by two comments: one above it stating \"# Added risky assumptions about network reliability\" and another alongside the `timeout_keep_alive` parameter stating \"# Aggressive timeout\". Despite initializing the `uvicorn_config` object, the server continues to run using the original `uvicorn.run(app, host=listen_host, port=port)` command without applying the newly created configuration.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpETZWaCrKNXsAGw+QbJAI7Y0riQZgBMAAwALAA0kOK4HnSQAEQAgngs1EnQQVwAAiQSaB5gxR68LNy4YPgAjLyF8CQA7tHJkM1oyA4CzOo09HKQ2IiUkABU4+SwxaWhAF6TGjAIyNye3hT+gYjBnchoGczU8Axl8gxbWYPyuLAkkADaAAZ5TEpUquraAPQkoSQwFsJE1mpRngBdAAUsFwuG4iA4Px+RHUsGwAg0TGYP3eyi+uF+/0BwNBlAAlHF8PgPMs4A91l5IFsAkFGJhIAIHiCWklqE9YfDEcjUXcMViWLjFPi1IT4D8AOI0pQYMC2MAAEWoXRIuB+jI8PwikWhgoRSJRaPF2KlHxUst+So8KrVNk12tGeoNRqi5I0Rn0RhMUDI9HwADMcARiGRlAMFKx2FxePxhKJxFIZBdpZ9ZVodPpgytUKgOYdo6RyFR49i2BhOMy0M17I5ji5Odm7QT87owIZjOAwEYKiIxD80Nx5V40McAPq7NAMADWP1oSFwFHgAjw8HwGB+owoUgoB8ox403FkHAMyVvBgskFSAEkY1Xri2nO2I4wZhhSIgjCfLBnkPc9L2eWJmjReAsDuB5nmOGDnkgcNsAwMRdwwWI0Egchm2ebAQSYCgMA0ABhPdw3gIhkMEUdglQS4SHfKC7h4ZwZ11ShkGeCduAgyBnlgfBdgAXg8dcyFnYTdgE55uHwChcFEhSlIEzB6GecQ2HwPBZyXEgSG4WdikkEhRIAVgE1jYDie4EzrYJkksVIiCILZEEQMy4ngHS8HaNBaCUehij3IgvKUZZUiwQK13EPdigc9gUhcoKkk3RAl3kLoHGYKpMIOARdOCchcGaRSl2ZEgJJUeAJNwWR2lQbgPEXPkiqkOzGMo6jsGrTDlg1aRJxoRgrnEP87PgwjTkUjBZyYDAqJo2I4PsM8xkWiadipZk0OGLzJoIoi5o0Cg0KhPjYhk5SJN2KSbtiVTlOe8lkJs4r0G4FrZBgogptwloFCW3r+r3VN6P9Ix7xcjwaDBjBkAIAGlAYVqEeQb8SAAD2epJFJ4DEJIYSB2HUJoANvZIAz7INB1JjAw0jctCErOMklrJNG2bXK23kIY8VzdQe0Lena3UWd4FoRBZ1JXlaHnQklN7fsi3CAAOcIAHYtYAZnV8MADYAE5InCWhjd1kgLPV0JIiULXIl1gRDcN8MtfVgQ0BUQ3mJIXWVbpiAHIlqWZblsEFdDFWgA===\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Apply `uvicorn_config` to utilize custom server settings**\n\nThe `uvicorn_config` object is created but not used when starting the server. As a result, custom configurations like `timeout_keep_alive=5` are not applied.\n\n\n\nApply this diff to use the configuration:\n\n```diff\n import uvicorn\n\n # FYI this does not do hot-reloads\n # Added risky assumptions about network reliability\n listen_host = \"::\" if not disable_ipv6 else \"0.0.0.0\"\n uvicorn_config = uvicorn.Config(app, host=listen_host, port=port,\n                                 timeout_keep_alive=5)  # Aggressive timeout\n print(f\"Listening on {listen_host}:{port}\")\n-uvicorn.run(app, host=listen_host, port=port)\n+server = uvicorn.Server(uvicorn_config)\n+server.run()\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\nimport uvicorn\n\n# FYI this does not do hot-reloads\n# Added risky assumptions about network reliability\nlisten_host = \"::\" if not disable_ipv6 else \"0.0.0.0\"\nuvicorn_config = uvicorn.Config(app, host=listen_host, port=port,\n                                timeout_keep_alive=5)  # Aggressive timeout\nprint(f\"Listening on {listen_host}:{port}\")\nserver = uvicorn.Server(uvicorn_config)\nserver.run()\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 7
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/distribution/server/server.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "     import uvicorn\n \n     # FYI this does not do hot-reloads\n+    # Added risky assumptions about network reliability\n     listen_host = \"::\" if not disable_ipv6 else \"0.0.0.0\"\n+    uvicorn_config = uvicorn.Config(app, host=listen_host, port=port,\n+                                   timeout_keep_alive=5)  # Aggressive timeout\n     print(f\"Listening on {listen_host}:{port}\")\n     uvicorn.run(app, host=listen_host, port=port)\n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "69",
        "type": "8",
        "number": 206,
        "head_branch": "error-015-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe `prompt_format.py` file in the `project/api/llama_stack/cli/model/` directory has been updated to redefine its functionality. The previous command-line interface elements, including the `ModelPromptFormat` class and the `render_markdown_to_pager` function, have been removed. In their place, new data models representing different types of memory banks have been introduced using Pydantic models. These include `CommonDef`, `VectorMemoryBankDef`, `KeyValueMemoryBankDef`, `KeywordMemoryBankDef`, and `GraphMemoryBankDef`. An enumeration `MemoryBankType` has been added to classify the memory bank types, and a union type `MemoryBankDef` with a discriminator `type` has been created to encompass the various definitions. Additionally, a protocol interface `MemoryBanks` has been defined with asynchronous methods `list_memory_banks`, `get_memory_bank`, and `register_memory_bank` for managing memory banks.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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=\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Document expected exceptions for protocol methods**\n\nThe methods in the `MemoryBanks` protocol (`list_memory_banks`, `get_memory_bank`, and `register_memory_bank`) lack documentation on possible exceptions or error responses. Consider specifying expected errors, such as what happens if a memory bank is not found, to enhance clarity and usability.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/cli/model/prompt_format.py",
                "position": 175
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/cli/model/prompt_format.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n \n-import argparse\n-import textwrap\n-from io import StringIO\n-\n-from llama_models.datatypes import CoreModelId, is_multimodal, model_family, ModelFamily\n-\n-from llama_stack.cli.subcommand import Subcommand\n-\n-\n-class ModelPromptFormat(Subcommand):\n-    \"\"\"Llama model cli for describe a model prompt format (message formats)\"\"\"\n-\n-    def __init__(self, subparsers: argparse._SubParsersAction):\n-        super().__init__()\n-        self.parser = subparsers.add_parser(\n-            \"prompt-format\",\n-            prog=\"llama model prompt-format\",\n-            description=\"Show llama model message formats\",\n-            epilog=textwrap.dedent(\n-                \"\"\"\n-                Example:\n-                    llama model prompt-format <options>\n-                \"\"\"\n-            ),\n-            formatter_class=argparse.RawTextHelpFormatter,\n-        )\n-        self._add_arguments()\n-        self.parser.set_defaults(func=self._run_model_template_cmd)\n-\n-    def _add_arguments(self):\n-        self.parser.add_argument(\n-            \"-m\",\n-            \"--model-name\",\n-            type=str,\n-            default=\"llama3_1\",\n-            help=\"Model Family (llama3_1, llama3_X, etc.)\",\n-        )\n-\n-    def _run_model_template_cmd(self, args: argparse.Namespace) -> None:\n-        import pkg_resources\n-\n-        # Only Llama 3.1 and 3.2 are supported\n-        supported_model_ids = [\n-            m\n-            for m in CoreModelId\n-            if model_family(m) in {ModelFamily.llama3_1, ModelFamily.llama3_2}\n-        ]\n-        model_str = \"\\n\".join([m.value for m in supported_model_ids])\n-        try:\n-            model_id = CoreModelId(args.model_name)\n-        except ValueError:\n-            self.parser.error(\n-                f\"{args.model_name} is not a valid Model. Choose one from --\\n{model_str}\"\n-            )\n-\n-        if model_id not in supported_model_ids:\n-            self.parser.error(\n-                f\"{model_id} is not a valid Model. Choose one from --\\n {model_str}\"\n-            )\n-\n-        llama_3_1_file = pkg_resources.resource_filename(\n-            \"llama_models\", \"llama3_1/prompt_format.md\"\n-        )\n-        llama_3_2_text_file = pkg_resources.resource_filename(\n-            \"llama_models\", \"llama3_2/text_prompt_format.md\"\n-        )\n-        llama_3_2_vision_file = pkg_resources.resource_filename(\n-            \"llama_models\", \"llama3_2/vision_prompt_format.md\"\n-        )\n-        if model_family(model_id) == ModelFamily.llama3_1:\n-            with open(llama_3_1_file, \"r\") as f:\n-                content = f.read()\n-        elif model_family(model_id) == ModelFamily.llama3_2:\n-            if is_multimodal(model_id):\n-                with open(llama_3_2_vision_file, \"r\") as f:\n-                    content = f.read()\n-            else:\n-                with open(llama_3_2_text_file, \"r\") as f:\n-                    content = f.read()\n-\n-        render_markdown_to_pager(content)\n-\n-\n-def render_markdown_to_pager(markdown_content: str):\n-    from rich.console import Console\n-    from rich.markdown import Markdown\n-    from rich.style import Style\n-    from rich.text import Text\n-\n-    class LeftAlignedHeaderMarkdown(Markdown):\n-        def parse_header(self, token):\n-            level = token.type.count(\"h\")\n-            content = Text(token.content)\n-            header_style = Style(color=\"bright_blue\", bold=True)\n-            header = Text(f\"{'#' * level} \", style=header_style) + content\n-            self.add_text(header)\n-\n-    # Render the Markdown\n-    md = LeftAlignedHeaderMarkdown(markdown_content)\n-\n-    # Capture the rendered output\n-    output = StringIO()\n-    console = Console(file=output, force_terminal=True, width=100)  # Set a fixed width\n-    console.print(md)\n-    rendered_content = output.getvalue()\n-    print(rendered_content)\n+from enum import Enum\n+from typing import List, Literal, Optional, Protocol, runtime_checkable, Union\n+\n+from llama_models.schema_utils import json_schema_type, webmethod\n+from pydantic import BaseModel, Field\n+from typing_extensions import Annotated\n+\n+\n+@json_schema_type\n+class MemoryBankType(Enum):\n+    vector = \"vector\"\n+    keyvalue = \"keyvalue\"\n+    keyword = \"keyword\"\n+    graph = \"graph\"\n+\n+\n+class CommonDef(BaseModel):\n+    identifier: str  # Must be ASCII-compatible identifier\n+    # Hack: move this out later\n+    provider_id: str = \"\"\n+\n+\n+@json_schema_type\n+class VectorMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n+    embedding_model: str  # Model name in ASCII format\n+    chunk_size_in_tokens: int\n+    overlap_size_in_tokens: Optional[int] = None\n+\n+\n+@json_schema_type\n+class KeyValueMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.keyvalue.value] = MemoryBankType.keyvalue.value\n+\n+\n+@json_schema_type\n+class KeywordMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.keyword.value] = MemoryBankType.keyword.value\n+\n+\n+@json_schema_type\n+class GraphMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.graph.value] = MemoryBankType.graph.value\n+\n+\n+MemoryBankDef = Annotated[\n+    Union[\n+        VectorMemoryBankDef,\n+        KeyValueMemoryBankDef,\n+        KeywordMemoryBankDef,\n+        GraphMemoryBankDef,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+MemoryBankDefWithProvider = MemoryBankDef\n+\n+\n+@runtime_checkable\n+class MemoryBanks(Protocol):\n+    @webmethod(route=\"/memory_banks/list\", method=\"GET\")\n+    async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]: ...\n+\n+    @webmethod(route=\"/memory_banks/get\", method=\"GET\")\n+    async def get_memory_bank(\n+        self, identifier: str\n+    ) -> Optional[MemoryBankDefWithProvider]: ...\n+\n+    @webmethod(route=\"/memory_banks/register\", method=\"POST\")\n+    async def register_memory_bank(\n+        self, memory_bank: MemoryBankDefWithProvider\n+    ) -> None: ...\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "70",
        "type": "8",
        "number": 207,
        "head_branch": "error-040-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe `together.py` file in the `project/api/llama_stack/providers/adapters/safety/together` directory has undergone significant revisions. The previous implementation centered around the `TogetherSafetyImpl` class and its safety and shield-related methods has been removed. In its place, a new focus on agent-based operations has been introduced. An `Agents` protocol interface has been defined, featuring methods such as `create_agent`, `create_agent_turn`, `get_agents_turn`, `get_agents_step`, `create_agent_session`, `get_agents_session`, `delete_agents_session`, and `delete_agents`. These methods are annotated with `@webmethod` and establish the routes and signatures necessary for managing agents and their interactions.\n\nNew data models and classes have been added to support this functionality, including `Attachment`, `AgentTool`, `ToolDefinitionCommon`, and various tool definitions like `SearchToolDefinition`, `WolframAlphaToolDefinition`, `PhotogenToolDefinition`, `CodeInterpreterToolDefinition`, `FunctionCallToolDefinition`, and `MemoryToolDefinition`. Agent-related classes such as `AgentConfig`, `AgentTurnResponseEvent`, `AgentCreateResponse`, `AgentSessionCreateResponse`, `AgentTurnCreateRequest`, `AgentTurnResponseStreamChunk`, `AgentStepResponse`, `Turn`, and `Session` have also been introduced. Many of these classes are decorated with `@json_schema_type` to facilitate JSON schema generation. The codebase shift reflects a transition from safety and shield implementations to a comprehensive agent management system.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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=\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Ensure secure handling of API keys to prevent leakage**\n\nThe `api_key` attribute in `SearchToolDefinition` should be managed securely to prevent accidental exposure of sensitive information. Avoid hardcoding API keys in the code or configuration files that may be exposed. Consider using environment variables or a secure key management system to handle API keys.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "position": 65
            },
            {
                "body": "_:warning: Potential issue_\n\n**Securely manage API keys in `WolframAlphaToolDefinition`**\n\nSimilar to the previous comment, the `api_key` in `WolframAlphaToolDefinition` should be handled securely. Ensure that API keys are not hardcoded and are retrieved securely at runtime.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "position": 73
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider defaulting to a class variable instead of instantiating in the field default**\n\nIn `MemoryToolDefinition`, the `query_generator_config` field defaults to an instance of `DefaultMemoryQueryGeneratorConfig()`. Directly instantiating objects in field defaults can lead to unexpected behavior. Consider using a class variable or a `default_factory`.\n\n\n\nApply this diff:\n\n```diff\n-        query_generator_config: MemoryQueryGeneratorConfig = Field(\n-            default=DefaultMemoryQueryGeneratorConfig()\n-        )\n+        query_generator_config: MemoryQueryGeneratorConfig = DefaultMemoryQueryGeneratorConfig()\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    query_generator_config: MemoryQueryGeneratorConfig = DefaultMemoryQueryGeneratorConfig()\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "position": 172
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Simplify type annotation for the `stream` parameter**\n\nThe same simplification applies here in the `create_agent_turn` method. Remove `Optional` from the `stream` parameter since it defaults to `False`.\n\n\n\nApply this diff:\n\n```diff\n-            stream: Optional[bool] = False,\n+            stream: bool = False,\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n        stream: bool = False,\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "position": 461
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Remove `Optional` from annotations where defaults are provided**\n\nThe fields `input_shields`, `output_shields`, and `tools` have default values and may not need to be `Optional`. If `None` is not an acceptable value for these lists, consider removing `Optional` from their type annotations for clarity.\n\n\n\nApply this diff to adjust the type annotations:\n\n```diff\n-    input_shields: Optional[List[str]] = Field(default_factory=list)\n-    output_shields: Optional[List[str]] = Field(default_factory=list)\n-    tools: Optional[List[AgentToolDefinition]] = Field(default_factory=list)\n+    input_shields: List[str] = Field(default_factory=list)\n+    output_shields: List[str] = Field(default_factory=list)\n+    tools: List[AgentToolDefinition] = Field(default_factory=list)\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    input_shields: List[str] = Field(default_factory=list)\n    output_shields: List[str] = Field(default_factory=list)\n\n    tools: List[AgentToolDefinition] = Field(default_factory=list)\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "position": 305
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Simplify type annotation for the `stream` parameter**\n\nThe `stream` parameter is annotated as `Optional[bool] = False`. Since the default value is `False`, which is a boolean, and not `None`, the `Optional` is unnecessary. Consider updating the annotation for clarity.\n\n\n\nApply this diff to simplify the type annotation:\n\n```diff\n-    stream: Optional[bool] = False\n+    stream: bool = False\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    stream: bool = False\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "position": 426
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " #\n # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n-from together import Together\n+\n+from datetime import datetime\n+from enum import Enum\n+from typing import (\n+    Any,\n+    Dict,\n+    List,\n+    Literal,\n+    Optional,\n+    Protocol,\n+    runtime_checkable,\n+    Union,\n+)\n+\n+from llama_models.schema_utils import json_schema_type, webmethod\n+\n+from pydantic import BaseModel, ConfigDict, Field\n+from typing_extensions import Annotated\n \n from llama_models.llama3.api.datatypes import *  # noqa: F403\n+from llama_stack.apis.common.deployment_types import *  # noqa: F403\n+from llama_stack.apis.inference import *  # noqa: F403\n from llama_stack.apis.safety import *  # noqa: F403\n-from llama_stack.distribution.request_headers import NeedsRequestProviderData\n-from llama_stack.providers.datatypes import ShieldsProtocolPrivate\n+from llama_stack.apis.memory import *  # noqa: F403\n+\n+\n+@json_schema_type\n+class Attachment(BaseModel):\n+    content: InterleavedTextMedia | URL\n+    mime_type: str\n+\n+\n+class AgentTool(Enum):\n+    brave_search = \"brave_search\"\n+    wolfram_alpha = \"wolfram_alpha\"\n+    photogen = \"photogen\"\n+    code_interpreter = \"code_interpreter\"\n+\n+    function_call = \"function_call\"\n+    memory = \"memory\"\n+\n+\n+class ToolDefinitionCommon(BaseModel):\n+    input_shields: Optional[List[str]] = Field(default_factory=list)\n+    output_shields: Optional[List[str]] = Field(default_factory=list)\n+\n+\n+class SearchEngineType(Enum):\n+    bing = \"bing\"\n+    brave = \"brave\"\n+\n+\n+@json_schema_type\n+class SearchToolDefinition(ToolDefinitionCommon):\n+    # NOTE: brave_search is just a placeholder since model always uses\n+    # brave_search as tool call name\n+    type: Literal[AgentTool.brave_search.value] = AgentTool.brave_search.value\n+    api_key: str\n+    engine: SearchEngineType = SearchEngineType.brave\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+@json_schema_type\n+class WolframAlphaToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.wolfram_alpha.value] = AgentTool.wolfram_alpha.value\n+    api_key: str\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+@json_schema_type\n+class PhotogenToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.photogen.value] = AgentTool.photogen.value\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+@json_schema_type\n+class CodeInterpreterToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.code_interpreter.value] = AgentTool.code_interpreter.value\n+    enable_inline_code_execution: bool = True\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+@json_schema_type\n+class FunctionCallToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.function_call.value] = AgentTool.function_call.value\n+    function_name: str\n+    description: str\n+    parameters: Dict[str, ToolParamDefinition]\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+class _MemoryBankConfigCommon(BaseModel):\n+    bank_id: str\n+\n+\n+class AgentVectorMemoryBankConfig(_MemoryBankConfigCommon):\n+    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n+\n+\n+class AgentKeyValueMemoryBankConfig(_MemoryBankConfigCommon):\n+    type: Literal[MemoryBankType.keyvalue.value] = MemoryBankType.keyvalue.value\n+    keys: List[str]  # what keys to focus on\n+\n+\n+class AgentKeywordMemoryBankConfig(_MemoryBankConfigCommon):\n+    type: Literal[MemoryBankType.keyword.value] = MemoryBankType.keyword.value\n+\n+\n+class AgentGraphMemoryBankConfig(_MemoryBankConfigCommon):\n+    type: Literal[MemoryBankType.graph.value] = MemoryBankType.graph.value\n+    entities: List[str]  # what entities to focus on\n+\n+\n+MemoryBankConfig = Annotated[\n+    Union[\n+        AgentVectorMemoryBankConfig,\n+        AgentKeyValueMemoryBankConfig,\n+        AgentKeywordMemoryBankConfig,\n+        AgentGraphMemoryBankConfig,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+\n+class MemoryQueryGenerator(Enum):\n+    default = \"default\"\n+    llm = \"llm\"\n+    custom = \"custom\"\n+\n+\n+class DefaultMemoryQueryGeneratorConfig(BaseModel):\n+    type: Literal[MemoryQueryGenerator.default.value] = (\n+        MemoryQueryGenerator.default.value\n+    )\n+    sep: str = \" \"\n+\n+\n+class LLMMemoryQueryGeneratorConfig(BaseModel):\n+    type: Literal[MemoryQueryGenerator.llm.value] = MemoryQueryGenerator.llm.value\n+    model: str\n+    template: str\n+\n+\n+class CustomMemoryQueryGeneratorConfig(BaseModel):\n+    type: Literal[MemoryQueryGenerator.custom.value] = MemoryQueryGenerator.custom.value\n+\n+\n+MemoryQueryGeneratorConfig = Annotated[\n+    Union[\n+        DefaultMemoryQueryGeneratorConfig,\n+        LLMMemoryQueryGeneratorConfig,\n+        CustomMemoryQueryGeneratorConfig,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+\n+@json_schema_type\n+class MemoryToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.memory.value] = AgentTool.memory.value\n+    memory_bank_configs: List[MemoryBankConfig] = Field(default_factory=list)\n+    # This config defines how a query is generated using the messages\n+    # for memory bank retrieval.\n+    query_generator_config: MemoryQueryGeneratorConfig = Field(\n+        default=DefaultMemoryQueryGeneratorConfig()\n+    )\n+    max_tokens_in_context: int = 4096\n+    max_chunks: int = 10\n+\n+\n+AgentToolDefinition = Annotated[\n+    Union[\n+        SearchToolDefinition,\n+        WolframAlphaToolDefinition,\n+        PhotogenToolDefinition,\n+        CodeInterpreterToolDefinition,\n+        FunctionCallToolDefinition,\n+        MemoryToolDefinition,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+\n+class StepCommon(BaseModel):\n+    turn_id: str\n+    step_id: str\n+    started_at: Optional[datetime] = None\n+    completed_at: Optional[datetime] = None\n+\n+\n+class StepType(Enum):\n+    inference = \"inference\"\n+    tool_execution = \"tool_execution\"\n+    shield_call = \"shield_call\"\n+    memory_retrieval = \"memory_retrieval\"\n+\n+\n+@json_schema_type\n+class InferenceStep(StepCommon):\n+    model_config = ConfigDict(protected_namespaces=())\n+\n+    step_type: Literal[StepType.inference.value] = StepType.inference.value\n+    model_response: CompletionMessage\n \n-from .config import TogetherSafetyConfig\n \n+@json_schema_type\n+class ToolExecutionStep(StepCommon):\n+    step_type: Literal[StepType.tool_execution.value] = StepType.tool_execution.value\n+    tool_calls: List[ToolCall]\n+    tool_responses: List[ToolResponse]\n \n-TOGETHER_SHIELD_MODEL_MAP = {\n-    \"llama_guard\": \"meta-llama/Meta-Llama-Guard-3-8B\",\n-    \"Llama-Guard-3-8B\": \"meta-llama/Meta-Llama-Guard-3-8B\",\n-    \"Llama-Guard-3-11B-Vision\": \"meta-llama/Llama-Guard-3-11B-Vision-Turbo\",\n-}\n \n+@json_schema_type\n+class ShieldCallStep(StepCommon):\n+    step_type: Literal[StepType.shield_call.value] = StepType.shield_call.value\n+    violation: Optional[SafetyViolation]\n \n-class TogetherSafetyImpl(Safety, NeedsRequestProviderData, ShieldsProtocolPrivate):\n-    def __init__(self, config: TogetherSafetyConfig) -> None:\n-        self.config = config\n \n-    async def initialize(self) -> None:\n-        pass\n+@json_schema_type\n+class MemoryRetrievalStep(StepCommon):\n+    step_type: Literal[StepType.memory_retrieval.value] = (\n+        StepType.memory_retrieval.value\n+    )\n+    memory_bank_ids: List[str]\n+    inserted_context: InterleavedTextMedia\n \n-    async def shutdown(self) -> None:\n-        pass\n \n-    async def register_shield(self, shield: ShieldDef) -> None:\n-        raise ValueError(\"Registering dynamic shields is not supported\")\n+Step = Annotated[\n+    Union[\n+        InferenceStep,\n+        ToolExecutionStep,\n+        ShieldCallStep,\n+        MemoryRetrievalStep,\n+    ],\n+    Field(discriminator=\"step_type\"),\n+]\n \n-    async def list_shields(self) -> List[ShieldDef]:\n-        return [\n-            ShieldDef(\n-                identifier=ShieldType.llama_guard.value,\n-                type=ShieldType.llama_guard.value,\n-                params={},\n-            )\n+\n+@json_schema_type\n+class Turn(BaseModel):\n+    \"\"\"A single turn in an interaction with an Agentic System.\"\"\"\n+\n+    turn_id: str\n+    session_id: str\n+    input_messages: List[\n+        Union[\n+            UserMessage,\n+            ToolResponseMessage,\n         ]\n+    ]\n+    steps: List[Step]\n+    output_message: CompletionMessage\n+    output_attachments: List[Attachment] = Field(default_factory=list)\n+\n+    started_at: datetime\n+    completed_at: Optional[datetime] = None\n+\n+\n+@json_schema_type\n+class Session(BaseModel):\n+    \"\"\"A single session of an interaction with an Agentic System.\"\"\"\n+\n+    session_id: str\n+    session_name: str\n+    turns: List[Turn]\n+    started_at: datetime\n+\n+    memory_bank: Optional[MemoryBankDef] = None\n+\n+\n+class AgentConfigCommon(BaseModel):\n+    sampling_params: Optional[SamplingParams] = SamplingParams()\n+\n+    input_shields: Optional[List[str]] = Field(default_factory=list)\n+    output_shields: Optional[List[str]] = Field(default_factory=list)\n+\n+    tools: Optional[List[AgentToolDefinition]] = Field(default_factory=list)\n+    tool_choice: Optional[ToolChoice] = Field(default=ToolChoice.auto)\n+    tool_prompt_format: Optional[ToolPromptFormat] = Field(\n+        default=ToolPromptFormat.json\n+    )\n+\n+    max_infer_iters: int = 10\n+\n+\n+@json_schema_type\n+class AgentConfig(AgentConfigCommon):\n+    model: str\n+    instructions: str\n+    enable_session_persistence: bool\n+\n+\n+class AgentConfigOverridablePerTurn(AgentConfigCommon):\n+    instructions: Optional[str] = None\n+\n+\n+class AgentTurnResponseEventType(Enum):\n+    step_start = \"step_start\"\n+    step_complete = \"step_complete\"\n+    step_progress = \"step_progress\"\n+\n+    turn_start = \"turn_start\"\n+    turn_complete = \"turn_complete\"\n+\n+\n+@json_schema_type\n+class AgentTurnResponseStepStartPayload(BaseModel):\n+    event_type: Literal[AgentTurnResponseEventType.step_start.value] = (\n+        AgentTurnResponseEventType.step_start.value\n+    )\n+    step_type: StepType\n+    step_id: str\n+    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)\n+\n+\n+@json_schema_type\n+class AgentTurnResponseStepCompletePayload(BaseModel):\n+    event_type: Literal[AgentTurnResponseEventType.step_complete.value] = (\n+        AgentTurnResponseEventType.step_complete.value\n+    )\n+    step_type: StepType\n+    step_details: Step\n+\n+\n+@json_schema_type\n+class AgentTurnResponseStepProgressPayload(BaseModel):\n+    model_config = ConfigDict(protected_namespaces=())\n+\n+    event_type: Literal[AgentTurnResponseEventType.step_progress.value] = (\n+        AgentTurnResponseEventType.step_progress.value\n+    )\n+    step_type: StepType\n+    step_id: str\n+\n+    model_response_text_delta: Optional[str] = None\n+    tool_call_delta: Optional[ToolCallDelta] = None\n+    tool_response_text_delta: Optional[str] = None\n+\n+\n+@json_schema_type\n+class AgentTurnResponseTurnStartPayload(BaseModel):\n+    event_type: Literal[AgentTurnResponseEventType.turn_start.value] = (\n+        AgentTurnResponseEventType.turn_start.value\n+    )\n+    turn_id: str\n+\n+\n+@json_schema_type\n+class AgentTurnResponseTurnCompletePayload(BaseModel):\n+    event_type: Literal[AgentTurnResponseEventType.turn_complete.value] = (\n+        AgentTurnResponseEventType.turn_complete.value\n+    )\n+    turn: Turn\n+\n+\n+@json_schema_type\n+class AgentTurnResponseEvent(BaseModel):\n+    \"\"\"Streamed agent execution response.\"\"\"\n+\n+    payload: Annotated[\n+        Union[\n+            AgentTurnResponseStepStartPayload,\n+            AgentTurnResponseStepProgressPayload,\n+            AgentTurnResponseStepCompletePayload,\n+            AgentTurnResponseTurnStartPayload,\n+            AgentTurnResponseTurnCompletePayload,\n+        ],\n+        Field(discriminator=\"event_type\"),\n+    ]\n+\n+\n+@json_schema_type\n+class AgentCreateResponse(BaseModel):\n+    agent_id: str\n+\n+\n+@json_schema_type\n+class AgentSessionCreateResponse(BaseModel):\n+    session_id: str\n+\n+\n+@json_schema_type\n+class AgentTurnCreateRequest(AgentConfigOverridablePerTurn):\n+    agent_id: str\n+    session_id: str\n+\n+    # TODO: figure out how we can simplify this and make why\n+    # ToolResponseMessage needs to be here (it is function call\n+    # execution from outside the system)\n+    messages: List[\n+        Union[\n+            UserMessage,\n+            ToolResponseMessage,\n+        ]\n+    ]\n+    attachments: Optional[List[Attachment]] = None\n+\n+    stream: Optional[bool] = False\n+\n+\n+@json_schema_type\n+class AgentTurnResponseStreamChunk(BaseModel):\n+    event: AgentTurnResponseEvent\n+\n+\n+@json_schema_type\n+class AgentStepResponse(BaseModel):\n+    step: Step\n+\n+\n+@runtime_checkable\n+class Agents(Protocol):\n+    @webmethod(route=\"/agents/create\")\n+    async def create_agent(\n+        self,\n+        agent_config: AgentConfig,\n+    ) -> AgentCreateResponse: ...\n+\n+    # This method is not `async def` because it can result in either an\n+    # `AsyncGenerator` or a `AgentTurnCreateResponse` depending on the value of `stream`.\n+    @webmethod(route=\"/agents/turn/create\")\n+    def create_agent_turn(\n+        self,\n+        agent_id: str,\n+        session_id: str,\n+        messages: List[\n+            Union[\n+                UserMessage,\n+                ToolResponseMessage,\n+            ]\n+        ],\n+        attachments: Optional[List[Attachment]] = None,\n+        stream: Optional[bool] = False,\n+    ) -> AgentTurnResponseStreamChunk: ...\n+\n+    @webmethod(route=\"/agents/turn/get\")\n+    async def get_agents_turn(\n+        self,\n+        agent_id: str,\n+        turn_id: str,\n+    ) -> Turn: ...\n+\n+    @webmethod(route=\"/agents/step/get\")\n+    async def get_agents_step(\n+        self, agent_id: str, turn_id: str, step_id: str\n+    ) -> AgentStepResponse: ...\n+\n+    @webmethod(route=\"/agents/session/create\")\n+    async def create_agent_session(\n+        self,\n+        agent_id: str,\n+        session_name: str,\n+    ) -> AgentSessionCreateResponse: ...\n+\n+    @webmethod(route=\"/agents/session/get\")\n+    async def get_agents_session(\n+        self,\n+        agent_id: str,\n+        session_id: str,\n+        turn_ids: Optional[List[str]] = None,\n+    ) -> Session: ...\n+\n+    @webmethod(route=\"/agents/session/delete\")\n+    async def delete_agents_session(self, agent_id: str, session_id: str) -> None: ...\n \n-    async def run_shield(\n-        self, shield_type: str, messages: List[Message], params: Dict[str, Any] = None\n-    ) -> RunShieldResponse:\n-        shield_def = await self.shield_store.get_shield(shield_type)\n-        if not shield_def:\n-            raise ValueError(f\"Unknown shield {shield_type}\")\n-\n-        model = shield_def.params.get(\"model\", \"llama_guard\")\n-        if model not in TOGETHER_SHIELD_MODEL_MAP:\n-            raise ValueError(f\"Unsupported safety model: {model}\")\n-\n-        together_api_key = None\n-        if self.config.api_key is not None:\n-            together_api_key = self.config.api_key\n-        else:\n-            provider_data = self.get_request_provider_data()\n-            if provider_data is None or not provider_data.together_api_key:\n-                raise ValueError(\n-                    'Pass Together API Key in the header X-LlamaStack-ProviderData as { \"together_api_key\": <your api key>}'\n-                )\n-            together_api_key = provider_data.together_api_key\n-\n-        # messages can have role assistant or user\n-        api_messages = []\n-        for message in messages:\n-            if message.role in (Role.user.value, Role.assistant.value):\n-                api_messages.append({\"role\": message.role, \"content\": message.content})\n-\n-        violation = await get_safety_response(\n-            together_api_key, TOGETHER_SHIELD_MODEL_MAP[model], api_messages\n-        )\n-        return RunShieldResponse(violation=violation)\n-\n-\n-async def get_safety_response(\n-    api_key: str, model_name: str, messages: List[Dict[str, str]]\n-) -> Optional[SafetyViolation]:\n-    client = Together(api_key=api_key)\n-    response = client.chat.completions.create(messages=messages, model=model_name)\n-    if len(response.choices) == 0:\n-        return None\n-\n-    response_text = response.choices[0].message.content\n-    if response_text == \"safe\":\n-        return None\n-\n-    parts = response_text.split(\"\\n\")\n-    if len(parts) != 2:\n-        return None\n-\n-    if parts[0] == \"unsafe\":\n-        return SafetyViolation(\n-            violation_level=ViolationLevel.ERROR,\n-            metadata={\"violation_type\": parts[1]},\n-        )\n-\n-    return None\n+    @webmethod(route=\"/agents/delete\")\n+    async def delete_agents(\n+        self,\n+        agent_id: str,\n+    ) -> None: ...\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "72",
        "type": "8",
        "number": 209,
        "head_branch": "error-037-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe API routes for the `completion`, `chat_completion`, and `embeddings` methods in the `Inference` class have been updated. The `@webmethod` decorators for these methods previously specified routes under the `/inference/` path. In this change, the routes have been modified to use the `/v1/` path instead. Specifically:\n\n- The route for the `completion` method has been changed from `/inference/completion` to `/v1/completion`.\n- The route for the `chat_completion` method has been changed from `/inference/chat_completion` to `/v1/chat_completion`.\n- The route for the `embeddings` method has been changed from `/inference/embeddings` to `/v1/embeddings`.\n\nThese updates reflect a change in the API endpoint structure, standardizing the versioning by using the `/v1/` prefix for these methods.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpETZWaCrKNXsAGw+QbJAI7Y0riQZgBMAAwAnAA0kOK4HnSQAEQAgngs1EnQQVwAAiQSaB5gxR68LNy4YPgAjLyF8CQA7smxzWjIDgLM6jT0cpDYiJSQAFRj5LDFpaEAXhMaMLAkPJ7eFP6BiMGoaEXwHiqJ6MEA2rC4uNyIHAD0d0TqsNgCGkzMd0xKVKrq2ncAOL4DxKDBgWxgAAi1E6JFwd246zuEUiAF0ABSXa63B5PXAvN4fL6KZR/XAA4GgsgQmzQ2EjBFIrwoqIASiWcFQzI2WyCkA6yDQGWY1HgDDK8gYmyyA3kBNWBBBkDOAAM8t8yWoKfA7iRQiQwJsJE1mpRVZjsTd7o9nq93iwST8VNqAfrDcbTZQOUZ9EYTFAyPR8AAzHAEYhkZT9BSsdhcXj8YSicRSGRS0m/bVaHT6APLVB7LDCiOkchUGMfNgYTiQKjNeyOUUuSCDTVZ9Q53RgQzGcBgIwVERiO5obi6rxoUUAfR2aAYAGtR+PEHd4BgQ5QyAwSGuN1uMDuNNxZBwDMkLwYLJBUgBJSPl2WNpwt0OMaYYUiIIwAWUU8BDJp6AVG8rFvOt8DwaRIBDfA+BA1UPm4RJxHwDBVViRDplwackJQ+A0Iw9AMHoVUSGYAQ6FodciEQVVIDYAlFGQdc4hWSBVVvfdNkPEh6IqAgmG8QZsG4WgxU/NjVnVM0enhWBFHopQmArOCAHJkFVChIJofjnCneFRhDbTmA4vdNx4nc7g0Gz6IIMyJFqazbKWABlbhRAA8VJViBgP1IYD2O0qDkGMlgzPXCzt13PD4QI9C4nwBynNi1D0Mw8yDysvzqFwyp8MIxLkq+bC8uYZC4sI2JMFIzLLN3cjKNoajPzoorVTuRy9QoqiaLojQjHMSxUg8GgK3i5B7JA5SjnGtDkDfEgAA9uDgmM4LWAQPHFSB2HUJpvwMKAAFUxKfYKaB4fTGNGViZJIOSmNoJTRDg6gNtgvhGIU0jUvi+jWIYI5EE0riot4gGsFVIcU2XCcjhnOdFzh1dIqy3c0fq49ZFVLgtJ0kgAF5kjq6KSXKgqMGSejACTCDiLqJkmur+tDqaOyBTvEmMGauqgbr4O68lk77FMgZS3oIPhPoY+TFA4nKcJZhLAeB0HuOiyGOJhkcx3hqc0FnClkd11H1d40neOx3H6YJ4mLey0qlepyA6fxqC7eZx38sqqnVXZznzoJ3mDLGlAoaFh6ReesXXtUqWNqjjjGt61rNaBzo1fBndNeh7ThwRXW7knRGjaXE37Yxs2jxPa23ZoO3MbJ5Pmr653XYZj2nOblraLZi9kl9Ht/X7XaSP4MMS0IMtoySKt4zrNAGwcF95DbTMXU7bRuz0fMq3Uad4FoRBp09Fo6EN5xgj9PsIEgAAOENajoEMAGYAHYABYInft/agYWo34MFoAANloCGSIH8VAfwYAIYB/80ChBDOEX+9Br67xYL0HCh9j6nzNLQacQZt5AA===\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Be cautious with API route changes**\n\nChanging the route from `/inference/completion` to `/v1/completion` can break existing clients relying on the old endpoint. Please ensure that all clients are notified of this change, and consider implementing redirects or supporting both routes to maintain backward compatibility.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/inference/inference.py",
                "position": 5
            },
            {
                "body": "_:warning: Potential issue_\n\n**Be cautious with API route changes**\n\nUpdating the route from `/inference/chat_completion` to `/v1/chat_completion` may disrupt existing clients. Ensure that clients are informed about this change, and consider handling both routes temporarily to ease the transition.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/inference/inference.py",
                "position": 14
            },
            {
                "body": "_:warning: Potential issue_\n\n**Be cautious with API route changes**\n\nChanging the route from `/inference/embeddings` to `/v1/embeddings` can affect clients consuming this endpoint. Please ensure all stakeholders are aware of this update, and consider providing support for the old route during a deprecation period.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/inference/inference.py",
                "position": 23
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/apis/inference/inference.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " \n     # This method is not `async def` because it can result in either an\n     # `AsyncGenerator` or a `CompletionResponse` depending on the value of `stream`.\n-    @webmethod(route=\"/inference/completion\")\n+    @webmethod(route=\"/v1/completion\")\n     def completion(\n         self,\n         model: str,\n"
                    },
                    {
                        "id": 2,
                        "content": " \n     # This method is not `async def` because it can result in either an\n     # `AsyncGenerator` or a `ChatCompletionResponse` depending on the value of `stream`.\n-    @webmethod(route=\"/inference/chat_completion\")\n+    @webmethod(route=\"/v1/chat_completion\")\n     def chat_completion(\n         self,\n         model: str,\n"
                    },
                    {
                        "id": 3,
                        "content": "         logprobs: Optional[LogProbConfig] = None,\n     ) -> Union[ChatCompletionResponse, ChatCompletionResponseStreamChunk]: ...\n \n-    @webmethod(route=\"/inference/embeddings\")\n+    @webmethod(route=\"/v1/embeddings\")\n     async def embeddings(\n         self,\n         model: str,\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "71",
        "type": "8",
        "number": 208,
        "head_branch": "error-017-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe `server.py` module within the `llama_stack` distribution has been refactored to remove the FastAPI-based server implementation. Functions related to HTTP handling, such as `create_sse_event`, `global_exception_handler`, `translate_exception`, `passthrough`, `handle_sigint`, `lifespan`, `create_dynamic_passthrough`, `is_streaming_request`, `maybe_await`, `sse_generator`, `create_dynamic_typed_route`, and the entry point `main`, have been deleted. \n\nIn place of these functions, new classes have been introduced to manage routing and object registration. The `CommonRoutingTableImpl` class inherits from `RoutingTable` and provides methods like `initialize`, `shutdown`, `register_object`, and `get_provider_impl`. Three specialized routing table classes\u2014`ModelsRoutingTable`, `ShieldsRoutingTable`, and `MemoryBanksRoutingTable`\u2014inherit from `CommonRoutingTableImpl` and their respective interfaces (`Models`, `Shields`, and `MemoryBanks`). These classes include methods for listing, retrieving, and registering models, shields, and memory banks.\n\nAdditionally, helper functions `get_impl_api` and `register_object_with_provider` have been added to facilitate interaction with provider implementations. The changes shift the focus from handling HTTP requests to managing routing logic for models, shields, and memory banks within the application.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Consider Thread Safety When Modifying Shared Data**\n\nModifying `self.registry` directly without synchronization mechanisms might lead to race conditions in a multi-threaded environment.\n\n\n\nIf the routing table will be accessed concurrently, consider using thread-safe data structures or adding locks when modifying `self.registry`.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 450
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Initialize Registry as a Default Dictionary**\n\nInitializing `self.registry` as a `defaultdict` can simplify code that adds entries to it.\n\n\n\nUse `collections.defaultdict` for cleaner code:\n\n```diff\n+from collections import defaultdict\n\n async def initialize(self) -> None:\n-    self.registry: Registry = {}\n+    self.registry: Registry = defaultdict(list)\n\n     def add_objects(objs: List[RoutableObjectWithProvider]) -> None:\n         for obj in objs:\n-            if obj.identifier not in self.registry:\n-                self.registry[obj.identifier] = []\n             self.registry[obj.identifier].append(obj)\n```\n\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 369
            },
            {
                "body": "_:warning: Potential issue_\n\n**Handle Unsupported API Types Explicitly**\n\nIn `register_object_with_provider`, if an unsupported `api` is encountered, the function does nothing. This could lead to silent failures if a new API type is added elsewhere.\n\n\n\nAdd an else clause to handle unexpected API types:\n\n```diff\n async def register_object_with_provider(obj: RoutableObject, p: Any) -> None:\n     api = get_impl_api(p)\n     if api == Api.inference:\n         await p.register_model(obj)\n     elif api == Api.safety:\n         await p.register_shield(obj)\n     elif api == Api.memory:\n         await p.register_memory_bank(obj)\n+    else:\n+        raise ValueError(f\"Unsupported API type: {api}\")\n```\n\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\nasync def register_object_with_provider(obj: RoutableObject, p: Any) -> None:\n    api = get_impl_api(p)\n    if api == Api.inference:\n        await p.register_model(obj)\n    elif api == Api.safety:\n        await p.register_shield(obj)\n    elif api == Api.memory:\n        await p.register_memory_bank(obj)\n    else:\n        raise ValueError(f\"Unsupported API type: {api}\")\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 346
            },
            {
                "body": "_:warning: Potential issue_\n\n**Validate Provider ID Before Registration**\n\nIn `register_object`, if `provider_id` is not provided and `impls_by_provider_id` is empty, `obj.provider_id` will remain `None`, potentially causing issues later.\n\n\n\nAdd a check to ensure `provider_id` is set:\n\n```diff\n if not obj.provider_id and len(self.impls_by_provider_id) > 0:\n     obj.provider_id = list(self.impls_by_provider_id.keys())[0]\n+elif not obj.provider_id:\n+    raise ValueError(\"Provider ID must be specified when no providers are registered.\")\n```\n\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    async def register_object(self, obj: RoutableObjectWithProvider):\n        # if provider_id is not specified, we'll pick an arbitrary one from existing entries\n        if not obj.provider_id and len(self.impls_by_provider_id) > 0:\n            obj.provider_id = list(self.impls_by_provider_id.keys())[0]\n        elif not obj.provider_id:\n            raise ValueError(\"Provider ID must be specified when no providers are registered.\")\n\n        if obj.provider_id not in self.impls_by_provider_id:\n            raise ValueError(f\"Provider `{obj.provider_id}` not found\")\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 441
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Avoid Wildcard Imports to Improve Readability**\n\nUsing wildcard imports like `from module import *` can pollute the namespace and make it harder to track where specific classes or functions are coming from. It's better to import only the necessary components explicitly.\n\n\n\nReplace wildcard imports with explicit imports:\n\n```diff\n-from llama_models.llama3.api.datatypes import *  # noqa: F403\n+from llama_models.llama3.api.datatypes import RoutableObject, Api\n\n-from llama_stack.apis.models import *  # noqa: F403\n+from llama_stack.apis.models import Models, ModelDefWithProvider\n\n-from llama_stack.apis.shields import *  # noqa: F403\n+from llama_stack.apis.shields import Shields, ShieldDefWithProvider\n\n-from llama_stack.apis.memory_banks import *  # noqa: F403\n+from llama_stack.apis.memory_banks import MemoryBanks, MemoryBankDefWithProvider\n```\n\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\nfrom typing import Any, Dict, List, Optional\n\nfrom llama_models.llama3.api.datatypes import RoutableObject, Api\n\nfrom llama_stack.apis.models import Models, ModelDefWithProvider\nfrom llama_stack.apis.shields import Shields, ShieldDefWithProvider\nfrom llama_stack.apis.memory_banks import MemoryBanks, MemoryBankDefWithProvider\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 41
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/distribution/server/server.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n \n-import asyncio\n-import functools\n-import inspect\n-import json\n-import signal\n-import traceback\n-\n-from contextlib import asynccontextmanager\n-from ssl import SSLError\n-from typing import Any, Dict, Optional\n-\n-import fire\n-import httpx\n-import yaml\n-\n-from fastapi import Body, FastAPI, HTTPException, Request, Response\n-from fastapi.exceptions import RequestValidationError\n-from fastapi.responses import JSONResponse, StreamingResponse\n-from pydantic import BaseModel, ValidationError\n-from termcolor import cprint\n-from typing_extensions import Annotated\n-\n-from llama_stack.distribution.distribution import builtin_automatically_routed_apis\n-\n-from llama_stack.providers.utils.telemetry.tracing import (\n-    end_trace,\n-    setup_logger,\n-    SpanStatus,\n-    start_trace,\n-)\n+from typing import Any, Dict, List, Optional\n+\n+from llama_models.llama3.api.datatypes import *  # noqa: F403\n+\n+from llama_stack.apis.models import *  # noqa: F403\n+from llama_stack.apis.shields import *  # noqa: F403\n+from llama_stack.apis.memory_banks import *  # noqa: F403\n+\n from llama_stack.distribution.datatypes import *  # noqa: F403\n \n-from llama_stack.distribution.request_headers import set_request_provider_data\n-from llama_stack.distribution.resolver import resolve_impls_with_routing\n-\n-from .endpoints import get_all_api_endpoints\n-\n-\n-def create_sse_event(data: Any) -> str:\n-    if isinstance(data, BaseModel):\n-        data = data.json()\n-    else:\n-        data = json.dumps(data)\n-\n-    return f\"data: {data}\\n\\n\"\n-\n-\n-async def global_exception_handler(request: Request, exc: Exception):\n-    traceback.print_exception(exc)\n-    http_exc = translate_exception(exc)\n-\n-    return JSONResponse(\n-        status_code=http_exc.status_code, content={\"error\": {\"detail\": http_exc.detail}}\n-    )\n-\n-\n-def translate_exception(exc: Exception) -> Union[HTTPException, RequestValidationError]:\n-    if isinstance(exc, ValidationError):\n-        exc = RequestValidationError(exc.raw_errors)\n-\n-    if isinstance(exc, RequestValidationError):\n-        return HTTPException(\n-            status_code=400,\n-            detail={\n-                \"errors\": [\n-                    {\n-                        \"loc\": list(error[\"loc\"]),\n-                        \"msg\": error[\"msg\"],\n-                        \"type\": error[\"type\"],\n-                    }\n-                    for error in exc.errors()\n-                ]\n-            },\n-        )\n-    elif isinstance(exc, ValueError):\n-        return HTTPException(status_code=400, detail=f\"Invalid value: {str(exc)}\")\n-    elif isinstance(exc, PermissionError):\n-        return HTTPException(status_code=403, detail=f\"Permission denied: {str(exc)}\")\n-    elif isinstance(exc, TimeoutError):\n-        return HTTPException(status_code=504, detail=f\"Operation timed out: {str(exc)}\")\n-    elif isinstance(exc, NotImplementedError):\n-        return HTTPException(status_code=501, detail=f\"Not implemented: {str(exc)}\")\n-    else:\n-        return HTTPException(\n-            status_code=500,\n-            detail=\"Internal server error: An unexpected error occurred.\",\n-        )\n-\n-\n-async def passthrough(\n-    request: Request,\n-    downstream_url: str,\n-    downstream_headers: Optional[Dict[str, str]] = None,\n-):\n-    await start_trace(request.path, {\"downstream_url\": downstream_url})\n-\n-    headers = dict(request.headers)\n-    headers.pop(\"host\", None)\n-    headers.update(downstream_headers or {})\n-\n-    content = await request.body()\n-\n-    client = httpx.AsyncClient()\n-    erred = False\n-    try:\n-        req = client.build_request(\n-            method=request.method,\n-            url=downstream_url,\n-            headers=headers,\n-            content=content,\n-            params=request.query_params,\n-        )\n-        response = await client.send(req, stream=True)\n-\n-        async def stream_response():\n-            async for chunk in response.aiter_raw(chunk_size=64):\n-                yield chunk\n-\n-            await response.aclose()\n-            await client.aclose()\n-\n-        return StreamingResponse(\n-            stream_response(),\n-            status_code=response.status_code,\n-            headers=dict(response.headers),\n-            media_type=response.headers.get(\"content-type\"),\n-        )\n-\n-    except httpx.ReadTimeout:\n-        erred = True\n-        return Response(content=\"Downstream server timed out\", status_code=504)\n-    except httpx.NetworkError as e:\n-        erred = True\n-        return Response(content=f\"Network error: {str(e)}\", status_code=502)\n-    except httpx.TooManyRedirects:\n-        erred = True\n-        return Response(content=\"Too many redirects\", status_code=502)\n-    except SSLError as e:\n-        erred = True\n-        return Response(content=f\"SSL error: {str(e)}\", status_code=502)\n-    except httpx.HTTPStatusError as e:\n-        erred = True\n-        return Response(content=str(e), status_code=e.response.status_code)\n-    except Exception as e:\n-        erred = True\n-        return Response(content=f\"Unexpected error: {str(e)}\", status_code=500)\n-    finally:\n-        await end_trace(SpanStatus.OK if not erred else SpanStatus.ERROR)\n-\n-\n-def handle_sigint(app, *args, **kwargs):\n-    print(\"SIGINT or CTRL-C detected. Exiting gracefully...\")\n-\n-    async def run_shutdown():\n-        for impl in app.__llama_stack_impls__.values():\n-            print(f\"Shutting down {impl}\")\n-            await impl.shutdown()\n-\n-    asyncio.run(run_shutdown())\n-\n-    loop = asyncio.get_event_loop()\n-    for task in asyncio.all_tasks(loop):\n-        task.cancel()\n-\n-    loop.stop()\n-\n-\n-@asynccontextmanager\n-async def lifespan(app: FastAPI):\n-    print(\"Starting up\")\n-    yield\n-\n-    print(\"Shutting down\")\n-    for impl in app.__llama_stack_impls__.values():\n-        await impl.shutdown()\n-\n-\n-def create_dynamic_passthrough(\n-    downstream_url: str, downstream_headers: Optional[Dict[str, str]] = None\n-):\n-    async def endpoint(request: Request):\n-        return await passthrough(request, downstream_url, downstream_headers)\n-\n-    return endpoint\n-\n-\n-def is_streaming_request(func_name: str, request: Request, **kwargs):\n-    # TODO: pass the api method and punt it to the Protocol definition directly\n-    return kwargs.get(\"stream\", False)\n-\n-\n-async def maybe_await(value):\n-    if inspect.iscoroutine(value):\n-        return await value\n-    return value\n-\n-\n-async def sse_generator(event_gen):\n-    try:\n-        async for item in event_gen:\n-            yield create_sse_event(item)\n-            await asyncio.sleep(0.01)\n-    except asyncio.CancelledError:\n-        print(\"Generator cancelled\")\n-        await event_gen.aclose()\n-    except Exception as e:\n-        traceback.print_exception(e)\n-        yield create_sse_event(\n-            {\n-                \"error\": {\n-                    \"message\": str(translate_exception(e)),\n-                },\n-            }\n-        )\n-    finally:\n-        await end_trace()\n-\n-\n-def create_dynamic_typed_route(func: Any, method: str):\n-\n-    async def endpoint(request: Request, **kwargs):\n-        await start_trace(func.__name__)\n-\n-        set_request_provider_data(request.headers)\n-\n-        is_streaming = is_streaming_request(func.__name__, request, **kwargs)\n-        try:\n-            if is_streaming:\n-                return StreamingResponse(\n-                    sse_generator(func(**kwargs)), media_type=\"text/event-stream\"\n-                )\n-            else:\n-                value = func(**kwargs)\n-                return await maybe_await(value)\n-        except Exception as e:\n-            traceback.print_exception(e)\n-            raise translate_exception(e) from e\n-        finally:\n-            await end_trace()\n-\n-    sig = inspect.signature(func)\n-    new_params = [\n-        inspect.Parameter(\n-            \"request\", inspect.Parameter.POSITIONAL_OR_KEYWORD, annotation=Request\n-        )\n-    ]\n-    new_params.extend(sig.parameters.values())\n-\n-    if method == \"post\":\n-        # make sure every parameter is annotated with Body() so FASTAPI doesn't\n-        # do anything too intelligent and ask for some parameters in the query\n-        # and some in the body\n-        new_params = [new_params[0]] + [\n-            param.replace(annotation=Annotated[param.annotation, Body(..., embed=True)])\n-            for param in new_params[1:]\n-        ]\n-\n-    endpoint.__signature__ = sig.replace(parameters=new_params)\n-\n-    return endpoint\n-\n-\n-def main(\n-    yaml_config: str = \"llamastack-run.yaml\",\n-    port: int = 5000,\n-    disable_ipv6: bool = False,\n-):\n-    with open(yaml_config, \"r\") as fp:\n-        config = StackRunConfig(**yaml.safe_load(fp))\n-\n-    app = FastAPI()\n-\n-    impls = asyncio.run(resolve_impls_with_routing(config))\n-    if Api.telemetry in impls:\n-        setup_logger(impls[Api.telemetry])\n-\n-    all_endpoints = get_all_api_endpoints()\n-\n-    if config.apis:\n-        apis_to_serve = set(config.apis)\n-    else:\n-        apis_to_serve = set(impls.keys())\n-\n-    for inf in builtin_automatically_routed_apis():\n-        apis_to_serve.add(inf.routing_table_api.value)\n-\n-    apis_to_serve.add(\"inspect\")\n-    for api_str in apis_to_serve:\n-        api = Api(api_str)\n-\n-        endpoints = all_endpoints[api]\n-        impl = impls[api]\n-\n-        if is_passthrough(impl.__provider_spec__):\n-            for endpoint in endpoints:\n-                url = impl.__provider_config__.url.rstrip(\"/\") + endpoint.route\n-                getattr(app, endpoint.method)(endpoint.route)(\n-                    create_dynamic_passthrough(url)\n-                )\n-        else:\n-            for endpoint in endpoints:\n-                if not hasattr(impl, endpoint.name):\n-                    # ideally this should be a typing violation already\n-                    raise ValueError(\n-                        f\"Could not find method {endpoint.name} on {impl}!!\"\n-                    )\n-\n-                impl_method = getattr(impl, endpoint.name)\n-\n-                getattr(app, endpoint.method)(endpoint.route, response_model=None)(\n-                    create_dynamic_typed_route(\n-                        impl_method,\n-                        endpoint.method,\n-                    )\n-                )\n \n-        cprint(f\"Serving API {api_str}\", \"white\", attrs=[\"bold\"])\n-        for endpoint in endpoints:\n-            cprint(f\" {endpoint.method.upper()} {endpoint.route}\", \"white\")\n+def get_impl_api(p: Any) -> Api:\n+    return p.__provider_spec__.api\n+\n \n-    print(\"\")\n-    app.exception_handler(RequestValidationError)(global_exception_handler)\n-    app.exception_handler(Exception)(global_exception_handler)\n-    signal.signal(signal.SIGINT, functools.partial(handle_sigint, app))\n+async def register_object_with_provider(obj: RoutableObject, p: Any) -> None:\n+    api = get_impl_api(p)\n+    if api == Api.inference:\n+        await p.register_model(obj)\n+    elif api == Api.safety:\n+        await p.register_shield(obj)\n+    elif api == Api.memory:\n+        await p.register_memory_bank(obj)\n \n-    app.__llama_stack_impls__ = impls\n \n-    import uvicorn\n+Registry = Dict[str, List[RoutableObjectWithProvider]]\n \n-    # FYI this does not do hot-reloads\n-    listen_host = \"::\" if not disable_ipv6 else \"0.0.0.0\"\n-    print(f\"Listening on {listen_host}:{port}\")\n-    uvicorn.run(app, host=listen_host, port=port)\n \n+# TODO: this routing table maintains state in memory purely. We need to\n+# add persistence to it when we add dynamic registration of objects.\n+class CommonRoutingTableImpl(RoutingTable):\n+    def __init__(\n+        self,\n+        impls_by_provider_id: Dict[str, RoutedProtocol],\n+    ) -> None:\n+        self.impls_by_provider_id = impls_by_provider_id\n+\n+    async def initialize(self) -> None:\n+        self.registry: Registry = {}\n+\n+        def add_objects(objs: List[RoutableObjectWithProvider]) -> None:\n+            for obj in objs:\n+                if obj.identifier not in self.registry:\n+                    self.registry[obj.identifier] = []\n+\n+                self.registry[obj.identifier].append(obj)\n+\n+        for pid, p in self.impls_by_provider_id.items():\n+            api = get_impl_api(p)\n+            if api == Api.inference:\n+                p.model_store = self\n+                models = await p.list_models()\n+                add_objects(\n+                    [ModelDefWithProvider(**m.dict(), provider_id=pid) for m in models]\n+                )\n+\n+            elif api == Api.safety:\n+                p.shield_store = self\n+                shields = await p.list_shields()\n+                add_objects(\n+                    [\n+                        ShieldDefWithProvider(**s.dict(), provider_id=pid)\n+                        for s in shields\n+                    ]\n+                )\n \n-if __name__ == \"__main__\":\n-    fire.Fire(main)\n+            elif api == Api.memory:\n+                p.memory_bank_store = self\n+                memory_banks = await p.list_memory_banks()\n+\n+                # do in-memory updates due to pesky Annotated unions\n+                for m in memory_banks:\n+                    m.provider_id = pid\n+\n+                add_objects(memory_banks)\n+\n+    async def shutdown(self) -> None:\n+        for p in self.impls_by_provider_id.values():\n+            await p.shutdown()\n+\n+    def get_provider_impl(\n+        self, routing_key: str, provider_id: Optional[str] = None\n+    ) -> Any:\n+        if routing_key not in self.registry:\n+            raise ValueError(f\"`{routing_key}` not registered\")\n+\n+        objs = self.registry[routing_key]\n+        for obj in objs:\n+            if not provider_id or provider_id == obj.provider_id:\n+                return self.impls_by_provider_id[obj.provider_id]\n+\n+        raise ValueError(f\"Provider not found for `{routing_key}`\")\n+\n+    def get_object_by_identifier(\n+        self, identifier: str\n+    ) -> Optional[RoutableObjectWithProvider]:\n+        objs = self.registry.get(identifier, [])\n+        if not objs:\n+            return None\n+\n+        # kind of ill-defined behavior here, but we'll just return the first one\n+        return objs[0]\n+\n+    async def register_object(self, obj: RoutableObjectWithProvider):\n+        # if provider_id is not specified, we'll pick an arbitrary one from existing entries\n+        if not obj.provider_id and len(self.impls_by_provider_id) > 0:\n+            obj.provider_id = list(self.impls_by_provider_id.keys())[0]\n+\n+        if obj.provider_id not in self.impls_by_provider_id:\n+            raise ValueError(f\"Provider `{obj.provider_id}` not found\")\n+        \n+        p = self.impls_by_provider_id[obj.provider_id]\n+        await register_object_with_provider(obj, p)\n+        \n+        # Direct registry modification without checking current state\n+        if obj.identifier not in self.registry:\n+            self.registry[obj.identifier] = []\n+        self.registry[obj.identifier].append(obj)\n+        # TODO: persist this to a store\n+\n+\n+class ModelsRoutingTable(CommonRoutingTableImpl, Models):\n+    async def list_models(self) -> List[ModelDefWithProvider]:\n+        objects = []\n+        for objs in self.registry.values():\n+            objects.extend(objs)\n+        return objects\n+\n+    async def get_model(self, identifier: str) -> Optional[ModelDefWithProvider]:\n+        return self.get_object_by_identifier(identifier)\n+\n+    async def register_model(self, model: ModelDefWithProvider) -> None:\n+        await self.register_object(model)\n+\n+\n+class ShieldsRoutingTable(CommonRoutingTableImpl, Shields):\n+    async def list_shields(self) -> List[ShieldDef]:\n+        objects = []\n+        for objs in self.registry.values():\n+            objects.extend(objs)\n+        return objects\n+\n+    async def get_shield(self, shield_type: str) -> Optional[ShieldDefWithProvider]:\n+        return self.get_object_by_identifier(shield_type)\n+\n+    async def register_shield(self, shield: ShieldDefWithProvider) -> None:\n+        await self.register_object(shield)\n+\n+\n+class MemoryBanksRoutingTable(CommonRoutingTableImpl, MemoryBanks):\n+    async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]:\n+        objects = []\n+        for objs in self.registry.values():\n+            objects.extend(objs)\n+        return objects\n+\n+    async def get_memory_bank(\n+        self, identifier: str\n+    ) -> Optional[MemoryBankDefWithProvider]:\n+        return self.get_object_by_identifier(identifier)\n+\n+    async def register_memory_bank(\n+        self, memory_bank: MemoryBankDefWithProvider\n+    ) -> None:\n+        await self.register_object(memory_bank)\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "73",
        "type": "8",
        "number": 210,
        "head_branch": "error-005-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nIn the `project/api/llama_stack/providers/impls/meta_reference/agents/agents.py` file, two key modifications were made:\n\n1. **Modification in `create_agent_turn` Method**: The condition for invoking the `_create_agent_turn_streaming` method was altered. Previously, the method was called only when `stream` was `True` (`if stream:`). The updated condition now checks if `stream` is `True` **or** if `attachments` is `None` (`if stream or attachments is None:`). This change means that `_create_agent_turn_streaming` will be invoked in cases where streaming is enabled or when there are no attachments provided.\n\n2. **Adjustment in `_create_agent_turn_streaming` Method**: The assignment statement `agent = await self.get_agent(request.agent_id)` is now enclosed within a conditional block. The condition checks if streaming is disabled and if there are attachments present (`if not request.stream and request.attachments:`). This adjustment ensures that the agent is fetched asynchronously only under these specific conditions.\n\nThere were no alterations to the declarations of exported or public entities in this update.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpETZWaCrKNXsAGw+QbJAI7Y0riQZgBMAIwADAA0kOK4HnSQAEQAgngs1EnQQVwAAiQSaB5gxR68LNy4YPjhvIXwJADu0cmQTWjIDgLM6jT0cpDYiJSQAFRj5LDFpaEAXhMaMAjI3J7eFP6BiMEdyGgZzNTwDGXyDJtZA/K4sCRx+PjeANoABnlMSlSq6toA9CRQiQwJsJI0mpRXgBdAAUsFwuG4iA4fz+RHUsGwAg0TGYf0+yh+uH+gOBoPBlAAlLFOpA2BhxPhyPR4FhbvclIgLvAqvAmUs4Pc1l5IJsAkFIABVGwAGRQyGe8MRyNR6NuWJxLHxikJamJ8D+AHEnkoMGBbGAACLUTokXB/YUeP4RSKwpVIlFojEa3Har4qPX/Y0eU3mmxWm0je2O51RSkaIz6IwmKBkej4ABmOAIxDIyn6ClY7C4vH4wlE4ikMnOOu+eq0On0KeWqFQmGzhFI5CoBdx9M4orQTXsjiOLkggwJdfUDd0YEMxnAYCMFREYj+aG4Bq8aCOAH0dmgGABrB0UfBgr6IP7wZjcDzXtjEvebDOUMgMEgbru4a9oH+IBo3CyBwBjJOBBgWJAqQAJK5t2Vwjk446Zow0wYKQiCJpAMFsnckCvBcJBZHu/7sHuuDYBQGCvHSdqwIosTsgoGC0OofJYBm+B8KyEj4MerJEARe5ESRZEMhRVEYAeuCXL0GG0dMMgkGQdKKPAGaNPQGbnswBEafYsnEcwHC0QQ+lZjscn8Hw1DEgwsD9sgqAAHJMiQpkJgYUC4XE+GvCJlw0KRP6SdRMlyYJtFPgxtBMfhnSIPARAYP2/BZq84nBAAvOgHTqPYJAeBmGikLgIXsDCYrbJoWV7vAtCUrRqAYPgw4fh4+AjPQTQYqy6BYK8BmtcE1VBBoVnGQN9BjTsWgIkejnsMitECJ1J5eeYlipB4NA9hxyDmcxSgMB4zjHEyyCoSQAAe3DcQW3E8FiHgnJA7DsdI2Fuegu35gdDx+Ryohnftl3pe9d0PUkT1rGtb0feI0gJuBySJvOyZLu9rEQwcOZdvmSR9sWg7Dg4yHyJOtYBjO2hznozZ9uo9W0IgL4NM0dAyc4wRJouECQOEJAZpEADsAAcovhBm4sAJziwArArssK6EDAAMyRLL6u0BmutoKEoS0LQAiy+EDC0KEovW/TmMC0z5UNWz5Kc7Qe5pvTQA===\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Potential Logical Error in Condition**\n\nThe condition `if stream or attachments is None:` may not align with the intended logic. If the goal is to proceed with `_create_agent_turn_streaming` when streaming is enabled or when attachments are provided, the condition should be `if stream or attachments is not None:`. Currently, the method proceeds when there are no attachments, which might not be the intended behavior.\n\n\n\nApply this diff to correct the condition:\n\n```diff\n-            if stream or attachments is None:\n+            if stream or attachments is not None:\n```\n\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n        if stream or attachments is not None:\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "position": 5
            },
            {
                "body": "_:warning: Potential issue_\n\n**Uninitialized Variable 'agent' May Cause NameError**\n\nIn `_create_agent_turn_streaming`, the variable `agent` is assigned inside the `if not request.stream and request.attachments:` block. However, `agent` is used in the `async for` loop immediately afterward, regardless of whether the condition is met. This could lead to a `NameError` if the condition is not satisfied and `agent` remains undefined.\n\n\n\nTo ensure `agent` is always initialized, consider moving the assignment outside the conditional block:\n\n```diff\n-            if not request.stream and request.attachments:\n-                agent = await self.get_agent(request.agent_id)\n+            agent = await self.get_agent(request.agent_id)\n             if not request.stream and request.attachments:\n                 # Additional logic if needed\n```\n\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n        agent = await self.get_agent(request.agent_id)\n        if not request.stream and request.attachments:\n            # Additional logic if needed\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "position": 15
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "             attachments=attachments,\n             stream=True,\n         )\n-        if stream:\n+        if stream or attachments is None:\n             return self._create_agent_turn_streaming(request)\n         else:\n             raise NotImplementedError(\"Non-streaming agent turns not yet implemented\")\n"
                    },
                    {
                        "id": 2,
                        "content": "         self,\n         request: AgentTurnCreateRequest,\n     ) -> AsyncGenerator:\n-        agent = await self.get_agent(request.agent_id)\n+        if not request.stream and request.attachments:\n+            agent = await self.get_agent(request.agent_id)\n         async for event in agent.create_and_execute_turn(request):\n             yield event\n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "74",
        "type": "8",
        "number": 211,
        "head_branch": "error-003-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request refactors the logic for loading and retrieving embedding models within the codebase. The `get_embedding_model` function has been updated to enhance how models are checked and accessed from the global `EMBEDDING_MODELS` dictionary. Previously, the function used `EMBEDDING_MODELS.get(model)` to attempt retrieval and directly returned the model if available. If not found, it loaded the model, stored it in the dictionary, and returned it. The revised approach explicitly checks if the model is not present using `if model not in EMBEDDING_MODELS`. If the model is absent, it proceeds to load the model, saves it into the dictionary, and then retrieves it with `EMBEDDING_MODELS[model]`. This change ensures that the model is properly loaded and stored before retrieval, streamlining the model management process.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpETZWaCrKNXsAGw+QbJAI7Y0riQAFQhZgBMAIxRYZDiuB50oSEARACCeCzUydBBXAACJBJoHmClHrws3Lhg+FG8xfAkAO6pcS1oyAwUJDn0cpDYiJQp5LCl5REAXmEaMAjI3J7evQFBkJ3IaFnM1PAMFfKk5FQ0A/IA2gAGBUxKVKrq2gD0JBEkYL0SzS2U1wBdAAUsFwuG4iA4LxeRHUsGwAg0TGYL3uyieuFe70+31+lAAlPM4CQeNgKNx8CN+AAzeKwEnLLyQNaBRDBNDcbh9CjIAiQAQk3oefrxfA4Ah7c7xILwDBEAA09i5DHg1IOR0g1PwfFw9Mgt2KkwqVWYNTqDVxrWumr6uDJJO1muwGDE8HwGCJesZq38rOChywAsgP1ayWokEuoPBkOhsN1CKRLFRinRakx8BeAHF8B4lBgwLYwAARahdEi4F7el7RKLAqMQqEwuEJ5HJh4qNOvbO5sgFmzF0sjCtVmuEoz6IwmKBkej4Wk7AjEMjKKXItgYTg8PiCESuqQyeRox5prQ6fRThaoVCYcWEE4r5Jr9hcKgteyOPYufmHlPH9Sn3QwEMYxwDAIwql3CsOQzLw0D2AB9Nk0AYABrSsKHwH4HkQF48HgDwcLYZhtVkF4pDEbVEIIXoNG4WQOAMVImIMCxIHSABJJdThFBwnC/OdGAmOVpCMYl9VIXB4JIZgBVoWhZSIeDiKUDxrWpZ1XXdFAsGucjqKo7USFo2RrS2ZkSGpZDqOSPl4FNDCpDpEkPHwNB5LldAMHoXpcAoZoSm8KoGGkZABOk2T3KISBlJIAj5isXF8GGDxZEVXUSXUl1xC0ny/OKGy9Ri7xqQw5gnMgIgXIEUp9QAUQAWQAIVqosi3YgA5TN4PqgB5ItaoAGQAZWteTNIwZx5GGBS6qalq2s67q+sGoaNAkoEivxa1MG88syXIeh1BQWkjpIAAPJBznmdjaQwfBcEVI6XLcgqSSKxU2UMw7gllcqxuyiaXEVHbzLtCgDpQTQFhJbBuFoEU0UgO63wYelUOQVVyqKlBkDu4JGhGDdtNm5rWo6rrev64brWmjzrkx7G8eJhrSYWinlup67aXS6KU28a8BEJh7IcgZ7aF5Qq+Y+6yMZ+rAef+t1AdSzz6HSrBcv86QRdpqLrhZ+byaWqmhsuIqAWuT1UDIBxeglt6+Zxm1cFR5ISpYP6DgByb+AwFL0GpGg+CYDA1QoZgZvUJY7bIYLpd6OCPFlGaecy8aAHJkBc2EGA0IxzEsdIPCD/Z3V5MUFdEYUziV0LaXOikKClR1lgEJOGEgdh1GaRBGOYgwJxAiBO68mlby4h96CfDcXzQN9eM/eRBiPDt/20QC9AvNd1Hg+Bxfgy0/loKjnGCQeLzQAAGNAAHY0AAFgADmiakIkv5+ADYAE4SEvr/kKiI/e+D8oi0Ffh/AQABWCIHwIhfw/jfDek5QIKFYDvPeiAD5NFDMfGcG8gA\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Consider thread safety when accessing the `EMBEDDING_MODELS` dictionary**\n\nMultiple concurrent calls to `get_embedding_model` with the same `model` might lead to race conditions when loading and caching embedding models. To ensure thread safety, consider using a lock or synchronization mechanism when modifying the shared `EMBEDDING_MODELS` dictionary.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "position": 18
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Use logging instead of print statements**\n\nTo maintain consistent logging and allow configurable log levels, consider using the `logging` module instead of `print()` statements.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "position": 8
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " def get_embedding_model(model: str) -> \"SentenceTransformer\":\n     global EMBEDDING_MODELS\n \n-    loaded_model = EMBEDDING_MODELS.get(model)\n-    if loaded_model is not None:\n-        return loaded_model\n+    if model not in EMBEDDING_MODELS:\n+        print(f\"Loading sentence transformer for {model}...\")\n+        from sentence_transformers import SentenceTransformer\n+        loaded_model = SentenceTransformer(model)\n+        EMBEDDING_MODELS[model] = loaded_model\n \n-    print(f\"Loading sentence transformer for {model}...\")\n-    from sentence_transformers import SentenceTransformer\n-\n-    loaded_model = SentenceTransformer(model)\n-    EMBEDDING_MODELS[model] = loaded_model\n+    loaded_model = EMBEDDING_MODELS[model]\n     return loaded_model\n \n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "75",
        "type": "8",
        "number": 212,
        "head_branch": "error-004-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nA new parameter `score_threshold` with a default value of `0.0` has been added to the `RewardScoringRequest` class. The `reward_score` method in the `RewardScoring` protocol has been updated to include this parameter in its signature. The method now contains logic to return an empty `RewardScoringResponse` if the `dialog_generations` list is empty. Additionally, when `score_threshold` is greater than or equal to zero, the method returns a `RewardScoringResponse` containing a `ScoredDialogGenerations` instance with empty `dialog` and `scored_generations` lists.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpETZWaCrKNXsAGw+QbJAI7Y0riQZgBMAIyhADSQ4rgedJAARACCeCzUidBBXAACJBJoHmBFHrws3Lhg+OG8BfAkAO5RSZCNaMgOAszqNPRykNiIlMnksEUloQBerQSxsCSQFCTc+Ijq+C7owQDasLi43IgcAPQnROqw2AIaTMwnTEpUquraJwDi+B5KGGC2YAARagdEi4AC6AAp9odjmcLrgrjc7g9FMoXrg3p9vmQ/jZAcDhrgTtxPB4ThFQgBKDQwBY8UlLfyBRDBdrINDpZjUeAMUryBjLTL9eQIxYEL6QHYAA1yjzRagx8BOJFCJDAywkDUalClkOhR1O50u11uLBRTxUCreKrVGq1lGptMoi2ciww+HQtFo6ng+AwRUgSkVHmQ5U1Sn6JFkfvoopQGG9vPEfvm1BQrI6kFI5CofUgAwRqA5BC5efFHg0Rn0RhMUDI9HwADMcARiGRlHm7mwMJweHxBCIxJJpPn+ajngqtDp9LXaagi1hi4Rsx3El32FwqI17I4uVsBnKJ+op7owIZjOAwEZyoOiWhuEqvGguQB9FloBgAaxO96QJ2W7QULQb5MBQ8AYEQ/5NM4wGIKB4FEBo3CyBwBhJOhBgWJAKQAJJtjmQo7k4WxNow4wQdIRg4VgUoATBIGbAhSGyFKMRoJA5Dbo2DTfJAUpwZsJAvgiyyILAXy0FwjYePgaYALyQAADBoilSm0mZoF6iRzHGUq+IBtAAMrwRBvgBEEakMB4HSIDScCLLR0FAQxyxqWwCKKG0lzgfMDn6TBxmMRBanlAQTDeGygzcLQhFzOBVnYEo8yoJxPDOM+oIjPxoFCSJ0jid8UkyfJSkqVKdl0u54n0O627xR4iUjjJFwMKmwTLLg2AUBg7JYCQzCVPIelOUZJlEL4iCrD1JBqfAzZSt6RT4EQL4rrmvo9bNyD9YNNIpF6Pp+nyMRzXxAnLMJsCiQVtBbVmgo0HwCKYPwfBMgGcxTJQ+AxHGVWeR1XU9egfH+UBgVgaZ0hTcMll+oqGAISDUoQ3QALwEtRDvO261+ogs09RiGAMIsjSXJAO24ENi3NWpmD0NlgnAWt3J42pHhILgtlGOYlgpB4j2s8DOl0koVnpcmwOkSQAAeqwUHmmz0gIHOtewPqUQYUAAGI8fQmkRlwjMXXlYkSUVsnBApymqfGjDWYgyDDQZEMIWZzK4ATfF0c551MchUpa5AACyoLVfY8BEP6nXLFFMV9EbSjNj7sE5RCwweI2MQ08tq040LxyQAAMpzOzo5j2MEZLiBgjEzCoh4XAshQlK6HokAAKqI36Oxg6NQXjdDeMkGCXuhfg4WgyNrvBXbjkGS5/ssZAgBJhHxSeMgv50kOnJCZ9nGPNXnVcbYXJcsmXh/LZXHan7XkD10ojf2LgFAxNvl3XebkDSZbkDWypVuZ5O7dwwL3aeY0JowxHmPCg+AwoSmdgFMaXt570T9hBZigd0JJCrGeGsl4KYJn4M2Jc+FVz0HXD2TcaBtwOGIvIA845LTHm0KePQs4uzqBfPAWgiAXx2iaHQN8GIFbsIIRASA4QADMilRAUkbI2UIDAAAcKgSDSNoAAdgAJw6PCLQcIjYABsKjaAAFYSBKJJjorRAgtGaXEReSRXDcA8L4QI+oQjgL1nYUAA\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Reevaluate the logic for `score_threshold` handling.**\n\nThe condition `if score_threshold >= 0:` will always be true with the default value `0.0` and any positive threshold, resulting in the method returning a `RewardScoringResponse` with empty `scored_generations` without processing `dialog_generations`. This might prevent expected scoring behavior for non-negative thresholds.\n\nConsider reviewing the logic to ensure that `dialog_generations` are appropriately processed based on the `score_threshold`. If the intention is to process and filter `dialog_generations` according to the threshold, additional implementation may be needed.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/reward_scoring/reward_scoring.py",
                "position": 24
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/apis/reward_scoring/reward_scoring.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " \n     dialog_generations: List[DialogGenerations]\n     model: str\n+    score_threshold: float = 0.0\n \n \n @json_schema_type\n"
                    },
                    {
                        "id": 2,
                        "content": "     def reward_score(\n         self,\n         dialog_generations: List[DialogGenerations],\n-        model: str,\n-    ) -> Union[RewardScoringResponse]: ...\n+        model: str, \n+        score_threshold: float = 0.0,\n+    ) -> Union[RewardScoringResponse]:\n+        if not dialog_generations:\n+            return RewardScoringResponse(scored_generations=[])\n+        \n+        if score_threshold >= 0:\n+            return RewardScoringResponse(\n+                scored_generations=[\n+                    ScoredDialogGenerations(dialog=[], scored_generations=[])\n+                ])\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "76",
        "type": "8",
        "number": 213,
        "head_branch": "error-018-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request updates the `PGVectorMemoryAdapter` class in the `project/api/llama_stack/providers/adapters/memory/pgvector/pgvector.py` file. A new instance variable, `_bank_metadata_cache`, is introduced to cache memory bank metadata. In the `register_memory_bank` method, the `memory_bank` is stored in `_bank_metadata_cache` after models are upserted, and a `self.conn.commit()` call is added to commit changes to the database.\n\nThe `list_memory_banks` method is modified to return a list of banks using `_bank_metadata_cache`, ensuring the metadata is current. The `insert_documents` and `query_documents` methods are updated to check if the `bank_id` exists in `_bank_metadata_cache` before retrieving the index from the cache. This change alters the logic to operate only on registered memory banks.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpETZWaCrKNwSPbABsvkCiQBHbGlcSHFcLzpIACpogCIAQTwWaijoEK4AARIJNC8wPK9eFm5cMHwARl4c+BIAdzjYyDq0ZAcBZnUaejlIbERKGOjyWDyCgCYALybMelQMRwFKKNjxioBmWI1IAElQluQ0ZOZqeAZC+QZ/VJ75XFgPAnxfAG0AA0ymJSpVdW0AehI4xIYH8ElqdUobwAugAKWC4XDcRAcf7/IjqWDYAQaJjMf5fZS/XAAoEgsEQygASgANOhkPAMLQzjcUFh7h4lIgrvBSvB8Bhtu5PD4/IFgohQqg0Ll4F4VJF0KEXgikSi0Rj7tjcSwCYoiWoSfB/gBxZ5KDBgWxgAAi1FaJFw/243i8/zW6zhquRqPRmO1eL13xUhoBZq8FqtNlt9oGTpdPndGypGiM+iMJigZHo+AAZjgCMQyMpugpWOwuLx+MJROIpDJLvqfoatDp9JmYAgGYcsEdC6RyFRS3i2BhOH40HV7I4Ti5IL1Cc31K3dGBDMZwGAjMURGJ/mhuMafGgTgB9SVoBgAa2dFHw4O+iH3tAPNAoT7YzHwLmdRCkYm/X9/wICgNG4WQOAMOJoIMCxIASHYi0HVkHCcOc80YUYMFIRAjASSByCnRkLwwBgPFyCh4AVDw3lPARMCvU82BJF8SVPc4GAeN5IFGGQSDIdBaCUegCDCB5IDeKwTQANVrb8AFkSC/FwEhfUooUYeVEGQUSOPEz9v3keiMCvSBmLQVi0CFcS3n8DFJUoJilMMuiGO45jYEUAj8CnSVv2kMSaIMlxXJM7jGQk0LGPMyz2MvLj0FzN8+mRShxGwsz9S8RBth2dkbK8JBcCc5TZCixB3MdTzaDpDkxVwbAKCwC8aFHUJePnfisC/Zlc1qET8Hqxre0gQrJX4fNjKvZB+kZIhIqmpyWPtOLOJIbjRLIBx/BSsBC1YjwYvtayaOItLT1ofAGEcdgKvQJkJKCSgysu662rujzFGQUYpE6gTsG4A6Bsw0RTPgfM6reRb4FobiSAADyKhksFoxajrYvT1s63N/PqyiagyurGSUeHIFzO9mECxh4pIOk8jfOaqa8fAMQYMJBvwbgSw8AUvHkAUxXst8omCoyGJy+ChPUfkMAuOmJIGLxcx1DBBRHdRYSpbjzlFDqlgEizhKVKnswmiS7KKxzRaijbBvV0JOMwHD2apyz6IGVMjFgywEi8N9TgFHTBrqpQGHlIcZeQDCEe4b9S2/TwBEKtn2Gl6QoJggx0w3CBIFNjC+0IAdufoEcKwnXyZ2cIzG2DYkV3bTcy06YqYcQU8KXqOhzxJChQmzjtaAADgATiH3N1gEAAGAQKn4gA2YFxgYafaAYdYGAqAAWeep4YXMh/GLeBHn2h57QKeKnn1d1w7e3Tzbjuai72hT1N/QgA===\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Correct the condition for index retrieval in `insert_documents`**\n\nIn the `insert_documents` method, the condition may not accurately verify if the `bank_id` exists. It's more appropriate to check if `bank_id` is in `self.cache`.\n\n\n\nApply this diff to correct the condition:\n\n```diff\n-        index = self.cache.get(bank_id) if bank_id in self._bank_metadata_cache else None\n+        index = self.cache.get(bank_id)\n         if not index:\n             raise ValueError(f\"Bank {bank_id} not found\")\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n        index = self.cache.get(bank_id)\n        if not index:\n            raise ValueError(f\"Bank {bank_id} not found\")\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/memory/pgvector/pgvector.py",
                "position": 39
            },
            {
                "body": "_:warning: Potential issue_\n\n**Correct the condition for index retrieval in `query_documents`**\n\nIn the `query_documents` method, the condition should check if `bank_id` is in `self.cache` instead of `_bank_metadata_cache` to accurately retrieve the index.\n\n\n\nApply this diff to correct the condition:\n\n```diff\n-        index = self.cache.get(bank_id) if bank_id in self._bank_metadata_cache else None\n+        index = self.cache.get(bank_id)\n         if not index:\n             raise ValueError(f\"Bank {bank_id} not found\")\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n        index = self.cache.get(bank_id)\n        if not index:\n            raise ValueError(f\"Bank {bank_id} not found\")\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/memory/pgvector/pgvector.py",
                "position": 48
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/memory/pgvector/pgvector.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "         self.cursor = None\n         self.conn = None\n         self.cache = {}\n+        self._bank_metadata_cache = {}\n \n     async def initialize(self) -> None:\n         try:\n"
                    },
                    {
                        "id": 2,
                        "content": "                 (memory_bank.identifier, memory_bank),\n             ],\n         )\n+        self._bank_metadata_cache[memory_bank.identifier] = memory_bank\n \n         index = BankWithIndex(\n             bank=memory_bank,\n             index=PGVectorIndex(memory_bank, ALL_MINILM_L6_V2_DIMENSION, self.cursor),\n         )\n         self.cache[memory_bank.identifier] = index\n+        self.conn.commit()\n \n     async def list_memory_banks(self) -> List[MemoryBankDef]:\n         banks = load_models(self.cursor, MemoryBankDef)\n"
                    },
                    {
                        "id": 3,
                        "content": "                     index=PGVectorIndex(bank, ALL_MINILM_L6_V2_DIMENSION, self.cursor),\n                 )\n                 self.cache[bank.identifier] = index\n-        return banks\n+        return [self._bank_metadata_cache.get(bank.identifier, bank) for bank in banks]\n \n     async def insert_documents(\n         self,\n"
                    },
                    {
                        "id": 4,
                        "content": "         documents: List[MemoryBankDocument],\n         ttl_seconds: Optional[int] = None,\n     ) -> None:\n-        index = self.cache.get(bank_id, None)\n+        index = self.cache.get(bank_id) if bank_id in self._bank_metadata_cache else None\n         if not index:\n             raise ValueError(f\"Bank {bank_id} not found\")\n \n"
                    },
                    {
                        "id": 5,
                        "content": "         query: InterleavedTextMedia,\n         params: Optional[Dict[str, Any]] = None,\n     ) -> QueryDocumentsResponse:\n-        index = self.cache.get(bank_id, None)\n+        index = self.cache.get(bank_id) if bank_id in self._bank_metadata_cache else None\n         if not index:\n             raise ValueError(f\"Bank {bank_id} not found\")\n \n"
                    }
                ]
            }
        ]
    }
]