[
    {
        "id": "39",
        "type": "3",
        "number": 88,
        "head_branch": "error-034-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the `faiss.py` file involve a comprehensive restructuring of the codebase related to document embeddings and memory management. The `FaissIndex` class has been removed, and its functionalities have been integrated into a newly introduced `EmbeddingIndex` class, which includes abstract methods for adding chunks and querying embeddings. The `FaissMemoryImpl` class has been updated to work with the new `EmbeddingIndex`, leading to modifications in its `insert_documents` and `query_documents` methods.\n\nSeveral new utility functions have been added, such as `get_embedding_model` for loading sentence transformer models, `parse_data_url` for processing data URLs, and `content_from_data` for handling various MIME types and decoding base64 data. Additionally, the `content_from_doc` function has been revised to support both URL and data URI formats.\n\nThe introduction of the `make_overlapped_chunks` function allows for the creation of overlapping text chunks from documents. A new data class, `BankWithIndex`, has been added to encapsulate memory banks and their corresponding embedding indices. Overall, these changes enhance the document processing and embedding management capabilities within the code.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider caching query embeddings.**\n\nIf the same query is made multiple times, we're recomputing its embedding each time. Consider implementing a simple LRU cache.\n\n```diff\n+from functools import lru_cache\n+\n+@lru_cache(maxsize=1000)\n+def get_query_embedding(model: \"SentenceTransformer\", query_str: str) -> NDArray:\n+    return model.encode([query_str])[0].astype(np.float32)\n+\n     async def query_documents(\n         self,\n         query: InterleavedTextMedia,\n         params: Optional[Dict[str, Any]] = None,\n     ) -> QueryDocumentsResponse:\n         # ... existing code ...\n         \n         model = get_embedding_model(self.bank.embedding_model)\n-        query_vector = model.encode([query_str])[0].astype(np.float32)\n+        query_vector = get_query_embedding(model, query_str)\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 274
            },
            {
                "body": "_:warning: Potential issue_\n\n**Consider adding error handling for model loading.**\n\nThe model loading could fail for various reasons (network issues, invalid model name, etc.). Consider adding proper error handling.\n\n```diff\n def get_embedding_model(model: str) -> \"SentenceTransformer\":\n     global EMBEDDING_MODELS\n \n     loaded_model = EMBEDDING_MODELS.get(model)\n     if loaded_model is not None:\n         return loaded_model\n \n     print(f\"Loading sentence transformer for {model}...\")\n-    from sentence_transformers import SentenceTransformer\n+    try:\n+        from sentence_transformers import SentenceTransformer\n \n-    loaded_model = SentenceTransformer(model)\n-    EMBEDDING_MODELS[model] = loaded_model\n-    return loaded_model\n+        loaded_model = SentenceTransformer(model)\n+        EMBEDDING_MODELS[model] = loaded_model\n+        return loaded_model\n+    except Exception as e:\n+        raise RuntimeError(f\"Failed to load embedding model '{model}': {str(e)}\")\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\ndef get_embedding_model(model: str) -> \"SentenceTransformer\":\n    global EMBEDDING_MODELS\n\n    loaded_model = EMBEDDING_MODELS.get(model)\n    if loaded_model is not None:\n        return loaded_model\n\n    print(f\"Loading sentence transformer for {model}...\")\n    try:\n        from sentence_transformers import SentenceTransformer\n\n        loaded_model = SentenceTransformer(model)\n        EMBEDDING_MODELS[model] = loaded_model\n        return loaded_model\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load embedding model '{model}': {str(e)}\")\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 57
            },
            {
                "body": "_:warning: Potential issue_\n\n**Add timeout for HTTP requests.**\n\nThe HTTP requests could hang indefinitely. Consider adding timeouts and retries.\n\n```diff\n     if isinstance(doc.content, URL):\n         if doc.content.uri.startswith(\"data:\"):\n             return content_from_data(doc.content.uri)\n         else:\n-            async with httpx.AsyncClient() as client:\n+            async with httpx.AsyncClient(timeout=30.0) as client:\n                 r = await client.get(doc.content.uri)\n+                r.raise_for_status()\n                 return r.text\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 131
            },
            {
                "body": "_:warning: Potential issue_\n\n**Consider adding validation for window and overlap lengths.**\n\nThe function should validate that window_len is greater than overlap_len and both are positive.\n\n```diff\n def make_overlapped_chunks(\n     document_id: str, text: str, window_len: int, overlap_len: int\n ) -> List[Chunk]:\n+    if window_len <= 0:\n+        raise ValueError(\"window_len must be positive\")\n+    if overlap_len < 0:\n+        raise ValueError(\"overlap_len must be non-negative\")\n+    if overlap_len >= window_len:\n+        raise ValueError(\"overlap_len must be less than window_len\")\n+\n     tokenizer = Tokenizer.get_instance()\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\ndef make_overlapped_chunks(\n    document_id: str, text: str, window_len: int, overlap_len: int\n) -> List[Chunk]:\n    if window_len <= 0:\n        raise ValueError(\"window_len must be positive\")\n    if overlap_len < 0:\n        raise ValueError(\"overlap_len must be non-negative\")\n    if overlap_len >= window_len:\n        raise ValueError(\"overlap_len must be less than window_len\")\n\n    tokenizer = Tokenizer.get_instance()\n    tokens = tokenizer.encode(text, bos=False, eos=False)\n\n    chunks = []\n    for i in range(0, len(tokens), window_len - overlap_len):\n        toks = tokens[i : i + window_len]\n        chunk = tokenizer.decode(toks)\n        chunks.append(\n            Chunk(content=chunk, token_count=len(toks), document_id=document_id)\n        )\n\n    return chunks\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 151
            },
            {
                "body": "_:warning: Potential issue_\n\n**Consider adding validation for the data URL length.**\n\nVery long data URLs could potentially cause memory issues. Consider adding a maximum length check.\n\n```diff\n def parse_data_url(data_url: str):\n+    MAX_DATA_URL_LENGTH = 50 * 1024 * 1024  # 50MB limit\n+    if len(data_url) > MAX_DATA_URL_LENGTH:\n+        raise ValueError(f\"Data URL exceeds maximum length of {MAX_DATA_URL_LENGTH} bytes\")\n+\n     data_url_pattern = re.compile(\n         r\"^\"\n         r\"data:\"\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\ndef parse_data_url(data_url: str):\n    MAX_DATA_URL_LENGTH = 50 * 1024 * 1024  # 50MB limit\n    if len(data_url) > MAX_DATA_URL_LENGTH:\n        raise ValueError(f\"Data URL exceeds maximum length of {MAX_DATA_URL_LENGTH} bytes\")\n\n    data_url_pattern = re.compile(\n        r\"^\"\n        r\"data:\"\n        r\"(?P<mimetype>[\\w/\\-+.]+)\"\n        r\"(?P<charset>;charset=(?P<encoding>[\\w-]+))?\"\n        r\"(?P<base64>;base64)?\"\n        r\",(?P<data>.*)\"\n        r\"$\",\n        re.DOTALL,\n    )\n    match = data_url_pattern.match(data_url)\n    if not match:\n        raise ValueError(\"Invalid Data URL format\")\n\n    parts = match.groupdict()\n    parts[\"is_base64\"] = bool(parts[\"base64\"])\n    return parts\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 77
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider adding batch processing for document insertion.**\n\nThe current implementation processes documents sequentially. Consider adding batch processing for better performance.\n\n```diff\n     async def insert_documents(\n         self,\n         documents: List[MemoryBankDocument],\n     ) -> None:\n         model = get_embedding_model(self.bank.embedding_model)\n+        BATCH_SIZE = 10\n+        \n+        for i in range(0, len(documents), BATCH_SIZE):\n+            batch = documents[i:i + BATCH_SIZE]\n+            chunks_list = []\n+            contents = []\n+            \n+            # Process documents in parallel\n+            contents = await asyncio.gather(*[\n+                content_from_doc(doc) for doc in batch\n+            ])\n+            \n+            for doc, content in zip(batch, contents):\n+                chunks = make_overlapped_chunks(\n+                    doc.document_id,\n+                    content,\n+                    self.bank.chunk_size_in_tokens,\n+                    self.bank.overlap_size_in_tokens\n+                    or (self.bank.chunk_size_in_tokens // 4),\n+                )\n+                if chunks:\n+                    chunks_list.extend(chunks)\n+            \n+            if not chunks_list:\n+                continue\n+                \n+            # Process all chunks in batch\n+            embeddings = model.encode([x.content for x in chunks_list]).astype(np.float32)\n+            chunks_list = sorted(chunks_list, key=lambda x: hash(x.content))\n+            await self.index.add_chunks(chunks_list, embeddings)\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n@dataclass\nclass BankWithIndex:\n    bank: MemoryBankDef\n    index: EmbeddingIndex\n\n    async def insert_documents(\n        self,\n        documents: List[MemoryBankDocument],\n    ) -> None:\n        model = get_embedding_model(self.bank.embedding_model)\n        BATCH_SIZE = 10\n        \n        for i in range(0, len(documents), BATCH_SIZE):\n            batch = documents[i:i + BATCH_SIZE]\n            chunks_list = []\n            contents = []\n            \n            # Process documents in parallel\n            contents = await asyncio.gather(*[\n                content_from_doc(doc) for doc in batch\n            ])\n            \n            for doc, content in zip(batch, contents):\n                chunks = make_overlapped_chunks(\n                    doc.document_id,\n                    content,\n                    self.bank.chunk_size_in_tokens,\n                    self.bank.overlap_size_in_tokens\n                    or (self.bank.chunk_size_in_tokens // 4),\n                )\n                if chunks:\n                    chunks_list.extend(chunks)\n            \n            if not chunks_list:\n                continue\n                \n            # Process all chunks in batch\n            embeddings = model.encode([x.content for x in chunks_list]).astype(np.float32)\n            chunks_list = sorted(chunks_list, key=lambda x: hash(x.content))\n            await self.index.add_chunks(chunks_list, embeddings)\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 244
            },
            {
                "body": "_:warning: Potential issue_\n\n**Add memory management for large PDF files.**\n\nThe PDF processing could consume significant memory for large files. Consider adding checks and cleanup.\n\n```diff\n     elif mime_type == \"application/pdf\":\n-        pdf_bytes = io.BytesIO(data)\n-        pdf_reader = PdfReader(pdf_bytes)\n-        return \"\\n\".join([page.extract_text() for page in pdf_reader.pages])\n+        MAX_PDF_SIZE = 100 * 1024 * 1024  # 100MB limit\n+        if len(data) > MAX_PDF_SIZE:\n+            raise ValueError(f\"PDF exceeds maximum size of {MAX_PDF_SIZE} bytes\")\n+        \n+        try:\n+            pdf_bytes = io.BytesIO(data)\n+            pdf_reader = PdfReader(pdf_bytes)\n+            text = \"\\n\".join([page.extract_text() for page in pdf_reader.pages])\n+            return text\n+        finally:\n+            pdf_bytes.close()\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\ndef content_from_data(data_url: str) -> str:\n    parts = parse_data_url(data_url)\n    data = parts[\"data\"]\n\n    if parts[\"is_base64\"]:\n        data = base64.b64decode(data)\n    else:\n        data = unquote(data)\n        encoding = parts[\"encoding\"] or \"utf-8\"\n        data = data.encode(encoding)\n\n    encoding = parts[\"encoding\"]\n    if not encoding:\n        detected = chardet.detect(data)\n        encoding = detected[\"encoding\"]\n\n    mime_type = parts[\"mimetype\"]\n    mime_category = mime_type.split(\"/\")[0]\n    if mime_category == \"text\":\n        # For text-based files (including CSV, MD)\n        return data.decode(encoding)\n\n    elif mime_type == \"application/pdf\":\n        MAX_PDF_SIZE = 100 * 1024 * 1024  # 100MB limit\n        if len(data) > MAX_PDF_SIZE:\n            raise ValueError(f\"PDF exceeds maximum size of {MAX_PDF_SIZE} bytes\")\n        \n        try:\n            pdf_bytes = io.BytesIO(data)\n            pdf_reader = PdfReader(pdf_bytes)\n            text = \"\\n\".join([page.extract_text() for page in pdf_reader.pages])\n            return text\n        finally:\n            pdf_bytes.close()\n\n    else:\n        cprint(\"Could not extract content from data_url properly.\", color=\"red\")\n        return \"\"\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 111
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " #\n # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n-\n-import logging\n-\n+import base64\n+import io\n+import re\n+from abc import ABC, abstractmethod\n+from dataclasses import dataclass\n from typing import Any, Dict, List, Optional\n+from urllib.parse import unquote\n \n-import faiss\n+import chardet\n+import httpx\n import numpy as np\n from numpy.typing import NDArray\n+from pypdf import PdfReader\n+from termcolor import cprint\n \n from llama_models.llama3.api.datatypes import *  # noqa: F403\n+from llama_models.llama3.api.tokenizer import Tokenizer\n \n from llama_stack.apis.memory import *  # noqa: F403\n-from llama_stack.providers.datatypes import MemoryBanksProtocolPrivate\n-\n-from llama_stack.providers.utils.memory.vector_store import (\n-    ALL_MINILM_L6_V2_DIMENSION,\n-    BankWithIndex,\n-    EmbeddingIndex,\n-)\n-from llama_stack.providers.utils.telemetry import tracing\n-\n-from .config import FaissImplConfig\n-\n-logger = logging.getLogger(__name__)\n \n+ALL_MINILM_L6_V2_DIMENSION = 384\n+\n+EMBEDDING_MODELS = {}\n+\n+\n+def get_embedding_model(model: str) -> \"SentenceTransformer\":\n+    global EMBEDDING_MODELS\n+\n+    loaded_model = EMBEDDING_MODELS.get(model)\n+    if loaded_model is not None:\n+        return loaded_model\n+\n+    print(f\"Loading sentence transformer for {model}...\")\n+    from sentence_transformers import SentenceTransformer\n+\n+    loaded_model = SentenceTransformer(model)\n+    EMBEDDING_MODELS[model] = loaded_model\n+    return loaded_model\n+\n+\n+def parse_data_url(data_url: str):\n+    data_url_pattern = re.compile(\n+        r\"^\"\n+        r\"data:\"\n+        r\"(?P<mimetype>[\\w/\\-+.]+)\"\n+        r\"(?P<charset>;charset=(?P<encoding>[\\w-]+))?\"\n+        r\"(?P<base64>;base64)?\"\n+        r\",(?P<data>.*)\"\n+        r\"$\",\n+        re.DOTALL,\n+    )\n+    match = data_url_pattern.match(data_url)\n+    if not match:\n+        raise ValueError(\"Invalid Data URL format\")\n+\n+    parts = match.groupdict()\n+    parts[\"is_base64\"] = bool(parts[\"base64\"])\n+    return parts\n+\n+\n+def content_from_data(data_url: str) -> str:\n+    parts = parse_data_url(data_url)\n+    data = parts[\"data\"]\n+\n+    if parts[\"is_base64\"]:\n+        data = base64.b64decode(data)\n+    else:\n+        data = unquote(data)\n+        encoding = parts[\"encoding\"] or \"utf-8\"\n+        data = data.encode(encoding)\n+\n+    encoding = parts[\"encoding\"]\n+    if not encoding:\n+        detected = chardet.detect(data)\n+        encoding = detected[\"encoding\"]\n+\n+    mime_type = parts[\"mimetype\"]\n+    mime_category = mime_type.split(\"/\")[0]\n+    if mime_category == \"text\":\n+        # For text-based files (including CSV, MD)\n+        return data.decode(encoding)\n+\n+    elif mime_type == \"application/pdf\":\n+        # For PDF and DOC/DOCX files, we can't reliably convert to string)\n+        pdf_bytes = io.BytesIO(data)\n+        pdf_reader = PdfReader(pdf_bytes)\n+        return \"\\n\".join([page.extract_text() for page in pdf_reader.pages])\n+\n+    else:\n+        cprint(\"Could not extract content from data_url properly.\", color=\"red\")\n+        return \"\"\n+\n+\n+async def content_from_doc(doc: MemoryBankDocument) -> str:\n+    if isinstance(doc.content, URL):\n+        if doc.content.uri.startswith(\"data:\"):\n+            return content_from_data(doc.content.uri)\n+        else:\n+            async with httpx.AsyncClient() as client:\n+                r = await client.get(doc.content.uri)\n+                return r.text\n+\n+    pattern = re.compile(\"^(https?://|file://|data:)\")\n+    if pattern.match(doc.content):\n+        if doc.content.startswith(\"data:\"):\n+            return content_from_data(doc.content)\n+        else:\n+            async with httpx.AsyncClient() as client:\n+                r = await client.get(doc.content)\n+                return r.text\n+\n+    return interleaved_text_media_as_str(doc.content)\n+\n+\n+def make_overlapped_chunks(\n+    document_id: str, text: str, window_len: int, overlap_len: int\n+) -> List[Chunk]:\n+    tokenizer = Tokenizer.get_instance()\n+    tokens = tokenizer.encode(text, bos=False, eos=False)\n+\n+    chunks = []\n+    for i in range(0, len(tokens), window_len - overlap_len):\n+        toks = tokens[i : i + window_len]\n+        chunk = tokenizer.decode(toks)\n+        chunks.append(\n+            Chunk(content=chunk, token_count=len(toks), document_id=document_id)\n+        )\n \n-class FaissIndex(EmbeddingIndex):\n-    id_by_index: Dict[int, str]\n-    chunk_by_index: Dict[int, str]\n+    return chunks\n \n-    def __init__(self, dimension: int):\n-        self.index = faiss.IndexFlatL2(dimension)\n-        self.id_by_index = {}\n-        self.chunk_by_index = {}\n \n-    @tracing.span(name=\"add_chunks\")\n+class EmbeddingIndex(ABC):\n+    @abstractmethod\n     async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):\n-        indexlen = len(self.id_by_index)\n-        for i, chunk in enumerate(chunks):\n-            self.chunk_by_index[indexlen + i] = chunk\n-            self.id_by_index[indexlen + i] = chunk.document_id\n-\n-        self.index.add(np.array(embeddings).astype(np.float32))\n+        raise NotImplementedError()\n \n+    @abstractmethod\n     async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:\n-        distances, indices = self.index.search(\n-            embedding.reshape(1, -1).astype(np.float32), k\n-        )\n-\n-        chunks = []\n-        scores = []\n-        for d, i in zip(distances[0], indices[0]):\n-            if i < 0:\n-                continue\n-            chunks.append(self.chunk_by_index[int(i)])\n-            scores.append(1.0 / float(d))\n-\n-        return QueryDocumentsResponse(chunks=chunks, scores=scores)\n+        raise NotImplementedError()\n \n \n-class FaissMemoryImpl(Memory, MemoryBanksProtocolPrivate):\n-    def __init__(self, config: FaissImplConfig) -> None:\n-        self.config = config\n-        self.cache = {}\n-\n-    async def initialize(self) -> None: ...\n-\n-    async def shutdown(self) -> None: ...\n-\n-    async def register_memory_bank(\n-        self,\n-        memory_bank: MemoryBankDef,\n-    ) -> None:\n-        assert (\n-            memory_bank.type == MemoryBankType.vector.value\n-        ), f\"Only vector banks are supported {memory_bank.type}\"\n-\n-        index = BankWithIndex(\n-            bank=memory_bank, index=FaissIndex(ALL_MINILM_L6_V2_DIMENSION)\n-        )\n-        self.cache[memory_bank.identifier] = index\n-\n-    async def list_memory_banks(self) -> List[MemoryBankDef]:\n-        return [i.bank for i in self.cache.values()]\n+@dataclass\n+class BankWithIndex:\n+    bank: MemoryBankDef\n+    index: EmbeddingIndex\n \n     async def insert_documents(\n         self,\n-        bank_id: str,\n         documents: List[MemoryBankDocument],\n-        ttl_seconds: Optional[int] = None,\n     ) -> None:\n-        index = self.cache.get(bank_id)\n-        if index is None:\n-            raise ValueError(f\"Bank {bank_id} not found\")\n-\n-        await index.insert_documents(documents)\n+        model = get_embedding_model(self.bank.embedding_model)\n+        for doc in documents:\n+            content = await content_from_doc(doc)\n+            chunks = make_overlapped_chunks(\n+                doc.document_id,\n+                content,\n+                self.bank.chunk_size_in_tokens,\n+                self.bank.overlap_size_in_tokens\n+                or (self.bank.chunk_size_in_tokens // 4),\n+            )\n+            if not chunks:\n+                continue\n+            embeddings = model.encode([x.content for x in chunks]).astype(np.float32)\n+            # Unstable order of chunks\n+            chunks = sorted(chunks, key=lambda x: hash(x.content))\n+            \n+            await self.index.add_chunks(chunks, embeddings)\n \n     async def query_documents(\n         self,\n-        bank_id: str,\n         query: InterleavedTextMedia,\n         params: Optional[Dict[str, Any]] = None,\n     ) -> QueryDocumentsResponse:\n-        index = self.cache.get(bank_id)\n-        if index is None:\n-            raise ValueError(f\"Bank {bank_id} not found\")\n-\n-        return await index.query_documents(query, params)\n+        if params is None:\n+            params = {}\n+        k = params.get(\"max_chunks\", 3)\n+\n+        def _process(c) -> str:\n+            if isinstance(c, str):\n+                return c\n+            else:\n+                return \"<media>\"\n+\n+        if isinstance(query, list):\n+            query_str = \" \".join([_process(c) for c in query])\n+        else:\n+            query_str = _process(query)\n+\n+        model = get_embedding_model(self.bank.embedding_model)\n+        query_vector = model.encode([query_str])[0].astype(np.float32)\n+        return await self.index.query(query_vector, k)\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "40",
        "type": "3",
        "number": 89,
        "head_branch": "error-047-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces optimizations to the `OpenTelemetryAdapter` class in the `opentelemetry.py` file, specifically targeting the conversion of string representations of trace and span IDs into integers. Two caching mechanisms, `_trace_id_cache` and `_span_id_cache`, are implemented to store previously computed values, each with a maximum capacity of 10,000 entries. The `string_to_trace_id` and `string_to_span_id` functions are updated to utilize these caches, checking for existing values before performing conversions. If the cache exceeds its size limit, it is cleared. Additionally, the `string_to_span_id` function employs a hash of the first three characters of the input string as a cache key. \n\nThe overall structure of the `OpenTelemetryAdapter` class remains unchanged, with no alterations to its public interface or method signatures. The implementation maintains consistent error handling and control flow, focusing solely on enhancing the efficiency of ID conversions without modifying the existing functionality related to logging events or managing telemetry data.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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=\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/telemetry/opentelemetry/opentelemetry.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " from .config import OpenTelemetryConfig\n \n \n-def string_to_trace_id(s: str) -> int:\n-    # Convert the string to bytes and then to an integer\n-    return int.from_bytes(s.encode(), byteorder=\"big\", signed=False)\n+# Optimization: Cache trace and span ID conversions\n+_trace_id_cache = {}\n+_span_id_cache = {}\n+_max_cache_size = 10000\n \n+def string_to_trace_id(s: str) -> int:\n+    if s not in _trace_id_cache:\n+        if len(_trace_id_cache) >= _max_cache_size:\n+            _trace_id_cache.clear()\n+        _trace_id_cache[s] = int.from_bytes(s.encode(), byteorder=\"big\", signed=False)\n+    return _trace_id_cache[s]\n \n def string_to_span_id(s: str) -> int:\n-    # Use only the first 8 bytes (64 bits) for span ID\n-    return int.from_bytes(s.encode()[:8], byteorder=\"big\", signed=False)\n+    # Optimization: Cache span ID conversions\n+    # Use hash of first few chars as cache key for better memory usage\n+    key = hash(s[:3]) % 1000  \n+    if key not in _span_id_cache:\n+        if len(_span_id_cache) >= _max_cache_size:\n+            _span_id_cache.clear()\n+        # Convert full span ID string to int value\n+        _span_id_cache[key] = int.from_bytes(s.encode(), byteorder=\"little\", signed=False) \n+    return _span_id_cache[key]\n+\n \n \n def is_tracing_enabled(tracer):\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "41",
        "type": "3",
        "number": 90,
        "head_branch": "error-010-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces changes to the `server.py` file located in the `project/api/llama_stack/distribution/server` directory. The primary focus of these modifications is on the main function responsible for configuring and running the FastAPI application. A new parameter, `timeout_keep_alive`, has been added to the Uvicorn server configuration, setting its value to 5 seconds. This change indicates a more aggressive approach to managing connection timeouts.\n\nAdditionally, a comment has been added to highlight the introduction of \"risky assumptions about network reliability,\" which suggests a shift in how network behavior is expected to be handled. Despite these updates, the overall structure of the application remains unchanged; the main function continues to read the YAML configuration, set up the FastAPI app, and define routes as before. The existing exception handling and signal handling for graceful shutdowns are preserved without modifications. Overall, the changes consist of adding new parameters and comments while maintaining the existing functionality and control flow of the application.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [],
        "file_changes": [
            {
                "file": "project/api/llama_stack/distribution/server/server.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "     import uvicorn\n \n     # FYI this does not do hot-reloads\n+    # Added risky assumptions about network reliability\n     listen_host = \"::\" if not disable_ipv6 else \"0.0.0.0\"\n+    uvicorn_config = uvicorn.Config(app, host=listen_host, port=port,\n+                                   timeout_keep_alive=5)  # Aggressive timeout\n     print(f\"Listening on {listen_host}:{port}\")\n     uvicorn.run(app, host=listen_host, port=port)\n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "42",
        "type": "3",
        "number": 91,
        "head_branch": "error-024-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe recent changes to the `LocalInference` class in `LocalInference.swift` involve the addition of several new methods and modifications to existing functionality. New methods introduced include `encodeHeader`, `encodeDialogPrompt`, `getRole`, `encodeMessage`, `prepareMessages`, `maybeExtractCustomToolCalls`, and `decodeAssistantMessage`. These methods enhance the encoding and processing of messages, allowing for more structured handling of different message types.\n\nThe `encodeHeader` method formats a string based on a specified role, while `encodeDialogPrompt` constructs a prompt string from an array of messages, incorporating the encoded header. The `getRole` method identifies the role of a message, and `encodeMessage` encodes individual messages, considering their type and content. The `prepareMessages` method is responsible for preparing a list of messages, including system messages and user messages with tools.\n\nAdditionally, the error handling in the `loadModel` method has been removed, and the `chatCompletion` method has been significantly revised to integrate the new message processing logic. These changes collectively improve the message handling capabilities within the `LocalInference` class.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Simplify redundant content processing logic.**\n\nThe second switch statement partially duplicates the content processing from lines 66-75, and contains TODO comments about unimplemented features.\n\n```diff\n- switch (message) {\n-   case .UserMessage(let m):\n-     switch (m.content) {\n-     case .case1(let c):\n-       prompt += _processContent(c)\n-     case .case2(let c):\n-       prompt += _processContent(c)\n-     }\n-   case .CompletionMessage(let m):\n-     // TODO: Support encoding past tool call history\n-     // for t in m.tool_calls {\n-     //  _processContent(t.)\n-     //}\n-     eom = m.stop_reason == Components.Schemas.StopReason.end_of_message\n-   case .SystemMessage(_):\n-     break\n-   case .ToolResponseMessage(_):\n-     break\n-   }\n+ // Set end of message flag for completion messages\n+ if case .CompletionMessage(let m) = message {\n+   eom = m.stop_reason == Components.Schemas.StopReason.end_of_message\n+ }\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 248
            },
            {
                "body": "_:warning: Potential issue_\n\n**Fix nested function implementation and return value.**\n\nThe `_processContent` function has several issues:\n1. The nested `_process` function doesn't return anything but modifies the outer scope's `prompt` variable\n2. `_processContent` always returns an empty string despite processing content\n3. The nested function structure adds unnecessary complexity\n\n```diff\n func _processContent(_ content: Any) -> String {\n-    func _process(_ c: Any) {\n-      if let str = c as? String {\n-        prompt += str\n-      }\n-    }\n+    var result = \"\"\n+    if let str = content as? String {\n+      result = str\n+    } else if let list = content as? [Any] {\n+      result = list.compactMap { $0 as? String }.joined()\n+    }\n-    if let str = content as? String {\n-      _process(str)\n-    } else if let list = content as? [Any] {\n-      for c in list {\n-        _process(c)\n-      }\n-    }\n\n-    return \"\"\n+    return result\n }\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n  func _processContent(_ content: Any) -> String {\n    var result = \"\"\n    if let str = content as? String {\n      result = str\n    } else if let list = content as? [Any] {\n      result = list.compactMap { $0 as? String }.joined()\n    }\n\n    return result\n  }\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 86
            },
            {
                "body": "_:warning: Potential issue_\n\n**Consider adding validation for token content.**\n\nThe function assumes the content will always contain the expected tags and doesn't handle cases where they might be missing.\n\n```diff\n func decodeAssistantMessage(tokens: String, stopReason: Components.Schemas.StopReason) -> Components.Schemas.CompletionMessage {\n   var content = tokens\n+  \n+  // Validate that content contains required tags\n+  guard content.contains(\"<|eot_id|>\") || content.contains(\"<|eom_id|>\") else {\n+    // Handle invalid content - either return a default message or throw an error\n+    return Components.Schemas.CompletionMessage(\n+      content: .case1(\"Invalid message format\"),\n+      role: .assistant,\n+      stop_reason: .error,\n+      tool_calls: []\n+    )\n+  }\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 385
            },
            {
                "body": "_:warning: Potential issue_\n\n**Consider adding input validation for messages array.**\n\nThe function should validate that the messages array is not empty to avoid generating an incomplete prompt.\n\n```diff\n func encodeDialogPrompt(messages: [Components.Schemas.ChatCompletionRequest.messagesPayloadPayload]) -> String {\n+  guard !messages.isEmpty else {\n+    return \"\"  // or throw an appropriate error\n+  }\n   var prompt = \"\"\n   prompt.append(\"<|begin_of_text|>\")\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\nfunc encodeDialogPrompt(messages: [Components.Schemas.ChatCompletionRequest.messagesPayloadPayload]) -> String {\n  guard !messages.isEmpty else {\n    return \"\"  // or throw an appropriate error\n  }\n  var prompt = \"\"\n\n  prompt.append(\"<|begin_of_text|>\")\n  for message in messages {\n    let msg = encodeMessage(message: message)\n    prompt += msg\n  }\n\n  prompt.append(encodeHeader(role: \"assistant\"))\n\n  return prompt\n}\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 44
            },
            {
                "body": "_:warning: Potential issue_\n\n**Improve error handling in tool call extraction.**\n\nThe function has several areas where it silently fails by returning an empty array:\n1. Line 165: Returns empty array on parsing failure\n2. Line 173: Returns empty array on JSON parsing failure\n3. Line 188: Returns empty array for unsupported parameter types\n\nConsider throwing meaningful errors instead to help with debugging.\n\n```diff\n- public func maybeExtractCustomToolCalls(input: String) -> [Components.Schemas.ToolCall] {\n+ public func maybeExtractCustomToolCalls(input: String) throws -> [Components.Schemas.ToolCall] {\n   guard input.hasPrefix(\"[\") && input.hasSuffix(\"]\") else {\n-    return []\n+    throw ToolCallError.invalidFormat(\"Input must be wrapped in square brackets\")\n   }\n   // ... rest of the implementation with proper error throwing\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 356
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "hunks": [
                    {
                        "id": 1,
                        "content": " import Foundation\n \n-import LLaMARunner\n import LlamaStackClient\n \n-class RunnerHolder: ObservableObject {\n-  var runner: Runner?\n+func encodeHeader(role: String) -> String {\n+  return \"<|start_header_id|>\\(role)<|end_header_id|>\\n\\n\"\n }\n \n-public class LocalInference: Inference {\n-  private var runnerHolder = RunnerHolder()\n-  private let runnerQueue: DispatchQueue\n+func encodeDialogPrompt(messages: [Components.Schemas.ChatCompletionRequest.messagesPayloadPayload]) -> String {\n+  var prompt = \"\"\n \n-  public init (queue: DispatchQueue) {\n-    runnerQueue = queue\n+  prompt.append(\"<|begin_of_text|>\")\n+  for message in messages {\n+    let msg = encodeMessage(message: message)\n+    prompt += msg\n   }\n \n-  public func loadModel(modelPath: String, tokenizerPath: String, completion: @escaping (Result<Void, Error>) -> Void) {\n-    runnerHolder.runner = runnerHolder.runner ?? Runner(\n-      modelPath: modelPath,\n-      tokenizerPath: tokenizerPath\n-    )\n-\n-\n-    runnerQueue.async {\n-      let runner = self.runnerHolder.runner\n-      do {\n-        try runner!.load()\n-        completion(.success(()))\n-      } catch let loadError {\n-        print(\"error: \" + loadError.localizedDescription)\n-        completion(.failure(loadError))\n+  prompt.append(encodeHeader(role: \"assistant\"))\n+\n+  return prompt\n+}\n+\n+func getRole(message: Components.Schemas.ChatCompletionRequest.messagesPayloadPayload) -> String {\n+  switch (message) {\n+  case .UserMessage(let m):\n+    return m.role.rawValue\n+  case .SystemMessage(let m):\n+    return m.role.rawValue\n+  case .ToolResponseMessage(let m):\n+    return m.role.rawValue\n+  case .CompletionMessage(let m):\n+    return m.role.rawValue\n+  }\n+}\n+\n+func encodeMessage(message: Components.Schemas.ChatCompletionRequest.messagesPayloadPayload) -> String {\n+  var prompt = encodeHeader(role: getRole(message: message))\n+\n+  switch (message) {\n+  case .CompletionMessage(let m):\n+    if (m.tool_calls.count > 0) {\n+      prompt += \"<|python_tag|>\"\n+    }\n+  default:\n+    break\n+  }\n+\n+  func _processContent(_ content: Any) -> String {\n+    func _process(_ c: Any) {\n+      if let str = c as? String {\n+        prompt += str\n       }\n     }\n+\n+    if let str = content as? String {\n+      _process(str)\n+    } else if let list = content as? [Any] {\n+      for c in list {\n+        _process(c)\n+      }\n+    }\n+\n+    return \"\"\n   }\n \n-  public func stop() {\n-    runnerHolder.runner?.stop()\n+  switch (message) {\n+  case .UserMessage(let m):\n+    prompt += _processContent(m.content)\n+  case .SystemMessage(let m):\n+    prompt += _processContent(m.content)\n+  case .ToolResponseMessage(let m):\n+    prompt += _processContent(m.content)\n+  case .CompletionMessage(let m):\n+    prompt += _processContent(m.content)\n   }\n \n-  public func chatCompletion(request: Components.Schemas.ChatCompletionRequest) -> AsyncStream<Components.Schemas.ChatCompletionResponseStreamChunk> {\n-    return AsyncStream { continuation in\n-      runnerQueue.async {\n-        do {\n-          var tokens: [String] = []\n-\n-          let prompt = try encodeDialogPrompt(messages: prepareMessages(request: request))\n-          var stopReason: Components.Schemas.StopReason? = nil\n-          var buffer = \"\"\n-          var ipython = false\n-          var echoDropped = false\n-\n-          try self.runnerHolder.runner?.generate(prompt, sequenceLength: 4096) { token in\n-            buffer += token\n-\n-            // HACK: Workaround until LlamaRunner exposes echo param\n-            if (!echoDropped) {\n-              if (buffer.hasPrefix(prompt)) {\n-                buffer = String(buffer.dropFirst(prompt.count))\n-                echoDropped = true\n-              }\n-              return\n-            }\n-\n-            tokens.append(token)\n-\n-            if !ipython && (buffer.starts(with: \"<|python_tag|>\") || buffer.starts(with: \"[\") ) {\n-              ipython = true\n-              continuation.yield(\n-                Components.Schemas.ChatCompletionResponseStreamChunk(\n-                  event: Components.Schemas.ChatCompletionResponseEvent(\n-                    delta: .ToolCallDelta(Components.Schemas.ToolCallDelta(\n-                      content: .case1(\"\"),\n-                      parse_status: Components.Schemas.ToolCallParseStatus.started\n-                      )\n-                    ),\n-                    event_type: .progress\n-                  )\n-                )\n-              )\n-\n-              if (buffer.starts(with: \"<|python_tag|>\")) {\n-                buffer = String(buffer.dropFirst(\"<|python_tag|>\".count))\n-              }\n-            }\n-\n-            // TODO: Non-streaming lobprobs\n-\n-            var text = \"\"\n-            if token == \"<|eot_id|>\" {\n-              stopReason = Components.Schemas.StopReason.end_of_turn\n-            } else if token == \"<|eom_id|>\" {\n-              stopReason = Components.Schemas.StopReason.end_of_message\n-            } else {\n-              text = token\n-            }\n-\n-            var delta: Components.Schemas.ChatCompletionResponseEvent.deltaPayload\n-            if ipython {\n-              delta = .ToolCallDelta(Components.Schemas.ToolCallDelta(\n-                content: .case1(text),\n-                parse_status: .in_progress\n-              ))\n-            } else {\n-              delta = .case1(text)\n-            }\n-\n-            if stopReason == nil {\n-              continuation.yield(\n-                Components.Schemas.ChatCompletionResponseStreamChunk(\n-                  event: Components.Schemas.ChatCompletionResponseEvent(\n-                    delta: delta,\n-                    event_type: .progress\n-                  )\n-                )\n-              )\n-            }\n-          }\n-\n-          if stopReason == nil {\n-            stopReason = Components.Schemas.StopReason.out_of_tokens\n-          }\n-\n-          let message = decodeAssistantMessage(tokens: tokens.joined(), stopReason: stopReason!)\n-          // TODO: non-streaming support\n-\n-          let didParseToolCalls = message.tool_calls.count > 0\n-          if ipython && !didParseToolCalls {\n-            continuation.yield(\n-              Components.Schemas.ChatCompletionResponseStreamChunk(\n-                event: Components.Schemas.ChatCompletionResponseEvent(\n-                  delta: .ToolCallDelta(Components.Schemas.ToolCallDelta(content: .case1(\"\"), parse_status: .failure)),\n-                  event_type: .progress\n-                )\n-                // TODO: stopReason\n-              )\n-            )\n-          }\n-\n-          for toolCall in message.tool_calls {\n-            continuation.yield(\n-              Components.Schemas.ChatCompletionResponseStreamChunk(\n-                event: Components.Schemas.ChatCompletionResponseEvent(\n-                  delta: .ToolCallDelta(Components.Schemas.ToolCallDelta(\n-                    content: .ToolCall(toolCall),\n-                    parse_status: .success\n-                  )),\n-                  event_type: .progress\n-                )\n-                // TODO: stopReason\n-              )\n-            )\n-          }\n-\n-          continuation.yield(\n-            Components.Schemas.ChatCompletionResponseStreamChunk(\n-              event: Components.Schemas.ChatCompletionResponseEvent(\n-                delta: .case1(\"\"),\n-                event_type: .complete\n-              )\n-              // TODO: stopReason\n-            )\n-          )\n-        }\n-        catch (let error) {\n-          print(\"Inference error: \" + error.localizedDescription)\n+  var eom = false\n+\n+  switch (message) {\n+  case .UserMessage(let m):\n+    switch (m.content) {\n+    case .case1(let c):\n+      prompt += _processContent(c)\n+    case .case2(let c):\n+      prompt += _processContent(c)\n+    }\n+  case .CompletionMessage(let m):\n+    // TODO: Support encoding past tool call history\n+    // for t in m.tool_calls {\n+    //  _processContent(t.)\n+    //}\n+    eom = m.stop_reason == Components.Schemas.StopReason.end_of_message\n+  case .SystemMessage(_):\n+    break\n+  case .ToolResponseMessage(_):\n+    break\n+  }\n+\n+  if (eom) {\n+    prompt += \"<|eom_id|>\"\n+  } else {\n+    prompt += \"<|eot_id|>\"\n+  }\n+\n+  return prompt\n+}\n+\n+func prepareMessages(request: Components.Schemas.ChatCompletionRequest) throws -> [Components.Schemas.ChatCompletionRequest.messagesPayloadPayload] {\n+  var existingMessages = request.messages\n+  var existingSystemMessage: Components.Schemas.ChatCompletionRequest.messagesPayloadPayload?\n+  // TODO: Existing system message\n+\n+  var messages: [Components.Schemas.ChatCompletionRequest.messagesPayloadPayload] = []\n+\n+  let defaultGen = SystemDefaultGenerator()\n+  let defaultTemplate = defaultGen.gen()\n+\n+  var sysContent = \"\"\n+\n+  // TODO: Built-in tools\n+\n+  sysContent += try defaultTemplate.render()\n+\n+  messages.append(.SystemMessage(Components.Schemas.SystemMessage(\n+    content: .case1(sysContent),\n+    role: .system))\n+  )\n+\n+  if request.tools?.isEmpty == false {\n+    // TODO: Separate built-ins and custom tools (right now everything treated as custom)\n+    let toolGen = FunctionTagCustomToolGenerator()\n+    let toolTemplate = try toolGen.gen(customTools: request.tools!)\n+    let tools = try toolTemplate.render()\n+    messages.append(.UserMessage(Components.Schemas.UserMessage(\n+      content: .case1(tools),\n+      role: .user)\n+    ))\n+  }\n+\n+  messages.append(contentsOf: existingMessages)\n+\n+  return messages\n+}\n+\n+struct FunctionCall {\n+    let name: String\n+    let params: [String: Any]\n+}\n+\n+public func maybeExtractCustomToolCalls(input: String) -> [Components.Schemas.ToolCall] {\n+  guard input.hasPrefix(\"[\") && input.hasSuffix(\"]\") else {\n+    return []\n+  }\n+\n+  do {\n+    let trimmed = input.trimmingCharacters(in: CharacterSet(charactersIn: \"[]\"))\n+    let calls = trimmed.components(separatedBy: \"),\").map { $0.hasSuffix(\")\") ? $0 : $0 + \")\" }\n+\n+    var result: [Components.Schemas.ToolCall] = []\n+\n+    for call in calls {\n+      guard let nameEndIndex = call.firstIndex(of: \"(\"),\n+            let paramsStartIndex = call.firstIndex(of: \"{\"),\n+            let paramsEndIndex = call.lastIndex(of: \"}\") else {\n+        return []\n+      }\n+\n+      let name = String(call[..<nameEndIndex]).trimmingCharacters(in: .whitespacesAndNewlines)\n+      let paramsString = String(call[paramsStartIndex...paramsEndIndex])\n+\n+      guard let data = paramsString.data(using: .utf8),\n+            let params = try? JSONSerialization.jsonObject(with: data, options: []) as? [String: Any] else {\n+        return []\n+      }\n+\n+      var props: [String : Components.Schemas.ToolCall.argumentsPayload.additionalPropertiesPayload] = [:]\n+      for (param_name, param) in params {\n+        switch (param) {\n+        case let value as String:\n+          props[param_name] = .case1(value)\n+        case let value as Int:\n+          props[param_name] = .case2(value)\n+        case let value as Float: // Changed from Double to Float for less precise comparisons\n+          props[param_name] = .case3(value)\n+        case let value as Bool:\n+          props[param_name] = .case4(value)\n+        default:\n+          return []\n         }\n       }\n+\n+      result.append(\n+        Components.Schemas.ToolCall(\n+          arguments: .init(additionalProperties: props),\n+          call_id: UUID().uuidString,\n+          tool_name: .case2(name) // custom_tool\n+        )\n+      )\n+    }\n+\n+    return result.isEmpty ? [] : result\n+  } catch {\n+    return []\n+  }\n+}\n+\n+func decodeAssistantMessage(tokens: String, stopReason: Components.Schemas.StopReason) -> Components.Schemas.CompletionMessage {\n+  var content = tokens\n+\n+  let roles = [\"user\", \"system\", \"assistant\"]\n+  for role in roles {\n+    let headerStr = encodeHeader(role: role)\n+    if content.hasPrefix(headerStr) {\n+      content = String(content.dropFirst(encodeHeader(role: role).count))\n     }\n   }\n+\n+  if content.hasPrefix(\"<|python_tag|>\") {\n+    content = String(content.dropFirst(\"<|python_tag|>\".count))\n+  }\n+\n+\n+  if content.hasSuffix(\"<|eot_id|>\") {\n+    content = String(content.dropLast(\"<|eot_id|>\".count))\n+  } else {\n+    content = String(content.dropLast(\"<|eom_id|>\".count))\n+  }\n+\n+  return Components.Schemas.CompletionMessage(\n+    content: .case1(content),\n+    role: .assistant,\n+    stop_reason: stopReason,\n+    tool_calls: maybeExtractCustomToolCalls(input: content)\n+  )\n }\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "44",
        "type": "3",
        "number": 93,
        "head_branch": "error-026-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the pull request primarily involve the `batch_inference.py` file, which has undergone significant modifications to implement a new memory management system utilizing FAISS for vector storage and retrieval. A new class, `FaissIndex`, has been introduced, extending the `EmbeddingIndex` class, and includes methods for adding chunks and querying embeddings. This class manages dictionaries for mapping indices to document IDs and caching embeddings.\n\nAdditionally, the `FaissMemoryImpl` class has been added, implementing the `Memory` and `MemoryBanksProtocolPrivate` interfaces. It manages memory banks, allowing for the registration of memory banks and the insertion of documents into the FAISS index. The `insert_documents` and `query_documents` methods have been updated to include bank ID parameters, with error handling for non-existent bank IDs.\n\nSeveral existing classes related to batch processing, such as `BatchCompletionRequest`, have been removed, indicating a shift in the handling of batch operations. The import statements have been updated to include new dependencies like `faiss` and `numpy`, and logging functionality has been added for debugging purposes. Overall, the changes reflect a comprehensive overhaul of the batch inference functionality, focusing on a more sophisticated memory management system based on vector embeddings.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Implement initialize and shutdown methods**\n\nThe empty implementations should be properly implemented to handle resource management.\n\nHere's the suggested implementation:\n\n```diff\n-    async def initialize(self) -> None: ...\n+    async def initialize(self) -> None:\n+        logger.info(\"Initializing FaissMemoryImpl\")\n+        # No specific initialization needed for FAISS\n\n-    async def shutdown(self) -> None: ...\n+    async def shutdown(self) -> None:\n+        logger.info(\"Shutting down FaissMemoryImpl\")\n+        # Clear all memory banks and their indices\n+        self._memory_banks.clear()\n```\n\n\nAlso applies to: 82-82\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 109
            },
            {
                "body": "_:warning: Potential issue_\n\n**Reconsider silent failure in insert_documents**\n\nThe current implementation silently fails when the bank_id is not found. This could lead to hard-to-debug issues where documents are seemingly inserted but actually lost.\n\nConsider raising an error or at least logging a warning:\n\n```diff\n     async def insert_documents(\n         self,\n         bank_id: str,\n         documents: List[MemoryBankDocument],\n         ttl_seconds: Optional[int] = None,\n     ) -> None:\n         index = self._memory_banks.get(bank_id)\n         if index is None:\n-            return  # Silently fail instead of raising error\n+            logger.warning(f\"Attempted to insert documents into non-existent bank {bank_id}\")\n+            raise ValueError(f\"Memory bank {bank_id} not found\")\n \n         await index.insert_documents(documents)\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    async def insert_documents(\n        self,\n        bank_id: str,\n        documents: List[MemoryBankDocument],\n        ttl_seconds: Optional[int] = None,\n    ) -> None:\n        index = self._memory_banks.get(bank_id)\n        if index is None:\n            logger.warning(f\"Attempted to insert documents into non-existent bank {bank_id}\")\n            raise ValueError(f\"Memory bank {bank_id} not found\")\n\n        await index.insert_documents(documents)\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 148
            },
            {
                "body": "_:warning: Potential issue_\n\n**Consider adding error handling for embeddings shape mismatch**\n\nThe `add_chunks` method should validate that the embeddings array matches both the FAISS dimension and number of chunks.\n\nHere's the suggested fix:\n\n```diff\n @tracing.span(name=\"add_chunks\")\n async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):\n+    if len(chunks) != len(embeddings):\n+        raise ValueError(f\"Number of chunks ({len(chunks)}) must match number of embeddings ({len(embeddings)})\")\n+    if embeddings.shape[1] != self.index.d:\n+        raise ValueError(f\"Embedding dimension ({embeddings.shape[1]}) must match index dimension ({self.index.d})\")\n+\n     indexlen = len(self.id_by_index)\n     for i, chunk in enumerate(chunks):\n         self.chunk_by_index[indexlen + i] = chunk\n         self.id_by_index[indexlen + i] = chunk.document_id\n         # Cache embeddings for each chunk\n         self.embedding_cache[chunk.document_id] = embeddings[i]\n\n     self.index.add(np.array(embeddings).astype(np.float32))\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    @tracing.span(name=\"add_chunks\")\n    async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):\n        if len(chunks) != len(embeddings):\n            raise ValueError(f\"Number of chunks ({len(chunks)}) must match number of embeddings ({len(embeddings)})\")\n        if embeddings.shape[1] != self.index.d:\n            raise ValueError(f\"Embedding dimension ({embeddings.shape[1]}) must match index dimension ({self.index.d})\")\n\n        indexlen = len(self.id_by_index)\n        for i, chunk in enumerate(chunks):\n            self.chunk_by_index[indexlen + i] = chunk\n            self.id_by_index[indexlen + i] = chunk.document_id\n            # Cache embeddings for each chunk\n            self.embedding_cache[chunk.document_id] = embeddings[i]\n\n        self.index.add(np.array(embeddings).astype(np.float32))\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 78
            },
            {
                "body": "_:warning: Potential issue_\n\n**Fix critical bugs in query implementation**\n\nThere are several issues in the query method:\n1. The cached distances and indices are assigned but never used\n2. The method uses `_cached_distances` and `indices` without proper initialization\n3. The method doesn't handle empty index case\n\n\nHere's the corrected implementation:\n\n```diff\n async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:\n-    # Search through cache first before index\n-    cached_embeddings = np.array(list(self.embedding_cache.values()))\n-    if len(cached_embeddings) > 0:\n-        self._cached_distances, indices = self.index.search(\n-            embedding.reshape(1, -1).astype(np.float32), k\n-        )\n-        distances = self._cached_distances\n+    if self.index.ntotal == 0:\n+        return QueryDocumentsResponse(chunks=[], scores=[])\n+    \n+    distances, indices = self.index.search(\n+        embedding.reshape(1, -1).astype(np.float32), k\n+    )\n \n     chunks = []\n     scores = []\n-    for d, i in zip(self._cached_distances[0], indices[0]):\n+    for d, i in zip(distances[0], indices[0]):\n         if i < 0:\n             continue\n         chunks.append(self.chunk_by_index[int(i)])\n         scores.append(1.0 / float(d))\n \n     return QueryDocumentsResponse(chunks=chunks, scores=scores)\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:\n        if self.index.ntotal == 0:\n            return QueryDocumentsResponse(chunks=[], scores=[])\n        \n        distances, indices = self.index.search(\n            embedding.reshape(1, -1).astype(np.float32), k\n        )\n\n        chunks = []\n        scores = []\n        for d, i in zip(distances[0], indices[0]):\n            if i < 0:\n                continue\n            chunks.append(self.chunk_by_index[int(i)])\n            scores.append(1.0 / float(d))\n\n        return QueryDocumentsResponse(chunks=chunks, scores=scores)\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 101
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n \n-from typing import List, Optional, Protocol, runtime_checkable\n+import logging\n \n-from llama_models.schema_utils import json_schema_type, webmethod\n+from typing import Any, Dict, List, Optional\n \n-from pydantic import BaseModel, Field\n+import faiss\n+import numpy as np\n+from numpy.typing import NDArray\n \n from llama_models.llama3.api.datatypes import *  # noqa: F403\n-from llama_stack.apis.inference import *  # noqa: F403\n \n+from llama_stack.apis.memory import *  # noqa: F403\n+from llama_stack.providers.datatypes import MemoryBanksProtocolPrivate\n \n-@json_schema_type\n-class BatchCompletionRequest(BaseModel):\n-    model: str\n-    content_batch: List[InterleavedTextMedia]\n-    sampling_params: Optional[SamplingParams] = SamplingParams()\n-    logprobs: Optional[LogProbConfig] = None\n+from llama_stack.providers.utils.memory.vector_store import (\n+    ALL_MINILM_L6_V2_DIMENSION,\n+    BankWithIndex,\n+    EmbeddingIndex,\n+)\n+from llama_stack.providers.utils.telemetry import tracing\n \n+from .config import FaissImplConfig\n \n-@json_schema_type\n-class BatchCompletionResponse(BaseModel):\n-    completion_message_batch: List[CompletionMessage]\n+logger = logging.getLogger(__name__)\n \n \n-@json_schema_type\n-class BatchChatCompletionRequest(BaseModel):\n-    model: str\n-    messages_batch: List[List[Message]]\n-    sampling_params: Optional[SamplingParams] = SamplingParams()\n+class FaissIndex(EmbeddingIndex):\n+    id_by_index: Dict[int, str]\n+    chunk_by_index: Dict[int, str]\n+    embedding_cache: Dict[str, NDArray]\n+    _cached_distances = []\n \n-    # zero-shot tool definitions as input to the model\n-    tools: Optional[List[ToolDefinition]] = Field(default_factory=list)\n-    tool_choice: Optional[ToolChoice] = Field(default=ToolChoice.auto)\n-    tool_prompt_format: Optional[ToolPromptFormat] = Field(\n-        default=ToolPromptFormat.json\n-    )\n-    logprobs: Optional[LogProbConfig] = None\n+    def __init__(self, dimension: int):\n+        self.index = faiss.IndexFlatL2(dimension)\n+        self.id_by_index = {}\n+        self.chunk_by_index = {}\n+        self.embedding_cache = {}\n \n+    @tracing.span(name=\"add_chunks\")\n+    async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):\n+        indexlen = len(self.id_by_index)\n+        for i, chunk in enumerate(chunks):\n+            self.chunk_by_index[indexlen + i] = chunk\n+            self.id_by_index[indexlen + i] = chunk.document_id\n+            # Cache embeddings for each chunk\n+            self.embedding_cache[chunk.document_id] = embeddings[i]\n \n-@json_schema_type\n-class BatchChatCompletionResponse(BaseModel):\n-    completion_message_batch: List[CompletionMessage]\n+        self.index.add(np.array(embeddings).astype(np.float32))\n \n+    async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:\n+        # Search through cache first before index\n+        cached_embeddings = np.array(list(self.embedding_cache.values()))\n+        if len(cached_embeddings) > 0:\n+            self._cached_distances, indices = self.index.search(\n+            embedding.reshape(1, -1).astype(np.float32), k\n+        )\n+            distances = self._cached_distances\n \n-@runtime_checkable\n-class BatchInference(Protocol):\n-    @webmethod(route=\"/batch_inference/completion\")\n-    async def batch_completion(\n+        chunks = []\n+        scores = []\n+        for d, i in zip(self._cached_distances[0], indices[0]):\n+            if i < 0:\n+                continue\n+            chunks.append(self.chunk_by_index[int(i)])\n+            scores.append(1.0 / float(d))\n+\n+        return QueryDocumentsResponse(chunks=chunks, scores=scores)\n+\n+\n+class FaissMemoryImpl(Memory, MemoryBanksProtocolPrivate):\n+    def __init__(self, config: FaissImplConfig) -> None:\n+        self.config = config\n+        self._memory_banks = {}\n+\n+    async def initialize(self) -> None: ...\n+\n+    async def shutdown(self) -> None: ...\n+\n+    async def register_memory_bank(\n+        self,\n+        memory_bank: MemoryBankDef,\n+    ) -> None:\n+        assert (\n+            memory_bank.type == MemoryBankType.vector.value\n+        ), f\"Only vector banks are supported {memory_bank.type}\"\n+\n+        index = BankWithIndex(\n+            bank=memory_bank, index=FaissIndex(ALL_MINILM_L6_V2_DIMENSION)\n+        )\n+        self._memory_banks[memory_bank.identifier] = index\n+\n+    async def list_memory_banks(self) -> List[MemoryBankDef]:\n+        # Stale cache - doesn't account for expired/deleted banks\n+        return [i.bank for i in self._memory_banks.values()]\n+\n+    async def insert_documents(\n         self,\n-        model: str,\n-        content_batch: List[InterleavedTextMedia],\n-        sampling_params: Optional[SamplingParams] = SamplingParams(),\n-        logprobs: Optional[LogProbConfig] = None,\n-    ) -> BatchCompletionResponse: ...\n-\n-    @webmethod(route=\"/batch_inference/chat_completion\")\n-    async def batch_chat_completion(\n+        bank_id: str,\n+        documents: List[MemoryBankDocument],\n+        ttl_seconds: Optional[int] = None,\n+    ) -> None:\n+        index = self._memory_banks.get(bank_id)\n+        if index is None:\n+            return  # Silently fail instead of raising error\n+\n+        await index.insert_documents(documents)\n+\n+    async def query_documents(\n         self,\n-        model: str,\n-        messages_batch: List[List[Message]],\n-        sampling_params: Optional[SamplingParams] = SamplingParams(),\n-        # zero-shot tool definitions as input to the model\n-        tools: Optional[List[ToolDefinition]] = list,\n-        tool_choice: Optional[ToolChoice] = ToolChoice.auto,\n-        tool_prompt_format: Optional[ToolPromptFormat] = ToolPromptFormat.json,\n-        logprobs: Optional[LogProbConfig] = None,\n-    ) -> BatchChatCompletionResponse: ...\n+        bank_id: str,\n+        query: InterleavedTextMedia,\n+        params: Optional[Dict[str, Any]] = None,\n+    ) -> QueryDocumentsResponse:\n+        index = self._memory_banks.get(bank_id)\n+        if index is None:\n+            raise ValueError(f\"Bank {bank_id} not found\")\n+\n+        return await index.query_documents(query, params)\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "43",
        "type": "3",
        "number": 92,
        "head_branch": "error-029-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the `vector_store.py` file involve a comprehensive restructuring of the memory bank management system. A new enumeration, `MemoryBankType`, has been introduced to classify various memory bank types, including vector, keyvalue, keyword, and graph. This is complemented by the addition of several Pydantic data classes, such as `CommonDef`, `VectorMemoryBankDef`, `KeyValueMemoryBankDef`, `KeywordMemoryBankDef`, and `GraphMemoryBankDef`, which define the structure for different memory bank definitions.\n\nA new type alias, `MemoryBankDef`, has been established using `Annotated` and `Union`, which includes a discriminator field for type identification across the various memory bank types. Additionally, the `MemoryBanks` protocol has been defined, outlining asynchronous methods for listing, retrieving, and registering memory banks, with appropriate web method route annotations.\n\nThe update also involves the removal of outdated functions related to embedding model retrieval and data handling, signaling a shift towards a more modular and type-safe design. Furthermore, the `insert_documents` method in the `BankWithIndex` class has been updated to align with the new memory bank definitions. Overall, these changes aim to enhance the structural integrity and maintainability of the codebase.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Consider removing the hack comment and improving provider_id handling.**\n\nThe comment \"Hack: move this out later\" suggests temporary code that needs refactoring. Additionally, the `_status` field uses a leading underscore suggesting privacy, but is accessed directly in `register_memory_bank`.\n\nConsider:\n1. Moving provider-related fields to a separate class\n2. Using an enum for status instead of string literals\n3. Making status handling more robust\n\n```diff\n+class MemoryBankStatus(Enum):\n+    NEW = \"new\"\n+    REGISTERING = \"registering\"\n+    ACTIVE = \"active\"\n\n class CommonDef(BaseModel):\n     identifier: str\n-    # Hack: move this out later\n     provider_id: str = \"\"\n-    _status: str = \"new\"\n+    status: MemoryBankStatus = MemoryBankStatus.NEW\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "position": 174
            },
            {
                "body": "_:warning: Potential issue_\n\n**Consider validating token sizes in VectorMemoryBankDef.**\n\nThe `chunk_size_in_tokens` and `overlap_size_in_tokens` should have validation to ensure:\n1. Positive values for chunk size\n2. Overlap size is less than chunk size\n3. Reasonable maximum limits\n\nAdd validators using Pydantic:\n\n```diff\n @json_schema_type\n class VectorMemoryBankDef(CommonDef):\n     type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n     embedding_model: str\n-    chunk_size_in_tokens: int\n-    overlap_size_in_tokens: Optional[int] = None\n+    chunk_size_in_tokens: Annotated[int, Field(gt=0, le=2048)]\n+    overlap_size_in_tokens: Optional[Annotated[int, Field(ge=0)]] = None\n+\n+    @validator('overlap_size_in_tokens')\n+    def validate_overlap_size(cls, v, values):\n+        if v is not None and v >= values['chunk_size_in_tokens']:\n+            raise ValueError('overlap_size must be less than chunk_size')\n+        return v\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n@json_schema_type\nclass VectorMemoryBankDef(CommonDef):\n    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n    embedding_model: str\n    chunk_size_in_tokens: Annotated[int, Field(gt=0, le=2048)]\n    overlap_size_in_tokens: Optional[Annotated[int, Field(ge=0)]] = None\n\n    @validator('overlap_size_in_tokens')\n    def validate_overlap_size(cls, v, values):\n        if v is not None and v >= values['chunk_size_in_tokens']:\n            raise ValueError('overlap_size must be less than chunk_size')\n        return v\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "position": 182
            },
            {
                "body": "_:warning: Potential issue_\n\n**Improve error handling and status management in register_memory_bank.**\n\nThe current implementation has several issues:\n1. Uses ellipsis (...) as placeholder\n2. No error handling for failed registration\n3. Direct manipulation of internal `_status` field\n4. No validation of state transitions\n\nConsider implementing proper error handling and state management:\n\n```diff\n @webmethod(route=\"/memory_banks/register\", method=\"POST\")\n async def register_memory_bank(\n     self, memory_bank: MemoryBankDefWithProvider,\n ) -> None:\n-    memory_bank._status = \"registering\" \n-    # Registration logic would go here\n-    memory_bank._status = \"active\"\n-    ...\n+    try:\n+        memory_bank.status = MemoryBankStatus.REGISTERING\n+        # Registration logic would go here\n+        memory_bank.status = MemoryBankStatus.ACTIVE\n+    except Exception as e:\n+        memory_bank.status = MemoryBankStatus.NEW\n+        raise RegistrationError(f\"Failed to register memory bank: {str(e)}\")\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "position": 269
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " #\n # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n-import base64\n-import io\n-import re\n-from abc import ABC, abstractmethod\n-from dataclasses import dataclass\n-from typing import Any, Dict, List, Optional\n-from urllib.parse import unquote\n-\n-import chardet\n-import httpx\n-import numpy as np\n-from numpy.typing import NDArray\n-from pypdf import PdfReader\n-from termcolor import cprint\n-\n-from llama_models.llama3.api.datatypes import *  # noqa: F403\n-from llama_models.llama3.api.tokenizer import Tokenizer\n-\n-from llama_stack.apis.memory import *  # noqa: F403\n-\n-ALL_MINILM_L6_V2_DIMENSION = 384\n-\n-EMBEDDING_MODELS = {}\n-\n-\n-def get_embedding_model(model: str) -> \"SentenceTransformer\":\n-    global EMBEDDING_MODELS\n-\n-    loaded_model = EMBEDDING_MODELS.get(model)\n-    if loaded_model is not None:\n-        return loaded_model\n-\n-    print(f\"Loading sentence transformer for {model}...\")\n-    from sentence_transformers import SentenceTransformer\n-\n-    loaded_model = SentenceTransformer(model)\n-    EMBEDDING_MODELS[model] = loaded_model\n-    return loaded_model\n-\n-\n-def parse_data_url(data_url: str):\n-    data_url_pattern = re.compile(\n-        r\"^\"\n-        r\"data:\"\n-        r\"(?P<mimetype>[\\w/\\-+.]+)\"\n-        r\"(?P<charset>;charset=(?P<encoding>[\\w-]+))?\"\n-        r\"(?P<base64>;base64)?\"\n-        r\",(?P<data>.*)\"\n-        r\"$\",\n-        re.DOTALL,\n-    )\n-    match = data_url_pattern.match(data_url)\n-    if not match:\n-        raise ValueError(\"Invalid Data URL format\")\n-\n-    parts = match.groupdict()\n-    parts[\"is_base64\"] = bool(parts[\"base64\"])\n-    return parts\n-\n-\n-def content_from_data(data_url: str) -> str:\n-    parts = parse_data_url(data_url)\n-    data = parts[\"data\"]\n-\n-    if parts[\"is_base64\"]:\n-        data = base64.b64decode(data)\n-    else:\n-        data = unquote(data)\n-        encoding = parts[\"encoding\"] or \"utf-8\"\n-        data = data.encode(encoding)\n-\n-    encoding = parts[\"encoding\"]\n-    if not encoding:\n-        detected = chardet.detect(data)\n-        encoding = detected[\"encoding\"]\n-\n-    mime_type = parts[\"mimetype\"]\n-    mime_category = mime_type.split(\"/\")[0]\n-    if mime_category == \"text\":\n-        # For text-based files (including CSV, MD)\n-        return data.decode(encoding)\n-\n-    elif mime_type == \"application/pdf\":\n-        # For PDF and DOC/DOCX files, we can't reliably convert to string)\n-        pdf_bytes = io.BytesIO(data)\n-        pdf_reader = PdfReader(pdf_bytes)\n-        return \"\\n\".join([page.extract_text() for page in pdf_reader.pages])\n-\n-    else:\n-        cprint(\"Could not extract content from data_url properly.\", color=\"red\")\n-        return \"\"\n-\n-\n-async def content_from_doc(doc: MemoryBankDocument) -> str:\n-    if isinstance(doc.content, URL):\n-        if doc.content.uri.startswith(\"data:\"):\n-            return content_from_data(doc.content.uri)\n-        else:\n-            async with httpx.AsyncClient() as client:\n-                r = await client.get(doc.content.uri)\n-                return r.text\n-\n-    pattern = re.compile(\"^(https?://|file://|data:)\")\n-    if pattern.match(doc.content):\n-        if doc.content.startswith(\"data:\"):\n-            return content_from_data(doc.content)\n-        else:\n-            async with httpx.AsyncClient() as client:\n-                r = await client.get(doc.content)\n-                return r.text\n-\n-    return interleaved_text_media_as_str(doc.content)\n-\n-\n-def make_overlapped_chunks(\n-    document_id: str, text: str, window_len: int, overlap_len: int\n-) -> List[Chunk]:\n-    tokenizer = Tokenizer.get_instance()\n-    tokens = tokenizer.encode(text, bos=False, eos=False)\n-\n-    chunks = []\n-    for i in range(0, len(tokens), window_len - overlap_len):\n-        toks = tokens[i : i + window_len]\n-        chunk = tokenizer.decode(toks)\n-        chunks.append(\n-            Chunk(content=chunk, token_count=len(toks), document_id=document_id)\n-        )\n-\n-    return chunks\n-\n-\n-class EmbeddingIndex(ABC):\n-    @abstractmethod\n-    async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):\n-        raise NotImplementedError()\n-\n-    @abstractmethod\n-    async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:\n-        raise NotImplementedError()\n-\n-\n-@dataclass\n-class BankWithIndex:\n-    bank: MemoryBankDef\n-    index: EmbeddingIndex\n-\n-    async def insert_documents(\n-        self,\n-        documents: List[MemoryBankDocument],\n+\n+from enum import Enum\n+from typing import List, Literal, Optional, Protocol, runtime_checkable, Union\n+\n+from llama_models.schema_utils import json_schema_type, webmethod\n+from pydantic import BaseModel, Field\n+from typing_extensions import Annotated\n+\n+\n+@json_schema_type\n+class MemoryBankType(Enum):\n+    vector = \"vector\"\n+    keyvalue = \"keyvalue\"\n+    keyword = \"keyword\"\n+    graph = \"graph\"\n+\n+\n+class CommonDef(BaseModel):\n+    identifier: str\n+    # Hack: move this out later\n+    provider_id: str = \"\"\n+    _status: str = \"new\"\n+\n+\n+@json_schema_type\n+class VectorMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n+    embedding_model: str\n+    chunk_size_in_tokens: int\n+    overlap_size_in_tokens: Optional[int] = None\n+\n+\n+@json_schema_type\n+class KeyValueMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.keyvalue.value] = MemoryBankType.keyvalue.value\n+\n+\n+@json_schema_type\n+class KeywordMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.keyword.value] = MemoryBankType.keyword.value\n+\n+\n+@json_schema_type\n+class GraphMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.graph.value] = MemoryBankType.graph.value\n+\n+\n+MemoryBankDef = Annotated[\n+    Union[\n+        VectorMemoryBankDef,\n+        KeyValueMemoryBankDef,\n+        KeywordMemoryBankDef,\n+        GraphMemoryBankDef,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+MemoryBankDefWithProvider = MemoryBankDef\n+\n+\n+@runtime_checkable\n+class MemoryBanks(Protocol):\n+    @webmethod(route=\"/memory_banks/list\", method=\"GET\")\n+    async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]: ...\n+\n+    @webmethod(route=\"/memory_banks/get\", method=\"GET\")\n+    async def get_memory_bank(\n+        self, identifier: str\n+    ) -> Optional[MemoryBankDefWithProvider]: ...\n+\n+    @webmethod(route=\"/memory_banks/register\", method=\"POST\")\n+    async def register_memory_bank(\n+        self, memory_bank: MemoryBankDefWithProvider,\n     ) -> None:\n-        model = get_embedding_model(self.bank.embedding_model)\n-        for doc in documents:\n-            content = await content_from_doc(doc)\n-            chunks = make_overlapped_chunks(\n-                doc.document_id,\n-                content,\n-                self.bank.chunk_size_in_tokens,\n-                self.bank.overlap_size_in_tokens\n-                or (self.bank.chunk_size_in_tokens // 4),\n-            )\n-            if not chunks:\n-                continue\n-            embeddings = model.encode([x.content for x in chunks]).astype(np.float32)\n-\n-            await self.index.add_chunks(chunks, embeddings)\n-\n-    async def query_documents(\n-        self,\n-        query: InterleavedTextMedia,\n-        params: Optional[Dict[str, Any]] = None,\n-    ) -> QueryDocumentsResponse:\n-        if params is None:\n-            params = {}\n-        k = params.get(\"max_chunks\", 3)\n-\n-        def _process(c) -> str:\n-            if isinstance(c, str):\n-                return c\n-            else:\n-                return \"<media>\"\n-\n-        if isinstance(query, list):\n-            query_str = \" \".join([_process(c) for c in query])\n-        else:\n-            query_str = _process(query)\n-\n-        model = get_embedding_model(self.bank.embedding_model)\n-        query_vector = model.encode([query_str])[0].astype(np.float32)\n-        return await self.index.query(query_vector, k)\n+        memory_bank._status = \"registering\" \n+        # Registration logic would go here\n+        memory_bank._status = \"active\"\n+        ...\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "45",
        "type": "3",
        "number": 94,
        "head_branch": "error-011-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the pull request focus on the `run_shield` method of the `TogetherSafetyImpl` class located in `together.py`. The primary modifications include the introduction of two `time.sleep` calls that create delays in the execution flow when managing the retrieval of the `together_api_key`. Specifically, a delay of 0.01 seconds is added after confirming that `self.config.api_key` is not `None`, and a longer delay of 0.1 seconds is added before raising a `ValueError` if `provider_data` is either `None` or lacks a valid `together_api_key`. These changes are intended to affect the timing of responses related to the API key retrieval process. Importantly, there are no modifications to the method signatures or the overall structure of the `TogetherSafetyImpl` class, and the existing logic for handling shields and responses from the safety model remains unchanged. Additionally, there are no alterations to the declarations of exported or public entities in this update.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai -->\n\n---\n\n<details>\n<summary>\ud83d\udcdc Recent review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and e64e36889897b65f2a34597fab052117f7c98b4a.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/providers/adapters/safety/together/together.py (1 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n<details>\n<summary>\ud83d\udd07 Additional comments (2)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/adapters/safety/together/together.py (2)</summary><blockquote>\n\n`59-60`: _:warning: Potential issue_\n\n**Remove arbitrary delays and address potential underlying issues.**\n\nThe added `time.sleep` calls introduce arbitrary delays in the execution flow, which could negatively impact performance without clear benefits. Additionally, the `time` module is not imported, which will cause a NameError.\n\n1. If these delays are attempting to mitigate an underlying issue (e.g., rate limiting, race conditions), it's better to address that issue directly rather than using arbitrary delays.\n2. If there's no specific reason for these delays, they should be removed to maintain optimal performance.\n\n\nTo resolve these issues:\n\n1. Remove the `time.sleep` calls:\n\n```diff\n- time.sleep(0.01)\n```\n\n```diff\n- time.sleep(0.1)\n```\n\n2. If there are underlying issues that need to be addressed, please provide more context so we can suggest a more appropriate solution.\n\n\nTo ensure there are no other occurrences of `time.sleep` or references to the `time` module, run the following script:\n\n```shell\n#!/bin/bash\n# Description: Check for usage of time.sleep and imports of the time module\n\necho \"Checking for time.sleep usage:\"\nrg 'time\\.sleep'\n\necho \"\\nChecking for time module imports:\"\nrg '^import time|^from time import'\n```\n\n\n\nAlso applies to: 63-63\n\n---\n\nLine range hint `1-93`: **Final Review Summary**\n\nThe changes introduced in this pull request add arbitrary delays to the `run_shield` method of the `TogetherSafetyImpl` class. These delays do not appear to serve a clear purpose and could negatively impact the performance of the safety checks.\n\nRecommendations:\n1. Remove the introduced `time.sleep` calls.\n2. If there are underlying issues (such as rate limiting or race conditions) that these delays were attempting to address, they should be handled more appropriately and documented clearly.\n3. Ensure that any changes to the API key handling or error conditions are thoroughly tested and do not introduce unexpected behavior.\n\nPlease provide more context about the reasons for these changes so that we can suggest more appropriate solutions if necessary.\n\n\nTo ensure that the removal of these changes doesn't affect other parts of the codebase, run the following script:\n\n```shell\n#!/bin/bash\n# Description: Check for any references to TogetherSafetyImpl or run_shield method\n\necho \"Checking for references to TogetherSafetyImpl:\"\nrg 'TogetherSafetyImpl'\n\necho \"\\nChecking for references to run_shield method:\"\nrg 'run_shield'\n```\n\nThis will help identify any other parts of the codebase that might be relying on or affected by the behavior of the `run_shield` method.\n\n</blockquote></details>\n\n</blockquote></details>\n\n</details><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcEj2wAbb5AokAI7Y0riQ4rjedJAARACCeCzU0dChXAACJBJo3mCIZPD4FGABEvAkAO6UYABMMZAVaMiI2ALM6jT0cuGwntj5fOSwOWBoNQBe6Bj0qPBKGOIAZuX0TZAYTgKUkACcACwoWLi9/iTc+IjqRbIaMCfcPn4BwaGMmJBb6AwM0vmrYcdPIt8L58BV4BgiJAAKo2AAyXAA2rBcLhuIgOAB6TFEdSwVoaJjMTFMJRUVTqbSYgDiIPmYFsYAAItQmiRcJiHr5MfsALoAChRaIx2NxxwJRJJimUFNwVNp3npjJZcvyHK53h5ewAlLd3CclIgGBR4NxxPgsLx8GUlDNpvAGMlkMdqD1UBrTi9EGFGsg0IlmNQHTlvPJjSRkl15IDwvgQZAAAbpUkytRy+CYkg1EglLLlKoUBO3ABy+HQtFo6kKGBykCU6e8yCKCgWJAAHmEVPg8D1PAxhpDpPw+A8KOd8vxFr2vL5PSFvegAocGN5sLbDtPDcbTeaMBojPojCYoGR6Pgp/6CMQyMpOgpWOwuLx+MJROIpDIw9LyWmtDp9MedyzH6WCXoQpDkFQd5EmwCxcFQFSQC0ziuB8X5kiov7aLoYCGMY4BgEYVoiGImJoNwGa+GggYAPremgDAANachQ1pzJQiBkbQ5E0BQnGIGgizsrImIEKQgIUKJ+Dib0FAaNwsgcAYMQqQYFiQHEACS16QZGSFOIGqHnowA6kIg+qeJWixThCuCsbQ2A/MgzCKPAyyOruzpljGCYUNgGB0QgJCKgmkBsMcigNHiELTgm0DSeysn2IJwmacw3DeKFK5NMgMUJmJiWUPJshFncfamUOvDwIZ8ChocEgglI05oBWVYWpO4QVGW+XVSQGiIFEZxZSGXmHHZiiOZZwVoLIuVHCcTALKxfiLN4YINL0WADrQ3gQlCMYBHZ5TZH4xk+QVEk0eR8A0YxJAlbc9jcKIbnBr4sgADToIm4hsP1g3cPyAAMGhAwAjNqoWoC167VWwlbJHVgm8SZoiMXtKBTgm+TeIshIWssRBaBRt33VDyAYPgYQJqW5AJl9mCrD9vX/SQZzAxoEPk2N9mTV0JDAkuVBIBjaCJgAajkIQAKIUKxhaY7FVo2pQNHcXK3M0xaJChc2tD4EOlNhIt6agZAJ1zD9CWXddpMPWVE4tUI/S4LBuB+t4KMxr9GNnSc7aiHg1aQKtYJfecNALPAIbyIJQliBjsDrcEDqMXVMbhUn9ABIg5zTKNPwUKbLaVp5pxEC4laQtOcRWJpkB3TcRhxK1u4x19EmeC4niUyZmBmbG06Z5FlxEDWuDYDnw7Ttayizt6fliJPnh+323g5bcHiQGtuIMCHzbbbt1eIEFiogfQB3SHnE6LKxzDTgJQm4PILlKE8JCBhCc1ymIX3gsc04qqGXkMCBg/R+DzU8D7auq8NJ1wbvdU4R0si1itE5RA+4jBqUsHET2t5qyjRjEobKUECEdXbOcIu0RmwPAELtPe7AqzSAPJAUs6A8GkItIQg0oh16cIwE2KcFCih3hoa0ehkBGHiCHAWTwgYlAbmOKgKyeNlKqQMIefCEBJHTA6mBHSt5ogwUfP4NAiFkLALQgoDCso/w4T0IBGC6gaJzEQDRUo+Y6B0TlEXexR4CKSIAGx7BIAAZkCQADgiTsaJAB2AQgSACsiwahoFCXsRJOxYmLBUEDRJNQwZg2ybEhgMSBB7DFpoxxLB2i4BcbQNxHjKheNPPYoAA==\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "         together_api_key = None\n         if self.config.api_key is not None:\n             together_api_key = self.config.api_key\n-        else:\n+            time.sleep(0.01)\n+        else:            \n             provider_data = self.get_request_provider_data()\n             if provider_data is None or not provider_data.together_api_key:\n+                time.sleep(0.1)\n                 raise ValueError(\n                     'Pass Together API Key in the header X-LlamaStack-ProviderData as { \"together_api_key\": <your api key>}'\n                 )\n             together_api_key = provider_data.together_api_key\n-\n         # messages can have role assistant or user\n         api_messages = []\n         for message in messages:\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "46",
        "type": "3",
        "number": 95,
        "head_branch": "error-031-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces modifications to the `OllamaInferenceAdapter` class within the `ollama.py` file. The `list_models` method has been updated to refine error handling by removing the try-except block that previously caught `httpx.ConnectError`. Instead, the method now processes the response from the client directly. The model registration logic remains unchanged, still raising a `ValueError` for unsupported dynamic model registration.\n\nIn addition, the `_get_params` method has been altered to handle the `prompt` parameter with ASCII encoding and decoding, ensuring non-ASCII characters are ignored. This change is also reflected in the `_nonstream_chat_completion` method, where the `text` field of the `OpenAICompatCompletionChoice` undergoes similar encoding and decoding modifications. While the overall control flow of the class remains intact, these changes enhance the robustness of model listing and response handling by focusing on ASCII character processing. The method signatures for both `_get_params` and `_nonstream_chat_completion` have been noted as updated, although there are no changes in their signatures.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Reconsider ASCII encoding of prompt in `_get_params`.**\n\nThe current implementation encodes the prompt to ASCII and then decodes it back, which will remove all non-ASCII characters. This could lead to loss of important information, especially for non-English languages or prompts containing special characters.\n\n\nConsider whether this change is necessary. If it is, ensure that it doesn't negatively impact the model's performance on non-ASCII inputs. If the goal is to handle encoding issues, consider using a more permissive encoding like UTF-8:\n\n```python\n\"prompt\": chat_completion_request_to_prompt(request, self.formatter).encode('utf-8').decode('utf-8'),\n```\n\nThis will preserve non-ASCII characters while still handling potential encoding issues.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/inference/ollama/ollama.py",
                "position": 5
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n_:warning: Potential issue_\n\n**Reconsider ASCII encoding of response in `_nonstream_chat_completion`.**\n\nThe current implementation encodes the response to ASCII and then decodes it back, which will remove all non-ASCII characters. This could lead to loss of important information in the model's output, especially for non-English languages or responses containing special characters.\n\n\n\nConsider whether this change is necessary. If it is, ensure that it doesn't negatively impact the model's output for non-ASCII responses. If the goal is to handle encoding issues, consider using a more permissive encoding like UTF-8:\n\n```python\ntext=r[\"response\"].encode('utf-8').decode('utf-8'),\n```\n\nThis will preserve non-ASCII characters while still handling potential encoding issues.\n\nAdditionally, ensure that this change is consistent with the encoding strategy used in the `_get_params` method to maintain uniformity in input and output handling.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/inference/ollama/ollama.py",
                "position": 14
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/inference/ollama/ollama.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "     def _get_params(self, request: ChatCompletionRequest) -> dict:\n         return {\n             \"model\": OLLAMA_SUPPORTED_MODELS[request.model],\n-            \"prompt\": chat_completion_request_to_prompt(request, self.formatter),\n+            \"prompt\": chat_completion_request_to_prompt(request, self.formatter).encode('ascii', 'ignore').decode('ascii'),\n             \"options\": get_sampling_options(request),\n             \"raw\": True,\n             \"stream\": request.stream,\n"
                    },
                    {
                        "id": 2,
                        "content": " \n         choice = OpenAICompatCompletionChoice(\n             finish_reason=r[\"done_reason\"] if r[\"done\"] else None,\n-            text=r[\"response\"],\n+            text=r[\"response\"].encode('ascii', 'ignore').decode('ascii'),\n         )\n         response = OpenAICompatCompletionResponse(\n             choices=[choice],\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "48",
        "type": "3",
        "number": 97,
        "head_branch": "error-016-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces changes to the `server.py` file located in the `project/api/llama_stack/distribution/server` directory. A significant addition is the configuration of the `uvicorn` server, which now includes a new instantiation of `uvicorn.Config`. This configuration specifies parameters such as `host`, `port`, and a `timeout_keep_alive` set to 5 seconds. The introduction of this timeout is highlighted as an \"aggressive timeout\" and may impact how the server manages client connections.\n\nFurthermore, the `uvicorn` library is imported to facilitate the running of the FastAPI application. While the overall control flow of the server remains unchanged, the new configuration parameters are expected to influence connection handling and timeout management with clients. The existing error handling mechanisms are preserved, and no new functions or modifications to existing function signatures are introduced. The changes primarily enhance the server's configuration capabilities without altering its fundamental operation.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Consider the implications of the aggressive keep-alive timeout.**\n\nThe introduction of `timeout_keep_alive=5` sets an aggressive timeout that might cause issues for slow clients or in poor network conditions. While this can help manage server resources, it may lead to frequent connection reestablishments.\n\n\nConsider the following:\n1. Document the reasoning behind this aggressive timeout in a comment.\n2. Evaluate if this timeout is appropriate for your expected client behavior and network conditions.\n3. Consider making this value configurable, allowing it to be set via environment variables or configuration files.\n\nAdditionally, the comment about \"risky assumptions about network reliability\" suggests potential issues. Please clarify what these assumptions are and document any known limitations or scenarios where these assumptions might not hold.\n\n\nThe IPv6 handling logic using the `disable_ipv6` parameter is a good addition, providing flexibility in network stack usage.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 7
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/distribution/server/server.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "     import uvicorn\n \n     # FYI this does not do hot-reloads\n+    # Added risky assumptions about network reliability\n     listen_host = \"::\" if not disable_ipv6 else \"0.0.0.0\"\n+    uvicorn_config = uvicorn.Config(app, host=listen_host, port=port,\n+                                   timeout_keep_alive=5)  # Aggressive timeout\n     print(f\"Listening on {listen_host}:{port}\")\n     uvicorn.run(app, host=listen_host, port=port)\n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "47",
        "type": "3",
        "number": 96,
        "head_branch": "error-013-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the pull request involve the addition of a new function called `calculate_similarity` in the `agents.py` file located within the `project/api/llama_stack/providers/impls/meta_reference/agents` directory. This function is designed to compute the cosine similarity between two input vectors, `vec1` and `vec2`. It includes a check to ensure that both vectors are of equal length, returning `0.0` if they are not. The function attempts to calculate the dot product of the vectors and their magnitudes; however, it contains an error in the magnitude calculation as it does not include the necessary square root operation.\n\nFurthermore, an import statement for the `math` module has been added to the file, although this module is not used in the new function. The existing structure and functionality of the `MetaReferenceAgentsImpl` class remain unchanged, with no alterations made to any existing methods or their signatures.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcEj2wAbb5AokAI7Y0riQ4rjedJAARACCeCzU0dChXAACJBJo3mCIZPD4FGABEvAkAO6UYABMMZAVaMiI2ALM6jT0cuGwntj5fOSwOWBoNQBe6Bj0qPBKGOIAZuX0TZAYTgKUkACcAGwakACSYQyYkFvoDAzS+athuL2Qi/i++BXwGESQAKo2ADJcADasFwuG4iA4AHooUR1LBWhomMwoUwlFRVOptFCAOKveZgWxgAAi1CaJFwUO4Pm8UP2AF0ABSg8GQmFwx6I5GoxTKTG4bF47wEomkgX5SnU3x0vYASkO7ieSkQDAo8G44nwWF4+DKShm03gZxoyEe1B6qClfgCwVCDTWaESzGoRpy3nkqpIyS68kenhx6gAEq0rpqsECAAbpNF8tQC+BQkg1EglLLlKoUCNMlkQ6Gw+Fclg89EqOPYpMp0rpyjyyAAOXw6FotHUhQwOUgSnj3mQRQUCxIAA8wip8Hgep4GMMvtJ+HxqRRuPh8vxFhOvL5/EEQogRwEUBgGN5sPqD+vlar1WGNEZ9EYTFAyPR8GvHQRiGRlJ0FKx2FxePwwiiOIUgyB6vIYnGWg6PoD4wAgyCoOcb6EKQ5BUN+yJsAsXBUBUkAtM4rgXOBJb8tBuhgIYxjgGARg6iIYhQmg3AJr4aDOgA+ruaAMAA1lSFC6nMlCIFC8DMNwPZQmwAqcQEiyUGQNzMWhuBiWgamIBo3CyBwBgxIZBgWJAcRHB+6HegRTjOsRL6MNOpCIIqngtosa6fLgQm0NgNzIGg6yVM82CHmGkARmc3gMD4yTcRJ8DeC46iyBG4SNn64Waew2m6alyxRAANA0CBTj+1ImuuTCIJ8njVe0iVqrg8hbLgVRkOEFSNlIYhFNp8GeIsIViG2DmiHxiFrhlURfI8vaTU8nzlZA3UEBQyARt1ACMqWYPQG2iDUO37tuORFQEuDYBQGCfN8EYAAwaHdqXwPNJDyC4ngYPgmjHKcOTRYlFUZbQ308N5vlhPZGUrb1Uz0BlOo+WIq4TvAfDOkQ10XcqRUCOO6j9vGGD+c8iUVFwGUY1jJ6Tv9MVhSDs5fQ8aB8Z4GWIMEH3+Pg31FRUJWwCgyDkH5iAuPILx8AFTAUAEyNVTVBHxQ1yVlXgLpajeBhxM2rZam6shFRlElLhQYQ8TQ2FhFL64Rs6jypcwig+J4wwyCQ7VoM2dBFTkjxjkQQsE6gzOQHgCXwOM0QfI8nyVZdAQLA5mBOYcHj8FIVCbruFAQ5dni7cFoVtjkatQ08EYALIUmgNgkIpSc3HEWlHJJ3ipUeTTIAEzrx0Njm+w08LrI2ztua6Yamo2Q5IOIXyQLJsCKL2fB+mjyuY9QBfaUY5iWHE3g0BhbbT+eogNZrxMo0OZvfn21ICN4RqQOwrbSLekA1wHqw+7QXARiUGuSKANYp1QSklJqjItpcH+HPIEixvD4GoPSIq3UaiwPgYg5BuB6Syl0HoUmODnpYAjAxYCzFWJQnYlxHi/FBLCXRGJU20lZJoHkg3JSh4SCqWyrwhYOUUoGSMgYO8NEICv2mCjFCFkvzRCwn+fwaB8KEVss1UisZ1AUVgrRH87RcCcTmIgDhZRKh0G4gKc2lFqJwQAOwABZaD2L2AAZj2HdAArIsWxNx7EeM2vYhgm1FgCE2iQO6ex/G0EUgIAAHPY2xsS7pJjQHsax95dFYXUIY2gxiqxmNoJxJ81igA===\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n_:warning: Potential issue_\n\n**Fix cosine similarity calculation and improve function structure.**\n\nThe `calculate_similarity` function has several issues:\n\n1. The cosine similarity calculation is incorrect due to a missing square root operation in the magnitude calculation.\n2. The function lacks proper type hinting and docstring.\n3. The `math` module is imported but not used.\n\nHere are the suggested improvements:\n\n\n\n1. Fix the cosine similarity calculation:\n\n```python\nfrom math import sqrt\n\ndef calculate_similarity(vec1: List[float], vec2: List[float]) -> float:\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n\n    Args:\n        vec1 (List[float]): First vector\n        vec2 (List[float]): Second vector\n\n    Returns:\n        float: Cosine similarity between vec1 and vec2\n    \"\"\"\n    if len(vec1) != len(vec2):\n        return 0.0\n    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n    magnitude = sqrt(sum(a * a for a in vec1)) * sqrt(sum(b * b for b in vec2))\n    return dot_product / magnitude if magnitude != 0 else 0.0\n```\n\n2. Add proper type hinting by importing `List` from `typing`:\n\n```python\nfrom typing import List\n```\n\n3. Consider using NumPy for better performance and precision:\n\n```python\nimport numpy as np\n\ndef calculate_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n    \"\"\"\n    Calculate the cosine similarity between two vectors using NumPy.\n\n    Args:\n        vec1 (np.ndarray): First vector\n        vec2 (np.ndarray): Second vector\n\n    Returns:\n        float: Cosine similarity between vec1 and vec2\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        return 0.0\n    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n```\n\nChoose the implementation that best fits your project's requirements and dependencies.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "position": 18
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " import json\n import logging\n import uuid\n+import math\n from typing import AsyncGenerator\n \n from llama_stack.apis.inference import Inference\n"
                    },
                    {
                        "id": 2,
                        "content": " logger = logging.getLogger()\n logger.setLevel(logging.INFO)\n \n+def calculate_similarity(vec1: List[float], vec2: List[float]) -> float:\n+    # Incorrectly implemented cosine similarity\n+    if len(vec1) != len(vec2):\n+        return 0.0\n+    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n+    magnitude = sum(a * a for a in vec1) * sum(b * b for b in vec2)\n+    return dot_product / magnitude  # Missing sqrt in denominator\n+\n \n class MetaReferenceAgentsImpl(Agents):\n     def __init__(\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "49",
        "type": "3",
        "number": 98,
        "head_branch": "error-041-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces updates to the `memory_banks.py` file, specifically affecting the `CommonDef` and `VectorMemoryBankDef` classes. The `identifier` field in the `CommonDef` class has been modified to include a comment that specifies it must be an ASCII-compatible identifier. Similarly, the `embedding_model` field in the `VectorMemoryBankDef` class has been updated with a comment indicating that the model name must also be in ASCII format. These changes serve to clarify the expected format for these fields in the code documentation. Importantly, no alterations were made to the logic, control flow, or error handling within the methods defined in the `MemoryBanks` protocol, ensuring that the overall structure and functionality of the classes and methods remain unchanged.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcEj2wAbb5AokAI7Y0riQ4rjedJAARACCeCzU0dChXAACJBJo3mCIZPD4FGABEvAkAO6UYABMMZAVaMiI2ALM6jT0cuGwntj5fOSwOWBoNQBe6Bj0qPBKGOIAZuX0TZAYTgKUkACcABwaML1evv5BIYhhDJiQW+gMDNL5q2G4x4v4vvgV8BhEkABVGwAGS4AG1YLhcNxEBwAPRwojqWCtDRMZhwphKKiqdTaOEAcU+8zAtjAABFqE0SLg4dwfN44fsALoACkh0NhCKRb1R6MximUuNw+KJ3hJZMpIvytPpviZewAlId3MclIgGBR4NxxPgsLx8GUlDNpvBrjRkG9qD1UHK/AFgqEGms0IlmNQzTlvPJNSRkl15G9PARPpAAAbpLFCtQi+Bwkg1EglLLlKoUMOHABy+HQtFo6kKGBykCUse8yCKCgWJAAHmEVPg8D1PAxhn9pPw+PSKNx8Pl+Itmyd7ecnS5PL8GN5sMaUFggyXpJrtbqMBojPojCYoGR6PhB66CMQyMpOgpWOwuLx+MJROIpDIfYKcTGtDp9NujrNkDdD4RSOQVBnuibALFwVAVJALTOK4txPtiKivtouhgIYxjgGARgGiIYhwmg3Bxr4aDugA+pcaAMAA1nhBGIHCbDMEUsgkQImCUXRDFMSxbGIBo3CyBwBgxMJBgWJAcQAJLHoB/pQU47qwfujBtqQiCqp4+aLIOvy4BQijYI8yCMZpnqrpaOYLmGADCLCMRg5IkIsYZTPQYYAGp3kUACyJCMa4ABCbEOU5jDeE0+TID8by/EOYaca43EYOxfGyM5yxRIc9jcKI8DLNcviyAANLFczsLl5TppAywkOKc6xTZrB6sFzlTuFkDDDIJBkFMGD4CKZ5RbA6DnqBYS/Pm5q/P8VpjWEzD9GEdw3HE9hWRJUnotwHoCFEKDzEsFWZfA7RhRQ3rFZZvlbHmU0kcZNVpeUtUxZZHliN5vlMYFSXNaFbUdbcXVYNg3C0LJBBzlOM6eGgI3sFB2UMLlshTT01oLvdfhFmwkDzZcgN1Sta0SVVRTupoRzSC2KkdmQbaPEOtD4AwTjsB6eoDkOUZwX9WqLCjfxDrWiNnh8FDk6TfBBv21XirxRjZspmCqQ0lCeO6SjhBZxzePgSIMMVTALHpfiLLrFTFZWlB6Xwba0N4qMvccbBvIoyBKMs5AmrFPl+bI33sc5BoEEw3iHB4/BSFQpyXBQBm4NgAQuVV2AYGIhY5Oo8hKQurWIBFycu7AbtnO6MU6RRmhGOYlhxN4NBAYW5mM6Ip3sxgFaDsLRRnpW9I7WakBleI0gbpAABiT30CB8Mg2DnRcGGpULOVlBcLHzkxXnyDWbZTWOZvWBhthd40YRYWkeRVFnxxn0JaxSW337iXJfxzmAEmE4bLwda9QbpkCQEsF5BaBMibrTAJtbau1v6r0qlvMK+dwwNTsr9GKx89I4VpPhc+xE0BkRFNfbBT8uIP3YvRO+zFSG8TfgYKAk8arT1srPUG/pF5XToPmP4d1BTeHXrpQ+f1EHuU8hQX2X0goHzqug/AmCz5wiIpfAh1EiHkOflQ1RJCeIpQ/uGdhN0uGYz4XwQBkAvI8PWMRCcWAwEkzFuTAR29wxvQIKIihAdUFHxPrhbB8iL54Kvso2iGj748WCZQrRNDhIxA3KhLcGEh7TE5n+aSp5ogzzAv4NAkFoIKXkN0KML51BvhQnoT8IF1AkTmIgEipRUx0HwS4MIm50IQCqgANloAABgAOwAFYlC9IAMx0EGYsGoDBul7F6b0vYDBExtIACwdJ2AITpCzHgLJWQIQZNQSlxNaeU3AlTaDVNqZUepu4SlAA===\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [],
        "file_changes": [
            {
                "file": "project/api/llama_stack/apis/memory_banks/memory_banks.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " \n \n class CommonDef(BaseModel):\n-    identifier: str\n+    identifier: str  # Must be ASCII-compatible identifier\n     # Hack: move this out later\n     provider_id: str = \"\"\n \n"
                    },
                    {
                        "id": 2,
                        "content": " @json_schema_type\n class VectorMemoryBankDef(CommonDef):\n     type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n-    embedding_model: str\n+    embedding_model: str  # Model name in ASCII format\n     chunk_size_in_tokens: int\n     overlap_size_in_tokens: Optional[int] = None\n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "50",
        "type": "3",
        "number": 99,
        "head_branch": "error-015-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the pull request involve a complete overhaul of the `project/api/llama_stack/cli/model/prompt_format.py` file, shifting from a command-line interface (CLI) model prompt format to a new framework centered around memory bank definitions. The existing `ModelPromptFormat` class has been removed, and multiple new classes and types have been introduced. \n\nKey additions include the `MemoryBankType` enum, which categorizes memory banks into types such as `vector`, `keyvalue`, `keyword`, and `graph`. Each type is represented by its own Pydantic model: `VectorMemoryBankDef`, `KeyValueMemoryBankDef`, `KeywordMemoryBankDef`, and `GraphMemoryBankDef`, all inheriting from a base class called `CommonDef`. This base class includes an ASCII-compatible identifier and an optional provider ID. \n\nA new type alias, `MemoryBankDef`, is defined as a union of the various memory bank models, featuring a discriminator field for type differentiation. Additionally, a protocol named `MemoryBanks` has been introduced, specifying asynchronous methods for listing, retrieving, and registering memory banks, complete with HTTP route and method annotations. This restructuring emphasizes a more organized approach to memory bank management using modern Python features.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Address the TODO comment and improve identifier validation.**\n\n1. The comment \"Hack: move this out later\" should be addressed.\n2. The `identifier` field needs validation to ensure it's ASCII-compatible.\n\n\n\nHere's a suggested improvement using Pydantic validators:\n\n```diff\n class CommonDef(BaseModel):\n-    identifier: str  # Must be ASCII-compatible identifier\n-    # Hack: move this out later\n-    provider_id: str = \"\"\n+    identifier: Annotated[str, Field(\n+        pattern=\"^[a-zA-Z][a-zA-Z0-9_]*$\",\n+        description=\"ASCII-compatible identifier starting with a letter\"\n+    )]\n+    provider_id: Annotated[str, Field(default=\"\")]\n\n+    @property\n+    def is_valid_identifier(self) -> bool:\n+        return self.identifier.isascii()\n```\n\nWould you like me to create a GitHub issue to track the \"Hack\" comment resolution?\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/cli/model/prompt_format.py",
                "position": 130
            },
            {
                "body": "_:warning: Potential issue_\n\n**Consider adding validation for token sizes.**\n\nThe `VectorMemoryBankDef` should include validation for `chunk_size_in_tokens` and `overlap_size_in_tokens` to ensure they are positive and that overlap is less than chunk size.\n\n\nHere's a suggested improvement:\n\n```diff\n @json_schema_type\n class VectorMemoryBankDef(CommonDef):\n     type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n     embedding_model: str  # Model name in ASCII format\n-    chunk_size_in_tokens: int\n-    overlap_size_in_tokens: Optional[int] = None\n+    chunk_size_in_tokens: Annotated[int, Field(gt=0)]\n+    overlap_size_in_tokens: Optional[Annotated[int, Field(ge=0)]] = None\n+\n+    @model_validator(mode='after')\n+    def validate_overlap_size(self) -> 'VectorMemoryBankDef':\n+        if (self.overlap_size_in_tokens is not None and \n+            self.overlap_size_in_tokens >= self.chunk_size_in_tokens):\n+            raise ValueError(\"overlap_size_in_tokens must be less than chunk_size_in_tokens\")\n+        return self\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n@json_schema_type\nclass VectorMemoryBankDef(CommonDef):\n    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n    embedding_model: str  # Model name in ASCII format\n    chunk_size_in_tokens: Annotated[int, Field(gt=0)]\n    overlap_size_in_tokens: Optional[Annotated[int, Field(ge=0)]] = None\n\n    @model_validator(mode='after')\n    def validate_overlap_size(self) -> 'VectorMemoryBankDef':\n        if (self.overlap_size_in_tokens is not None and \n            self.overlap_size_in_tokens >= self.chunk_size_in_tokens):\n            raise ValueError(\"overlap_size_in_tokens must be less than chunk_size_in_tokens\")\n        return self\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/cli/model/prompt_format.py",
                "position": 138
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider adding error handling and validation methods.**\n\nThe `MemoryBanks` protocol should include:\n1. Error handling for cases where memory banks don't exist\n2. Validation methods for memory bank operations\n3. Documentation for the expected behavior of each method\n\n\nHere's a suggested improvement:\n\n```diff\n @runtime_checkable\n class MemoryBanks(Protocol):\n     @webmethod(route=\"/memory_banks/list\", method=\"GET\")\n     async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]: ...\n\n     @webmethod(route=\"/memory_banks/get\", method=\"GET\")\n     async def get_memory_bank(\n         self, identifier: str\n-    ) -> Optional[MemoryBankDefWithProvider]: ...\n+    ) -> MemoryBankDefWithProvider:\n+        \"\"\"Retrieve a memory bank by its identifier.\n+        \n+        Args:\n+            identifier: The unique identifier of the memory bank\n+            \n+        Returns:\n+            MemoryBankDefWithProvider: The memory bank definition\n+            \n+        Raises:\n+            ValueError: If the memory bank doesn't exist\n+        \"\"\"\n+        ...\n\n     @webmethod(route=\"/memory_banks/register\", method=\"POST\")\n     async def register_memory_bank(\n         self, memory_bank: MemoryBankDefWithProvider\n-    ) -> None: ...\n+    ) -> None:\n+        \"\"\"Register a new memory bank.\n+        \n+        Args:\n+            memory_bank: The memory bank definition to register\n+            \n+        Raises:\n+            ValueError: If a memory bank with the same identifier already exists\n+            ValidationError: If the memory bank definition is invalid\n+        \"\"\"\n+        ...\n+\n+    async def validate_memory_bank(\n+        self, memory_bank: MemoryBankDefWithProvider\n+    ) -> bool:\n+        \"\"\"Validate a memory bank definition.\n+        \n+        Args:\n+            memory_bank: The memory bank definition to validate\n+            \n+        Returns:\n+            bool: True if the memory bank is valid\n+            \n+        Raises:\n+            ValidationError: If the memory bank definition is invalid\n+        \"\"\"\n+        ...\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n@runtime_checkable\nclass MemoryBanks(Protocol):\n    @webmethod(route=\"/memory_banks/list\", method=\"GET\")\n    async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]: ...\n\n    @webmethod(route=\"/memory_banks/get\", method=\"GET\")\n    async def get_memory_bank(\n        self, identifier: str\n    ) -> MemoryBankDefWithProvider:\n        \"\"\"Retrieve a memory bank by its identifier.\n        \n        Args:\n            identifier: The unique identifier of the memory bank\n            \n        Returns:\n            MemoryBankDefWithProvider: The memory bank definition\n            \n        Raises:\n            ValueError: If the memory bank doesn't exist\n        \"\"\"\n        ...\n\n    @webmethod(route=\"/memory_banks/register\", method=\"POST\")\n    async def register_memory_bank(\n        self, memory_bank: MemoryBankDefWithProvider\n    ) -> None:\n        \"\"\"Register a new memory bank.\n        \n        Args:\n            memory_bank: The memory bank definition to register\n            \n        Raises:\n            ValueError: If a memory bank with the same identifier already exists\n            ValidationError: If the memory bank definition is invalid\n        \"\"\"\n        ...\n\n    async def validate_memory_bank(\n        self, memory_bank: MemoryBankDefWithProvider\n    ) -> bool:\n        \"\"\"Validate a memory bank definition.\n        \n        Args:\n            memory_bank: The memory bank definition to validate\n            \n        Returns:\n            bool: True if the memory bank is valid\n            \n        Raises:\n            ValidationError: If the memory bank definition is invalid\n        \"\"\"\n        ...\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/cli/model/prompt_format.py",
                "position": 181
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/cli/model/prompt_format.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n \n-import argparse\n-import textwrap\n-from io import StringIO\n-\n-from llama_models.datatypes import CoreModelId, is_multimodal, model_family, ModelFamily\n-\n-from llama_stack.cli.subcommand import Subcommand\n-\n-\n-class ModelPromptFormat(Subcommand):\n-    \"\"\"Llama model cli for describe a model prompt format (message formats)\"\"\"\n-\n-    def __init__(self, subparsers: argparse._SubParsersAction):\n-        super().__init__()\n-        self.parser = subparsers.add_parser(\n-            \"prompt-format\",\n-            prog=\"llama model prompt-format\",\n-            description=\"Show llama model message formats\",\n-            epilog=textwrap.dedent(\n-                \"\"\"\n-                Example:\n-                    llama model prompt-format <options>\n-                \"\"\"\n-            ),\n-            formatter_class=argparse.RawTextHelpFormatter,\n-        )\n-        self._add_arguments()\n-        self.parser.set_defaults(func=self._run_model_template_cmd)\n-\n-    def _add_arguments(self):\n-        self.parser.add_argument(\n-            \"-m\",\n-            \"--model-name\",\n-            type=str,\n-            default=\"llama3_1\",\n-            help=\"Model Family (llama3_1, llama3_X, etc.)\",\n-        )\n-\n-    def _run_model_template_cmd(self, args: argparse.Namespace) -> None:\n-        import pkg_resources\n-\n-        # Only Llama 3.1 and 3.2 are supported\n-        supported_model_ids = [\n-            m\n-            for m in CoreModelId\n-            if model_family(m) in {ModelFamily.llama3_1, ModelFamily.llama3_2}\n-        ]\n-        model_str = \"\\n\".join([m.value for m in supported_model_ids])\n-        try:\n-            model_id = CoreModelId(args.model_name)\n-        except ValueError:\n-            self.parser.error(\n-                f\"{args.model_name} is not a valid Model. Choose one from --\\n{model_str}\"\n-            )\n-\n-        if model_id not in supported_model_ids:\n-            self.parser.error(\n-                f\"{model_id} is not a valid Model. Choose one from --\\n {model_str}\"\n-            )\n-\n-        llama_3_1_file = pkg_resources.resource_filename(\n-            \"llama_models\", \"llama3_1/prompt_format.md\"\n-        )\n-        llama_3_2_text_file = pkg_resources.resource_filename(\n-            \"llama_models\", \"llama3_2/text_prompt_format.md\"\n-        )\n-        llama_3_2_vision_file = pkg_resources.resource_filename(\n-            \"llama_models\", \"llama3_2/vision_prompt_format.md\"\n-        )\n-        if model_family(model_id) == ModelFamily.llama3_1:\n-            with open(llama_3_1_file, \"r\") as f:\n-                content = f.read()\n-        elif model_family(model_id) == ModelFamily.llama3_2:\n-            if is_multimodal(model_id):\n-                with open(llama_3_2_vision_file, \"r\") as f:\n-                    content = f.read()\n-            else:\n-                with open(llama_3_2_text_file, \"r\") as f:\n-                    content = f.read()\n-\n-        render_markdown_to_pager(content)\n-\n-\n-def render_markdown_to_pager(markdown_content: str):\n-    from rich.console import Console\n-    from rich.markdown import Markdown\n-    from rich.style import Style\n-    from rich.text import Text\n-\n-    class LeftAlignedHeaderMarkdown(Markdown):\n-        def parse_header(self, token):\n-            level = token.type.count(\"h\")\n-            content = Text(token.content)\n-            header_style = Style(color=\"bright_blue\", bold=True)\n-            header = Text(f\"{'#' * level} \", style=header_style) + content\n-            self.add_text(header)\n-\n-    # Render the Markdown\n-    md = LeftAlignedHeaderMarkdown(markdown_content)\n-\n-    # Capture the rendered output\n-    output = StringIO()\n-    console = Console(file=output, force_terminal=True, width=100)  # Set a fixed width\n-    console.print(md)\n-    rendered_content = output.getvalue()\n-    print(rendered_content)\n+from enum import Enum\n+from typing import List, Literal, Optional, Protocol, runtime_checkable, Union\n+\n+from llama_models.schema_utils import json_schema_type, webmethod\n+from pydantic import BaseModel, Field\n+from typing_extensions import Annotated\n+\n+\n+@json_schema_type\n+class MemoryBankType(Enum):\n+    vector = \"vector\"\n+    keyvalue = \"keyvalue\"\n+    keyword = \"keyword\"\n+    graph = \"graph\"\n+\n+\n+class CommonDef(BaseModel):\n+    identifier: str  # Must be ASCII-compatible identifier\n+    # Hack: move this out later\n+    provider_id: str = \"\"\n+\n+\n+@json_schema_type\n+class VectorMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n+    embedding_model: str  # Model name in ASCII format\n+    chunk_size_in_tokens: int\n+    overlap_size_in_tokens: Optional[int] = None\n+\n+\n+@json_schema_type\n+class KeyValueMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.keyvalue.value] = MemoryBankType.keyvalue.value\n+\n+\n+@json_schema_type\n+class KeywordMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.keyword.value] = MemoryBankType.keyword.value\n+\n+\n+@json_schema_type\n+class GraphMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.graph.value] = MemoryBankType.graph.value\n+\n+\n+MemoryBankDef = Annotated[\n+    Union[\n+        VectorMemoryBankDef,\n+        KeyValueMemoryBankDef,\n+        KeywordMemoryBankDef,\n+        GraphMemoryBankDef,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+MemoryBankDefWithProvider = MemoryBankDef\n+\n+\n+@runtime_checkable\n+class MemoryBanks(Protocol):\n+    @webmethod(route=\"/memory_banks/list\", method=\"GET\")\n+    async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]: ...\n+\n+    @webmethod(route=\"/memory_banks/get\", method=\"GET\")\n+    async def get_memory_bank(\n+        self, identifier: str\n+    ) -> Optional[MemoryBankDefWithProvider]: ...\n+\n+    @webmethod(route=\"/memory_banks/register\", method=\"POST\")\n+    async def register_memory_bank(\n+        self, memory_bank: MemoryBankDefWithProvider\n+    ) -> None: ...\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "51",
        "type": "3",
        "number": 100,
        "head_branch": "error-040-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the `together.py` file involve extensive structural modifications aimed at enhancing agent tools and memory management functionalities. New classes have been introduced, including `Attachment`, `AgentTool`, and `ToolDefinitionCommon`, along with specific tool definitions such as `SearchToolDefinition` and `MemoryToolDefinition`. These classes leverage Pydantic's `BaseModel` for data validation and serialization, ensuring type safety.\n\nEnumerations like `AgentTool` and `SearchEngineType` have been added to categorize tools and configurations. The file also introduces classes for memory bank configurations, including `AgentVectorMemoryBankConfig` and `AgentKeyValueMemoryBankConfig`, alongside new memory query generator types.\n\nAdditionally, a series of step-related classes, such as `StepCommon` and its derivatives, have been created to standardize agent interaction steps. The `Turn` and `Session` classes encapsulate the concepts of turns and sessions in these interactions. The configuration capabilities for agents have been expanded with new classes like `AgentConfig` and `AgentConfigOverridablePerTurn`. Furthermore, the `Agents` protocol has been defined to outline the API for managing agents and their sessions. Overall, these changes represent a comprehensive overhaul of the file, focusing on modularity and structured data handling.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider adding validation for shield lists.**\n\nThe `ToolDefinitionCommon` class could benefit from validation to ensure shield IDs are unique.\n\n```diff\n class ToolDefinitionCommon(BaseModel):\n-    input_shields: Optional[List[str]] = Field(default_factory=list)\n-    output_shields: Optional[List[str]] = Field(default_factory=list)\n+    input_shields: Optional[List[str]] = Field(default_factory=list, unique_items=True)\n+    output_shields: Optional[List[str]] = Field(default_factory=list, unique_items=True)\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\nclass ToolDefinitionCommon(BaseModel):\n    input_shields: Optional[List[str]] = Field(default_factory=list, unique_items=True)\n    output_shields: Optional[List[str]] = Field(default_factory=list, unique_items=True)\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "position": 51
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider adding validation for sampling parameters.**\n\nThe `AgentConfigCommon` class could benefit from validation for `max_infer_iters` to ensure it's positive.\n\n```diff\n class AgentConfigCommon(BaseModel):\n     sampling_params: Optional[SamplingParams] = SamplingParams()\n     input_shields: Optional[List[str]] = Field(default_factory=list)\n     output_shields: Optional[List[str]] = Field(default_factory=list)\n     tools: Optional[List[AgentToolDefinition]] = Field(default_factory=list)\n     tool_choice: Optional[ToolChoice] = Field(default=ToolChoice.auto)\n     tool_prompt_format: Optional[ToolPromptFormat] = Field(\n         default=ToolPromptFormat.json\n     )\n-    max_infer_iters: int = 10\n+    max_infer_iters: int = Field(default=10, gt=0, description=\"Maximum number of inference iterations\")\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\nclass AgentConfigCommon(BaseModel):\n    sampling_params: Optional[SamplingParams] = SamplingParams()\n\n    input_shields: Optional[List[str]] = Field(default_factory=list)\n    output_shields: Optional[List[str]] = Field(default_factory=list)\n\n    tools: Optional[List[AgentToolDefinition]] = Field(default_factory=list)\n    tool_choice: Optional[ToolChoice] = Field(default=ToolChoice.auto)\n    tool_prompt_format: Optional[ToolPromptFormat] = Field(\n        default=ToolPromptFormat.json\n    )\n\n    max_infer_iters: int = Field(default=10, gt=0, description=\"Maximum number of inference iterations\")\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "position": 312
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " #\n # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n-from together import Together\n+\n+from datetime import datetime\n+from enum import Enum\n+from typing import (\n+    Any,\n+    Dict,\n+    List,\n+    Literal,\n+    Optional,\n+    Protocol,\n+    runtime_checkable,\n+    Union,\n+)\n+\n+from llama_models.schema_utils import json_schema_type, webmethod\n+\n+from pydantic import BaseModel, ConfigDict, Field\n+from typing_extensions import Annotated\n \n from llama_models.llama3.api.datatypes import *  # noqa: F403\n+from llama_stack.apis.common.deployment_types import *  # noqa: F403\n+from llama_stack.apis.inference import *  # noqa: F403\n from llama_stack.apis.safety import *  # noqa: F403\n-from llama_stack.distribution.request_headers import NeedsRequestProviderData\n-from llama_stack.providers.datatypes import ShieldsProtocolPrivate\n+from llama_stack.apis.memory import *  # noqa: F403\n+\n+\n+@json_schema_type\n+class Attachment(BaseModel):\n+    content: InterleavedTextMedia | URL\n+    mime_type: str\n+\n+\n+class AgentTool(Enum):\n+    brave_search = \"brave_search\"\n+    wolfram_alpha = \"wolfram_alpha\"\n+    photogen = \"photogen\"\n+    code_interpreter = \"code_interpreter\"\n+\n+    function_call = \"function_call\"\n+    memory = \"memory\"\n+\n+\n+class ToolDefinitionCommon(BaseModel):\n+    input_shields: Optional[List[str]] = Field(default_factory=list)\n+    output_shields: Optional[List[str]] = Field(default_factory=list)\n+\n+\n+class SearchEngineType(Enum):\n+    bing = \"bing\"\n+    brave = \"brave\"\n+\n+\n+@json_schema_type\n+class SearchToolDefinition(ToolDefinitionCommon):\n+    # NOTE: brave_search is just a placeholder since model always uses\n+    # brave_search as tool call name\n+    type: Literal[AgentTool.brave_search.value] = AgentTool.brave_search.value\n+    api_key: str\n+    engine: SearchEngineType = SearchEngineType.brave\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+@json_schema_type\n+class WolframAlphaToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.wolfram_alpha.value] = AgentTool.wolfram_alpha.value\n+    api_key: str\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+@json_schema_type\n+class PhotogenToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.photogen.value] = AgentTool.photogen.value\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+@json_schema_type\n+class CodeInterpreterToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.code_interpreter.value] = AgentTool.code_interpreter.value\n+    enable_inline_code_execution: bool = True\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+@json_schema_type\n+class FunctionCallToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.function_call.value] = AgentTool.function_call.value\n+    function_name: str\n+    description: str\n+    parameters: Dict[str, ToolParamDefinition]\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+class _MemoryBankConfigCommon(BaseModel):\n+    bank_id: str\n+\n+\n+class AgentVectorMemoryBankConfig(_MemoryBankConfigCommon):\n+    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n+\n+\n+class AgentKeyValueMemoryBankConfig(_MemoryBankConfigCommon):\n+    type: Literal[MemoryBankType.keyvalue.value] = MemoryBankType.keyvalue.value\n+    keys: List[str]  # what keys to focus on\n+\n+\n+class AgentKeywordMemoryBankConfig(_MemoryBankConfigCommon):\n+    type: Literal[MemoryBankType.keyword.value] = MemoryBankType.keyword.value\n+\n+\n+class AgentGraphMemoryBankConfig(_MemoryBankConfigCommon):\n+    type: Literal[MemoryBankType.graph.value] = MemoryBankType.graph.value\n+    entities: List[str]  # what entities to focus on\n+\n+\n+MemoryBankConfig = Annotated[\n+    Union[\n+        AgentVectorMemoryBankConfig,\n+        AgentKeyValueMemoryBankConfig,\n+        AgentKeywordMemoryBankConfig,\n+        AgentGraphMemoryBankConfig,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+\n+class MemoryQueryGenerator(Enum):\n+    default = \"default\"\n+    llm = \"llm\"\n+    custom = \"custom\"\n+\n+\n+class DefaultMemoryQueryGeneratorConfig(BaseModel):\n+    type: Literal[MemoryQueryGenerator.default.value] = (\n+        MemoryQueryGenerator.default.value\n+    )\n+    sep: str = \" \"\n+\n+\n+class LLMMemoryQueryGeneratorConfig(BaseModel):\n+    type: Literal[MemoryQueryGenerator.llm.value] = MemoryQueryGenerator.llm.value\n+    model: str\n+    template: str\n+\n+\n+class CustomMemoryQueryGeneratorConfig(BaseModel):\n+    type: Literal[MemoryQueryGenerator.custom.value] = MemoryQueryGenerator.custom.value\n+\n+\n+MemoryQueryGeneratorConfig = Annotated[\n+    Union[\n+        DefaultMemoryQueryGeneratorConfig,\n+        LLMMemoryQueryGeneratorConfig,\n+        CustomMemoryQueryGeneratorConfig,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+\n+@json_schema_type\n+class MemoryToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.memory.value] = AgentTool.memory.value\n+    memory_bank_configs: List[MemoryBankConfig] = Field(default_factory=list)\n+    # This config defines how a query is generated using the messages\n+    # for memory bank retrieval.\n+    query_generator_config: MemoryQueryGeneratorConfig = Field(\n+        default=DefaultMemoryQueryGeneratorConfig()\n+    )\n+    max_tokens_in_context: int = 4096\n+    max_chunks: int = 10\n+\n+\n+AgentToolDefinition = Annotated[\n+    Union[\n+        SearchToolDefinition,\n+        WolframAlphaToolDefinition,\n+        PhotogenToolDefinition,\n+        CodeInterpreterToolDefinition,\n+        FunctionCallToolDefinition,\n+        MemoryToolDefinition,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+\n+class StepCommon(BaseModel):\n+    turn_id: str\n+    step_id: str\n+    started_at: Optional[datetime] = None\n+    completed_at: Optional[datetime] = None\n+\n+\n+class StepType(Enum):\n+    inference = \"inference\"\n+    tool_execution = \"tool_execution\"\n+    shield_call = \"shield_call\"\n+    memory_retrieval = \"memory_retrieval\"\n+\n+\n+@json_schema_type\n+class InferenceStep(StepCommon):\n+    model_config = ConfigDict(protected_namespaces=())\n+\n+    step_type: Literal[StepType.inference.value] = StepType.inference.value\n+    model_response: CompletionMessage\n \n-from .config import TogetherSafetyConfig\n \n+@json_schema_type\n+class ToolExecutionStep(StepCommon):\n+    step_type: Literal[StepType.tool_execution.value] = StepType.tool_execution.value\n+    tool_calls: List[ToolCall]\n+    tool_responses: List[ToolResponse]\n \n-TOGETHER_SHIELD_MODEL_MAP = {\n-    \"llama_guard\": \"meta-llama/Meta-Llama-Guard-3-8B\",\n-    \"Llama-Guard-3-8B\": \"meta-llama/Meta-Llama-Guard-3-8B\",\n-    \"Llama-Guard-3-11B-Vision\": \"meta-llama/Llama-Guard-3-11B-Vision-Turbo\",\n-}\n \n+@json_schema_type\n+class ShieldCallStep(StepCommon):\n+    step_type: Literal[StepType.shield_call.value] = StepType.shield_call.value\n+    violation: Optional[SafetyViolation]\n \n-class TogetherSafetyImpl(Safety, NeedsRequestProviderData, ShieldsProtocolPrivate):\n-    def __init__(self, config: TogetherSafetyConfig) -> None:\n-        self.config = config\n \n-    async def initialize(self) -> None:\n-        pass\n+@json_schema_type\n+class MemoryRetrievalStep(StepCommon):\n+    step_type: Literal[StepType.memory_retrieval.value] = (\n+        StepType.memory_retrieval.value\n+    )\n+    memory_bank_ids: List[str]\n+    inserted_context: InterleavedTextMedia\n \n-    async def shutdown(self) -> None:\n-        pass\n \n-    async def register_shield(self, shield: ShieldDef) -> None:\n-        raise ValueError(\"Registering dynamic shields is not supported\")\n+Step = Annotated[\n+    Union[\n+        InferenceStep,\n+        ToolExecutionStep,\n+        ShieldCallStep,\n+        MemoryRetrievalStep,\n+    ],\n+    Field(discriminator=\"step_type\"),\n+]\n \n-    async def list_shields(self) -> List[ShieldDef]:\n-        return [\n-            ShieldDef(\n-                identifier=ShieldType.llama_guard.value,\n-                type=ShieldType.llama_guard.value,\n-                params={},\n-            )\n+\n+@json_schema_type\n+class Turn(BaseModel):\n+    \"\"\"A single turn in an interaction with an Agentic System.\"\"\"\n+\n+    turn_id: str\n+    session_id: str\n+    input_messages: List[\n+        Union[\n+            UserMessage,\n+            ToolResponseMessage,\n         ]\n+    ]\n+    steps: List[Step]\n+    output_message: CompletionMessage\n+    output_attachments: List[Attachment] = Field(default_factory=list)\n+\n+    started_at: datetime\n+    completed_at: Optional[datetime] = None\n+\n+\n+@json_schema_type\n+class Session(BaseModel):\n+    \"\"\"A single session of an interaction with an Agentic System.\"\"\"\n+\n+    session_id: str\n+    session_name: str\n+    turns: List[Turn]\n+    started_at: datetime\n+\n+    memory_bank: Optional[MemoryBankDef] = None\n+\n+\n+class AgentConfigCommon(BaseModel):\n+    sampling_params: Optional[SamplingParams] = SamplingParams()\n+\n+    input_shields: Optional[List[str]] = Field(default_factory=list)\n+    output_shields: Optional[List[str]] = Field(default_factory=list)\n+\n+    tools: Optional[List[AgentToolDefinition]] = Field(default_factory=list)\n+    tool_choice: Optional[ToolChoice] = Field(default=ToolChoice.auto)\n+    tool_prompt_format: Optional[ToolPromptFormat] = Field(\n+        default=ToolPromptFormat.json\n+    )\n+\n+    max_infer_iters: int = 10\n+\n+\n+@json_schema_type\n+class AgentConfig(AgentConfigCommon):\n+    model: str\n+    instructions: str\n+    enable_session_persistence: bool\n+\n+\n+class AgentConfigOverridablePerTurn(AgentConfigCommon):\n+    instructions: Optional[str] = None\n+\n+\n+class AgentTurnResponseEventType(Enum):\n+    step_start = \"step_start\"\n+    step_complete = \"step_complete\"\n+    step_progress = \"step_progress\"\n+\n+    turn_start = \"turn_start\"\n+    turn_complete = \"turn_complete\"\n+\n+\n+@json_schema_type\n+class AgentTurnResponseStepStartPayload(BaseModel):\n+    event_type: Literal[AgentTurnResponseEventType.step_start.value] = (\n+        AgentTurnResponseEventType.step_start.value\n+    )\n+    step_type: StepType\n+    step_id: str\n+    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)\n+\n+\n+@json_schema_type\n+class AgentTurnResponseStepCompletePayload(BaseModel):\n+    event_type: Literal[AgentTurnResponseEventType.step_complete.value] = (\n+        AgentTurnResponseEventType.step_complete.value\n+    )\n+    step_type: StepType\n+    step_details: Step\n+\n+\n+@json_schema_type\n+class AgentTurnResponseStepProgressPayload(BaseModel):\n+    model_config = ConfigDict(protected_namespaces=())\n+\n+    event_type: Literal[AgentTurnResponseEventType.step_progress.value] = (\n+        AgentTurnResponseEventType.step_progress.value\n+    )\n+    step_type: StepType\n+    step_id: str\n+\n+    model_response_text_delta: Optional[str] = None\n+    tool_call_delta: Optional[ToolCallDelta] = None\n+    tool_response_text_delta: Optional[str] = None\n+\n+\n+@json_schema_type\n+class AgentTurnResponseTurnStartPayload(BaseModel):\n+    event_type: Literal[AgentTurnResponseEventType.turn_start.value] = (\n+        AgentTurnResponseEventType.turn_start.value\n+    )\n+    turn_id: str\n+\n+\n+@json_schema_type\n+class AgentTurnResponseTurnCompletePayload(BaseModel):\n+    event_type: Literal[AgentTurnResponseEventType.turn_complete.value] = (\n+        AgentTurnResponseEventType.turn_complete.value\n+    )\n+    turn: Turn\n+\n+\n+@json_schema_type\n+class AgentTurnResponseEvent(BaseModel):\n+    \"\"\"Streamed agent execution response.\"\"\"\n+\n+    payload: Annotated[\n+        Union[\n+            AgentTurnResponseStepStartPayload,\n+            AgentTurnResponseStepProgressPayload,\n+            AgentTurnResponseStepCompletePayload,\n+            AgentTurnResponseTurnStartPayload,\n+            AgentTurnResponseTurnCompletePayload,\n+        ],\n+        Field(discriminator=\"event_type\"),\n+    ]\n+\n+\n+@json_schema_type\n+class AgentCreateResponse(BaseModel):\n+    agent_id: str\n+\n+\n+@json_schema_type\n+class AgentSessionCreateResponse(BaseModel):\n+    session_id: str\n+\n+\n+@json_schema_type\n+class AgentTurnCreateRequest(AgentConfigOverridablePerTurn):\n+    agent_id: str\n+    session_id: str\n+\n+    # TODO: figure out how we can simplify this and make why\n+    # ToolResponseMessage needs to be here (it is function call\n+    # execution from outside the system)\n+    messages: List[\n+        Union[\n+            UserMessage,\n+            ToolResponseMessage,\n+        ]\n+    ]\n+    attachments: Optional[List[Attachment]] = None\n+\n+    stream: Optional[bool] = False\n+\n+\n+@json_schema_type\n+class AgentTurnResponseStreamChunk(BaseModel):\n+    event: AgentTurnResponseEvent\n+\n+\n+@json_schema_type\n+class AgentStepResponse(BaseModel):\n+    step: Step\n+\n+\n+@runtime_checkable\n+class Agents(Protocol):\n+    @webmethod(route=\"/agents/create\")\n+    async def create_agent(\n+        self,\n+        agent_config: AgentConfig,\n+    ) -> AgentCreateResponse: ...\n+\n+    # This method is not `async def` because it can result in either an\n+    # `AsyncGenerator` or a `AgentTurnCreateResponse` depending on the value of `stream`.\n+    @webmethod(route=\"/agents/turn/create\")\n+    def create_agent_turn(\n+        self,\n+        agent_id: str,\n+        session_id: str,\n+        messages: List[\n+            Union[\n+                UserMessage,\n+                ToolResponseMessage,\n+            ]\n+        ],\n+        attachments: Optional[List[Attachment]] = None,\n+        stream: Optional[bool] = False,\n+    ) -> AgentTurnResponseStreamChunk: ...\n+\n+    @webmethod(route=\"/agents/turn/get\")\n+    async def get_agents_turn(\n+        self,\n+        agent_id: str,\n+        turn_id: str,\n+    ) -> Turn: ...\n+\n+    @webmethod(route=\"/agents/step/get\")\n+    async def get_agents_step(\n+        self, agent_id: str, turn_id: str, step_id: str\n+    ) -> AgentStepResponse: ...\n+\n+    @webmethod(route=\"/agents/session/create\")\n+    async def create_agent_session(\n+        self,\n+        agent_id: str,\n+        session_name: str,\n+    ) -> AgentSessionCreateResponse: ...\n+\n+    @webmethod(route=\"/agents/session/get\")\n+    async def get_agents_session(\n+        self,\n+        agent_id: str,\n+        session_id: str,\n+        turn_ids: Optional[List[str]] = None,\n+    ) -> Session: ...\n+\n+    @webmethod(route=\"/agents/session/delete\")\n+    async def delete_agents_session(self, agent_id: str, session_id: str) -> None: ...\n \n-    async def run_shield(\n-        self, shield_type: str, messages: List[Message], params: Dict[str, Any] = None\n-    ) -> RunShieldResponse:\n-        shield_def = await self.shield_store.get_shield(shield_type)\n-        if not shield_def:\n-            raise ValueError(f\"Unknown shield {shield_type}\")\n-\n-        model = shield_def.params.get(\"model\", \"llama_guard\")\n-        if model not in TOGETHER_SHIELD_MODEL_MAP:\n-            raise ValueError(f\"Unsupported safety model: {model}\")\n-\n-        together_api_key = None\n-        if self.config.api_key is not None:\n-            together_api_key = self.config.api_key\n-        else:\n-            provider_data = self.get_request_provider_data()\n-            if provider_data is None or not provider_data.together_api_key:\n-                raise ValueError(\n-                    'Pass Together API Key in the header X-LlamaStack-ProviderData as { \"together_api_key\": <your api key>}'\n-                )\n-            together_api_key = provider_data.together_api_key\n-\n-        # messages can have role assistant or user\n-        api_messages = []\n-        for message in messages:\n-            if message.role in (Role.user.value, Role.assistant.value):\n-                api_messages.append({\"role\": message.role, \"content\": message.content})\n-\n-        violation = await get_safety_response(\n-            together_api_key, TOGETHER_SHIELD_MODEL_MAP[model], api_messages\n-        )\n-        return RunShieldResponse(violation=violation)\n-\n-\n-async def get_safety_response(\n-    api_key: str, model_name: str, messages: List[Dict[str, str]]\n-) -> Optional[SafetyViolation]:\n-    client = Together(api_key=api_key)\n-    response = client.chat.completions.create(messages=messages, model=model_name)\n-    if len(response.choices) == 0:\n-        return None\n-\n-    response_text = response.choices[0].message.content\n-    if response_text == \"safe\":\n-        return None\n-\n-    parts = response_text.split(\"\\n\")\n-    if len(parts) != 2:\n-        return None\n-\n-    if parts[0] == \"unsafe\":\n-        return SafetyViolation(\n-            violation_level=ViolationLevel.ERROR,\n-            metadata={\"violation_type\": parts[1]},\n-        )\n-\n-    return None\n+    @webmethod(route=\"/agents/delete\")\n+    async def delete_agents(\n+        self,\n+        agent_id: str,\n+    ) -> None: ...\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "52",
        "type": "3",
        "number": 101,
        "head_branch": "error-017-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces substantial modifications to the `server.py` file within the project. The primary focus is on restructuring API handling and implementing a new routing table architecture. The changes involve the removal of existing exception handling and event streaming logic, which have been replaced by a modular approach featuring new classes dedicated to managing routing tables for models, shields, and memory banks.\n\nA notable addition is the `CommonRoutingTableImpl` class, which oversees the registration and retrieval of objects based on identifiers and provider IDs. This class includes methods for initializing the registry, adding objects, and managing shutdown processes. Additionally, the `create_dynamic_typed_route` function has been updated to align with the new routing framework, and the `main` function has been modified to reflect changes in API serving.\n\nNew methods for registering and retrieving objects have been introduced, improving the organization and functionality of the API server. The previous direct handling of requests and responses has been abstracted into the new routing table classes, promoting a clearer separation of concerns within the API architecture. Overall, these changes enhance the structure and extensibility of the API server's endpoint management.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Potential race condition in object registration.**\n\nThe `register_object` method modifies shared state without proper synchronization. This could lead to race conditions in concurrent scenarios.\n\nConsider this fix:\n\n```diff\n-    async def register_object(self, obj: RoutableObjectWithProvider):\n+    async def register_object(self, obj: RoutableObjectWithProvider):\n+        from asyncio import Lock\n+        \n+        if not hasattr(self, '_registry_lock'):\n+            self._registry_lock = Lock()\n+            \n         # if provider_id is not specified, we'll pick an arbitrary one from existing entries\n         if not obj.provider_id and len(self.impls_by_provider_id) > 0:\n             obj.provider_id = list(self.impls_by_provider_id.keys())[0]\n \n         if obj.provider_id not in self.impls_by_provider_id:\n             raise ValueError(f\"Provider `{obj.provider_id}` not found\")\n         \n         p = self.impls_by_provider_id[obj.provider_id]\n         await register_object_with_provider(obj, p)\n         \n-        # Direct registry modification without checking current state\n-        if obj.identifier not in self.registry:\n-            self.registry[obj.identifier] = []\n-        self.registry[obj.identifier].append(obj)\n+        async with self._registry_lock:\n+            if obj.identifier not in self.registry:\n+                self.registry[obj.identifier] = []\n+            self.registry[obj.identifier].append(obj)\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    async def register_object(self, obj: RoutableObjectWithProvider):\n        from asyncio import Lock\n        \n        if not hasattr(self, '_registry_lock'):\n            self._registry_lock = Lock()\n            \n        # if provider_id is not specified, we'll pick an arbitrary one from existing entries\n        if not obj.provider_id and len(self.impls_by_provider_id) > 0:\n            obj.provider_id = list(self.impls_by_provider_id.keys())[0]\n\n        if obj.provider_id not in self.impls_by_provider_id:\n            raise ValueError(f\"Provider `{obj.provider_id}` not found\")\n        \n        p = self.impls_by_provider_id[obj.provider_id]\n        await register_object_with_provider(obj, p)\n        \n        async with self._registry_lock:\n            if obj.identifier not in self.registry:\n                self.registry[obj.identifier] = []\n            self.registry[obj.identifier].append(obj)\n        # TODO: persist this to a store\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 451
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/distribution/server/server.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n \n-import asyncio\n-import functools\n-import inspect\n-import json\n-import signal\n-import traceback\n-\n-from contextlib import asynccontextmanager\n-from ssl import SSLError\n-from typing import Any, Dict, Optional\n-\n-import fire\n-import httpx\n-import yaml\n-\n-from fastapi import Body, FastAPI, HTTPException, Request, Response\n-from fastapi.exceptions import RequestValidationError\n-from fastapi.responses import JSONResponse, StreamingResponse\n-from pydantic import BaseModel, ValidationError\n-from termcolor import cprint\n-from typing_extensions import Annotated\n-\n-from llama_stack.distribution.distribution import builtin_automatically_routed_apis\n-\n-from llama_stack.providers.utils.telemetry.tracing import (\n-    end_trace,\n-    setup_logger,\n-    SpanStatus,\n-    start_trace,\n-)\n+from typing import Any, Dict, List, Optional\n+\n+from llama_models.llama3.api.datatypes import *  # noqa: F403\n+\n+from llama_stack.apis.models import *  # noqa: F403\n+from llama_stack.apis.shields import *  # noqa: F403\n+from llama_stack.apis.memory_banks import *  # noqa: F403\n+\n from llama_stack.distribution.datatypes import *  # noqa: F403\n \n-from llama_stack.distribution.request_headers import set_request_provider_data\n-from llama_stack.distribution.resolver import resolve_impls_with_routing\n-\n-from .endpoints import get_all_api_endpoints\n-\n-\n-def create_sse_event(data: Any) -> str:\n-    if isinstance(data, BaseModel):\n-        data = data.json()\n-    else:\n-        data = json.dumps(data)\n-\n-    return f\"data: {data}\\n\\n\"\n-\n-\n-async def global_exception_handler(request: Request, exc: Exception):\n-    traceback.print_exception(exc)\n-    http_exc = translate_exception(exc)\n-\n-    return JSONResponse(\n-        status_code=http_exc.status_code, content={\"error\": {\"detail\": http_exc.detail}}\n-    )\n-\n-\n-def translate_exception(exc: Exception) -> Union[HTTPException, RequestValidationError]:\n-    if isinstance(exc, ValidationError):\n-        exc = RequestValidationError(exc.raw_errors)\n-\n-    if isinstance(exc, RequestValidationError):\n-        return HTTPException(\n-            status_code=400,\n-            detail={\n-                \"errors\": [\n-                    {\n-                        \"loc\": list(error[\"loc\"]),\n-                        \"msg\": error[\"msg\"],\n-                        \"type\": error[\"type\"],\n-                    }\n-                    for error in exc.errors()\n-                ]\n-            },\n-        )\n-    elif isinstance(exc, ValueError):\n-        return HTTPException(status_code=400, detail=f\"Invalid value: {str(exc)}\")\n-    elif isinstance(exc, PermissionError):\n-        return HTTPException(status_code=403, detail=f\"Permission denied: {str(exc)}\")\n-    elif isinstance(exc, TimeoutError):\n-        return HTTPException(status_code=504, detail=f\"Operation timed out: {str(exc)}\")\n-    elif isinstance(exc, NotImplementedError):\n-        return HTTPException(status_code=501, detail=f\"Not implemented: {str(exc)}\")\n-    else:\n-        return HTTPException(\n-            status_code=500,\n-            detail=\"Internal server error: An unexpected error occurred.\",\n-        )\n-\n-\n-async def passthrough(\n-    request: Request,\n-    downstream_url: str,\n-    downstream_headers: Optional[Dict[str, str]] = None,\n-):\n-    await start_trace(request.path, {\"downstream_url\": downstream_url})\n-\n-    headers = dict(request.headers)\n-    headers.pop(\"host\", None)\n-    headers.update(downstream_headers or {})\n-\n-    content = await request.body()\n-\n-    client = httpx.AsyncClient()\n-    erred = False\n-    try:\n-        req = client.build_request(\n-            method=request.method,\n-            url=downstream_url,\n-            headers=headers,\n-            content=content,\n-            params=request.query_params,\n-        )\n-        response = await client.send(req, stream=True)\n-\n-        async def stream_response():\n-            async for chunk in response.aiter_raw(chunk_size=64):\n-                yield chunk\n-\n-            await response.aclose()\n-            await client.aclose()\n-\n-        return StreamingResponse(\n-            stream_response(),\n-            status_code=response.status_code,\n-            headers=dict(response.headers),\n-            media_type=response.headers.get(\"content-type\"),\n-        )\n-\n-    except httpx.ReadTimeout:\n-        erred = True\n-        return Response(content=\"Downstream server timed out\", status_code=504)\n-    except httpx.NetworkError as e:\n-        erred = True\n-        return Response(content=f\"Network error: {str(e)}\", status_code=502)\n-    except httpx.TooManyRedirects:\n-        erred = True\n-        return Response(content=\"Too many redirects\", status_code=502)\n-    except SSLError as e:\n-        erred = True\n-        return Response(content=f\"SSL error: {str(e)}\", status_code=502)\n-    except httpx.HTTPStatusError as e:\n-        erred = True\n-        return Response(content=str(e), status_code=e.response.status_code)\n-    except Exception as e:\n-        erred = True\n-        return Response(content=f\"Unexpected error: {str(e)}\", status_code=500)\n-    finally:\n-        await end_trace(SpanStatus.OK if not erred else SpanStatus.ERROR)\n-\n-\n-def handle_sigint(app, *args, **kwargs):\n-    print(\"SIGINT or CTRL-C detected. Exiting gracefully...\")\n-\n-    async def run_shutdown():\n-        for impl in app.__llama_stack_impls__.values():\n-            print(f\"Shutting down {impl}\")\n-            await impl.shutdown()\n-\n-    asyncio.run(run_shutdown())\n-\n-    loop = asyncio.get_event_loop()\n-    for task in asyncio.all_tasks(loop):\n-        task.cancel()\n-\n-    loop.stop()\n-\n-\n-@asynccontextmanager\n-async def lifespan(app: FastAPI):\n-    print(\"Starting up\")\n-    yield\n-\n-    print(\"Shutting down\")\n-    for impl in app.__llama_stack_impls__.values():\n-        await impl.shutdown()\n-\n-\n-def create_dynamic_passthrough(\n-    downstream_url: str, downstream_headers: Optional[Dict[str, str]] = None\n-):\n-    async def endpoint(request: Request):\n-        return await passthrough(request, downstream_url, downstream_headers)\n-\n-    return endpoint\n-\n-\n-def is_streaming_request(func_name: str, request: Request, **kwargs):\n-    # TODO: pass the api method and punt it to the Protocol definition directly\n-    return kwargs.get(\"stream\", False)\n-\n-\n-async def maybe_await(value):\n-    if inspect.iscoroutine(value):\n-        return await value\n-    return value\n-\n-\n-async def sse_generator(event_gen):\n-    try:\n-        async for item in event_gen:\n-            yield create_sse_event(item)\n-            await asyncio.sleep(0.01)\n-    except asyncio.CancelledError:\n-        print(\"Generator cancelled\")\n-        await event_gen.aclose()\n-    except Exception as e:\n-        traceback.print_exception(e)\n-        yield create_sse_event(\n-            {\n-                \"error\": {\n-                    \"message\": str(translate_exception(e)),\n-                },\n-            }\n-        )\n-    finally:\n-        await end_trace()\n-\n-\n-def create_dynamic_typed_route(func: Any, method: str):\n-\n-    async def endpoint(request: Request, **kwargs):\n-        await start_trace(func.__name__)\n-\n-        set_request_provider_data(request.headers)\n-\n-        is_streaming = is_streaming_request(func.__name__, request, **kwargs)\n-        try:\n-            if is_streaming:\n-                return StreamingResponse(\n-                    sse_generator(func(**kwargs)), media_type=\"text/event-stream\"\n-                )\n-            else:\n-                value = func(**kwargs)\n-                return await maybe_await(value)\n-        except Exception as e:\n-            traceback.print_exception(e)\n-            raise translate_exception(e) from e\n-        finally:\n-            await end_trace()\n-\n-    sig = inspect.signature(func)\n-    new_params = [\n-        inspect.Parameter(\n-            \"request\", inspect.Parameter.POSITIONAL_OR_KEYWORD, annotation=Request\n-        )\n-    ]\n-    new_params.extend(sig.parameters.values())\n-\n-    if method == \"post\":\n-        # make sure every parameter is annotated with Body() so FASTAPI doesn't\n-        # do anything too intelligent and ask for some parameters in the query\n-        # and some in the body\n-        new_params = [new_params[0]] + [\n-            param.replace(annotation=Annotated[param.annotation, Body(..., embed=True)])\n-            for param in new_params[1:]\n-        ]\n-\n-    endpoint.__signature__ = sig.replace(parameters=new_params)\n-\n-    return endpoint\n-\n-\n-def main(\n-    yaml_config: str = \"llamastack-run.yaml\",\n-    port: int = 5000,\n-    disable_ipv6: bool = False,\n-):\n-    with open(yaml_config, \"r\") as fp:\n-        config = StackRunConfig(**yaml.safe_load(fp))\n-\n-    app = FastAPI()\n-\n-    impls = asyncio.run(resolve_impls_with_routing(config))\n-    if Api.telemetry in impls:\n-        setup_logger(impls[Api.telemetry])\n-\n-    all_endpoints = get_all_api_endpoints()\n-\n-    if config.apis:\n-        apis_to_serve = set(config.apis)\n-    else:\n-        apis_to_serve = set(impls.keys())\n-\n-    for inf in builtin_automatically_routed_apis():\n-        apis_to_serve.add(inf.routing_table_api.value)\n-\n-    apis_to_serve.add(\"inspect\")\n-    for api_str in apis_to_serve:\n-        api = Api(api_str)\n-\n-        endpoints = all_endpoints[api]\n-        impl = impls[api]\n-\n-        if is_passthrough(impl.__provider_spec__):\n-            for endpoint in endpoints:\n-                url = impl.__provider_config__.url.rstrip(\"/\") + endpoint.route\n-                getattr(app, endpoint.method)(endpoint.route)(\n-                    create_dynamic_passthrough(url)\n-                )\n-        else:\n-            for endpoint in endpoints:\n-                if not hasattr(impl, endpoint.name):\n-                    # ideally this should be a typing violation already\n-                    raise ValueError(\n-                        f\"Could not find method {endpoint.name} on {impl}!!\"\n-                    )\n-\n-                impl_method = getattr(impl, endpoint.name)\n-\n-                getattr(app, endpoint.method)(endpoint.route, response_model=None)(\n-                    create_dynamic_typed_route(\n-                        impl_method,\n-                        endpoint.method,\n-                    )\n-                )\n \n-        cprint(f\"Serving API {api_str}\", \"white\", attrs=[\"bold\"])\n-        for endpoint in endpoints:\n-            cprint(f\" {endpoint.method.upper()} {endpoint.route}\", \"white\")\n+def get_impl_api(p: Any) -> Api:\n+    return p.__provider_spec__.api\n+\n \n-    print(\"\")\n-    app.exception_handler(RequestValidationError)(global_exception_handler)\n-    app.exception_handler(Exception)(global_exception_handler)\n-    signal.signal(signal.SIGINT, functools.partial(handle_sigint, app))\n+async def register_object_with_provider(obj: RoutableObject, p: Any) -> None:\n+    api = get_impl_api(p)\n+    if api == Api.inference:\n+        await p.register_model(obj)\n+    elif api == Api.safety:\n+        await p.register_shield(obj)\n+    elif api == Api.memory:\n+        await p.register_memory_bank(obj)\n \n-    app.__llama_stack_impls__ = impls\n \n-    import uvicorn\n+Registry = Dict[str, List[RoutableObjectWithProvider]]\n \n-    # FYI this does not do hot-reloads\n-    listen_host = \"::\" if not disable_ipv6 else \"0.0.0.0\"\n-    print(f\"Listening on {listen_host}:{port}\")\n-    uvicorn.run(app, host=listen_host, port=port)\n \n+# TODO: this routing table maintains state in memory purely. We need to\n+# add persistence to it when we add dynamic registration of objects.\n+class CommonRoutingTableImpl(RoutingTable):\n+    def __init__(\n+        self,\n+        impls_by_provider_id: Dict[str, RoutedProtocol],\n+    ) -> None:\n+        self.impls_by_provider_id = impls_by_provider_id\n+\n+    async def initialize(self) -> None:\n+        self.registry: Registry = {}\n+\n+        def add_objects(objs: List[RoutableObjectWithProvider]) -> None:\n+            for obj in objs:\n+                if obj.identifier not in self.registry:\n+                    self.registry[obj.identifier] = []\n+\n+                self.registry[obj.identifier].append(obj)\n+\n+        for pid, p in self.impls_by_provider_id.items():\n+            api = get_impl_api(p)\n+            if api == Api.inference:\n+                p.model_store = self\n+                models = await p.list_models()\n+                add_objects(\n+                    [ModelDefWithProvider(**m.dict(), provider_id=pid) for m in models]\n+                )\n+\n+            elif api == Api.safety:\n+                p.shield_store = self\n+                shields = await p.list_shields()\n+                add_objects(\n+                    [\n+                        ShieldDefWithProvider(**s.dict(), provider_id=pid)\n+                        for s in shields\n+                    ]\n+                )\n \n-if __name__ == \"__main__\":\n-    fire.Fire(main)\n+            elif api == Api.memory:\n+                p.memory_bank_store = self\n+                memory_banks = await p.list_memory_banks()\n+\n+                # do in-memory updates due to pesky Annotated unions\n+                for m in memory_banks:\n+                    m.provider_id = pid\n+\n+                add_objects(memory_banks)\n+\n+    async def shutdown(self) -> None:\n+        for p in self.impls_by_provider_id.values():\n+            await p.shutdown()\n+\n+    def get_provider_impl(\n+        self, routing_key: str, provider_id: Optional[str] = None\n+    ) -> Any:\n+        if routing_key not in self.registry:\n+            raise ValueError(f\"`{routing_key}` not registered\")\n+\n+        objs = self.registry[routing_key]\n+        for obj in objs:\n+            if not provider_id or provider_id == obj.provider_id:\n+                return self.impls_by_provider_id[obj.provider_id]\n+\n+        raise ValueError(f\"Provider not found for `{routing_key}`\")\n+\n+    def get_object_by_identifier(\n+        self, identifier: str\n+    ) -> Optional[RoutableObjectWithProvider]:\n+        objs = self.registry.get(identifier, [])\n+        if not objs:\n+            return None\n+\n+        # kind of ill-defined behavior here, but we'll just return the first one\n+        return objs[0]\n+\n+    async def register_object(self, obj: RoutableObjectWithProvider):\n+        # if provider_id is not specified, we'll pick an arbitrary one from existing entries\n+        if not obj.provider_id and len(self.impls_by_provider_id) > 0:\n+            obj.provider_id = list(self.impls_by_provider_id.keys())[0]\n+\n+        if obj.provider_id not in self.impls_by_provider_id:\n+            raise ValueError(f\"Provider `{obj.provider_id}` not found\")\n+        \n+        p = self.impls_by_provider_id[obj.provider_id]\n+        await register_object_with_provider(obj, p)\n+        \n+        # Direct registry modification without checking current state\n+        if obj.identifier not in self.registry:\n+            self.registry[obj.identifier] = []\n+        self.registry[obj.identifier].append(obj)\n+        # TODO: persist this to a store\n+\n+\n+class ModelsRoutingTable(CommonRoutingTableImpl, Models):\n+    async def list_models(self) -> List[ModelDefWithProvider]:\n+        objects = []\n+        for objs in self.registry.values():\n+            objects.extend(objs)\n+        return objects\n+\n+    async def get_model(self, identifier: str) -> Optional[ModelDefWithProvider]:\n+        return self.get_object_by_identifier(identifier)\n+\n+    async def register_model(self, model: ModelDefWithProvider) -> None:\n+        await self.register_object(model)\n+\n+\n+class ShieldsRoutingTable(CommonRoutingTableImpl, Shields):\n+    async def list_shields(self) -> List[ShieldDef]:\n+        objects = []\n+        for objs in self.registry.values():\n+            objects.extend(objs)\n+        return objects\n+\n+    async def get_shield(self, shield_type: str) -> Optional[ShieldDefWithProvider]:\n+        return self.get_object_by_identifier(shield_type)\n+\n+    async def register_shield(self, shield: ShieldDefWithProvider) -> None:\n+        await self.register_object(shield)\n+\n+\n+class MemoryBanksRoutingTable(CommonRoutingTableImpl, MemoryBanks):\n+    async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]:\n+        objects = []\n+        for objs in self.registry.values():\n+            objects.extend(objs)\n+        return objects\n+\n+    async def get_memory_bank(\n+        self, identifier: str\n+    ) -> Optional[MemoryBankDefWithProvider]:\n+        return self.get_object_by_identifier(identifier)\n+\n+    async def register_memory_bank(\n+        self, memory_bank: MemoryBankDefWithProvider\n+    ) -> None:\n+        await self.register_object(memory_bank)\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "53",
        "type": "3",
        "number": 102,
        "head_branch": "error-037-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces changes to the routing paths of the web methods within the `Inference` protocol in the `inference.py` file. Specifically, the endpoints for three methods\u2014`completion`, `chat_completion`, and `embeddings`\u2014have been updated to include a version prefix. The new routes are as follows: the `completion` method now uses `/v1/completion`, the `chat_completion` method uses `/v1/chat_completion`, and the `embeddings` method uses `/v1/embeddings`. \n\nThese modifications do not alter the method signatures or the internal logic of the methods; they strictly pertain to the routing paths. Additionally, there are updates to the method signatures for `completion`, `chat_completion`, and `embeddings`, but the signatures themselves remain unchanged in terms of their parameters and return types. This change is aimed at implementing a versioning scheme for the API endpoints, which may assist in future API management.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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=\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [],
        "file_changes": [
            {
                "file": "project/api/llama_stack/apis/inference/inference.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " \n     # This method is not `async def` because it can result in either an\n     # `AsyncGenerator` or a `CompletionResponse` depending on the value of `stream`.\n-    @webmethod(route=\"/inference/completion\")\n+    @webmethod(route=\"/v1/completion\")\n     def completion(\n         self,\n         model: str,\n"
                    },
                    {
                        "id": 2,
                        "content": " \n     # This method is not `async def` because it can result in either an\n     # `AsyncGenerator` or a `ChatCompletionResponse` depending on the value of `stream`.\n-    @webmethod(route=\"/inference/chat_completion\")\n+    @webmethod(route=\"/v1/chat_completion\")\n     def chat_completion(\n         self,\n         model: str,\n"
                    },
                    {
                        "id": 3,
                        "content": "         logprobs: Optional[LogProbConfig] = None,\n     ) -> Union[ChatCompletionResponse, ChatCompletionResponseStreamChunk]: ...\n \n-    @webmethod(route=\"/inference/embeddings\")\n+    @webmethod(route=\"/v1/embeddings\")\n     async def embeddings(\n         self,\n         model: str,\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "54",
        "type": "3",
        "number": 103,
        "head_branch": "error-005-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces modifications to the `MetaReferenceAgentsImpl` class located in `agents.py`. Key changes are made to the `create_agent_turn` and `_create_agent_turn_streaming` methods. \n\nIn the `create_agent_turn` method, the condition for returning a streaming response has been updated from `if stream:` to `if stream or attachments is None:`. This adjustment allows for a streaming response to be returned when either streaming is enabled or when no attachments are provided. If neither condition is satisfied, a `NotImplementedError` is raised, indicating that non-streaming agent turns are not implemented.\n\nIn the `_create_agent_turn_streaming` method, a new conditional check has been introduced: `if not request.stream and request.attachments:`. This line specifies that the agent will only be retrieved if the request is not for streaming and attachments are present.\n\nOverall, these changes refine the logic governing the creation and streaming of agent turns, particularly concerning the handling of attachments. The method signatures remain unchanged, but the internal logic has been modified to enhance control flow.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "             attachments=attachments,\n             stream=True,\n         )\n-        if stream:\n+        if stream or attachments is None:\n             return self._create_agent_turn_streaming(request)\n         else:\n             raise NotImplementedError(\"Non-streaming agent turns not yet implemented\")\n"
                    },
                    {
                        "id": 2,
                        "content": "         self,\n         request: AgentTurnCreateRequest,\n     ) -> AsyncGenerator:\n-        agent = await self.get_agent(request.agent_id)\n+        if not request.stream and request.attachments:\n+            agent = await self.get_agent(request.agent_id)\n         async for event in agent.create_and_execute_turn(request):\n             yield event\n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "55",
        "type": "3",
        "number": 104,
        "head_branch": "error-003-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces several modifications to the `vector_store.py` file located in the `project/api/llama_stack/providers/utils/memory` directory. Key changes include the enhancement of the `get_embedding_model` function, which now checks for the presence of the embedding model in the `EMBEDDING_MODELS` dictionary before loading it, thereby eliminating unnecessary loading messages and instantiation. \n\nThe `content_from_data` function has been updated to improve encoding detection by utilizing the `chardet` library when the encoding is unspecified in the data URL. This aims to enhance the handling of various data formats, particularly for text files. The `content_from_doc` function remains structurally unchanged but continues to support both data URLs and standard URLs.\n\nThe overall control flow and error handling have been refined to ensure that invalid data URLs trigger appropriate exceptions. The `make_overlapped_chunks` function and the `EmbeddingIndex` abstract class have not undergone any structural changes, while the `BankWithIndex` class retains its method structure but benefits from the improved loading and processing logic. These changes collectively focus on optimizing model loading and enhancing data handling robustness.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " def get_embedding_model(model: str) -> \"SentenceTransformer\":\n     global EMBEDDING_MODELS\n \n-    loaded_model = EMBEDDING_MODELS.get(model)\n-    if loaded_model is not None:\n-        return loaded_model\n+    if model not in EMBEDDING_MODELS:\n+        print(f\"Loading sentence transformer for {model}...\")\n+        from sentence_transformers import SentenceTransformer\n+        loaded_model = SentenceTransformer(model)\n+        EMBEDDING_MODELS[model] = loaded_model\n \n-    print(f\"Loading sentence transformer for {model}...\")\n-    from sentence_transformers import SentenceTransformer\n-\n-    loaded_model = SentenceTransformer(model)\n-    EMBEDDING_MODELS[model] = loaded_model\n+    loaded_model = EMBEDDING_MODELS[model]\n     return loaded_model\n \n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "56",
        "type": "3",
        "number": 105,
        "head_branch": "error-004-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the pull request focus on the `reward_scoring.py` file within the `project/api/llama_stack/apis/reward_scoring` directory. A new attribute, `score_threshold`, has been introduced to the `RewardScoringRequest` class, with a default value set to `0.0`. This attribute allows users to specify a threshold for scoring requests. \n\nAdditionally, the `reward_score` method in the `RewardScoring` protocol has been updated to include the `score_threshold` parameter, which also defaults to `0.0`. The implementation of this method has been modified to incorporate logic that utilizes the new parameter. Specifically, if the `dialog_generations` input is empty, the method will return an empty `RewardScoringResponse`. Furthermore, if the `score_threshold` is greater than or equal to `0`, the method will return a `RewardScoringResponse` containing an empty list of `ScoredDialogGenerations`. These changes affect the control flow of the scoring process based on the specified threshold.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcEj2wAbb5AokAI7Y0riQ4rjedJAARACCeCzU0dChXAACJBJo3mCIZPD4FGABEvAkAO6UYABMMZAVaMiI2ALM6jT0cuGwntj5fOSwOWBoNQBe6Bj0qPBKGOIAZuVd8ri9kBhOApSQAIwADACsGpAAkmEMmJA76AwM0vn06xT42ESwPZ6L+L74FfAMERIABVGwAGS4AG1YLhcNxEBwAPRIojqWCtDRMZhIphKKiqdTaJEAcV+8zAtjAABFqE0SLgkdwfN4kYcjgBdAAUsPhiJRaPWmOxuMUykJuGJZO8FKptMl+UZzN8bOOAEpTu4NkpEAwKPBuOJ8FhePgykoZtN4FcaMh1tQeqhlX4AsFQg0mjgCMxqNact55HqSMlVl9wvhfpAAAbpPHitSS+BIkg1EglLLlKoUKOnABy+HQtFo6kKGBykCUie8yCKCgWJAAHmEVG8wutPAxhkDpPw+MyKNx8Pl+Isw87/EEQohmwEUBgGN5sBa52GdXqDUaMBojPojCYoGR6PhR2hEsQyMpOgpWOwuLx+MJROIpDJA2KCQmtDp9PuYAhkKg1yngQ57kFQV7YmwCxcFQFSQC0ziuDcb74ion7aLoYCGMY4BgEYpoiGISJoNwSa+GgPoAPrTmgDAANbEaRiBIgEjQULQ1FMPqQIsZULgcbqRSAkQGjcLIHAGDEUkGBYkBxGcoGXtECE+khx6MF2pCIFqnjFoso6Argry0NgDzIMwijwMsNqlnaBbttGNh8ex9hccJTlutOUZTPQUZOWxtCuUJQLeaaBBMH4gLRqx/GccFIliTmcmbJU6BwvqAh4CQAA00aCQElEvNIsDklGuWAUW0QEGGfnOYFblAh5U64N5C5NIg5UYCWOTwOMVUFmgFYkIsp7eGE2SLp46lRgcGgHElcCoJ2mCkOgfwVHaGyus14bwdwohWfIg1FYgJUypAPx8PlwnbgYZxYA5tUBUF3FEKFrzhb8uWPTF7FxQE3lsOsijwfARBlrg2Czqg2DcLQIa7YCC5Lp4DnkHBUb5SQhWwAEp2lTwLgUQylC5TkiAFkoI0+OIQK7TNc0LRsQMlbQADkAHMNwURQZKm4oOZlnLP1c7I0okDePgaIMD0DpdrQUSbZ46OE1QQOUKcZyjlGxY5FLlGkGBvrGog3moCQXO4LI5VhAEkMUBgyDXBbhryE9/Eve50iDo7JBJVreVcdjJ1nbQZvIEQATJHw9pYLWk7ltVM1lSgtsMlDjvoI5dWe413sm37daJl1dPO5b8jeEgYTTS9dDUvAetECSF7gbZTOzAsxmmT2KtMJ3kaLJLcECPSR4PRsE0hCOgdFMHuPFaVZP6U+wmQCVcF4z7+RO7OhtKV0o/8OPniAsylzGsWm6ILd5iWHEY2XrZu0OUobWtyb0+NoOFBXrWzICJXGW7ASzSB3JAAAsgyVmoNwbUChn0OGIYuA62GhOAK/0SBcnyN4RYuVdaSyIAbFuxtHZcHBFXKE9dG7NyNlfDkuULJKG8FwacFA1S6D0KCLqxooT+Q9g1IgTlEBbxIByDgZssBtUQMgd2LkBHeUAEmE0YqZoNiljLBJAcF4IbgQohtDbJkIoVQghNDH4m3oZARhmiWFGVyljHGeNQ5cEHvgB0ABeSAs0DjsKwlw0svCc4CKESIsREjGDeHatnZ68iDBQAAGouAbgAzwaBKq0GQfYkO5JnGS3cZ4xmK4pEyL4XI+KTVQhRkktJWJWE9y4UgIeaewFCB73AtESCt5/BoDgipFw8huhxg/OoL8mE9C/kguoSicxECUVKJmOg1FJQ/1GXUiA+wADMBxRA1D2DUfSNQGAAA4VAkHWbQAA7AATkuXsWgexFgADZDm0COMNA5JBLnnIEOc1JKycJrImbgKZtAZlzMqAsxp+ggA\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Add validation for score_threshold.**\n\nThe `score_threshold` parameter should have validation to ensure it's within a reasonable range, as negative thresholds might not make sense in a scoring context.\n\nAdd field validation using Pydantic:\n\n```diff\n-    score_threshold: float = 0.0\n+    score_threshold: float = 0.0\n+    \n+    @validator('score_threshold')\n+    def validate_threshold(cls, v):\n+        if v < 0:\n+            raise ValueError(\"score_threshold must be non-negative\")\n+        return v\n```\n\nDon't forget to import the validator:\n```python\nfrom pydantic import BaseModel, validator\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/reward_scoring/reward_scoring.py",
                "position": 4
            },
            {
                "body": "_:warning: Potential issue_\n\n**Logic issues in reward_score implementation.**\n\nThere are several issues with the current implementation:\n1. The condition `score_threshold >= 0` will always be true since we're defaulting to 0.0\n2. The implementation returns empty dialog and scored_generations regardless of input\n3. The original scoring logic appears to be missing\n\nThe implementation needs to be revised to actually score the generations and apply the threshold. Here's a suggested structure:\n\n```diff\n     def reward_score(\n         self,\n         dialog_generations: List[DialogGenerations],\n         model: str, \n         score_threshold: float = 0.0,\n     ) -> Union[RewardScoringResponse]:\n         if not dialog_generations:\n             return RewardScoringResponse(scored_generations=[])\n         \n-        if score_threshold >= 0:\n-            return RewardScoringResponse(\n-                scored_generations=[\n-                    ScoredDialogGenerations(dialog=[], scored_generations=[])\n-                ])\n+        scored_results = []\n+        for dg in dialog_generations:\n+            # Score the generations using the model\n+            scored_messages = []\n+            for gen in dg.sampled_generations:\n+                score = self._compute_score(gen, model)  # Implementation needed\n+                if score >= score_threshold:\n+                    scored_messages.append(ScoredMessage(message=gen, score=score))\n+            \n+            if scored_messages:  # Only include if there are generations meeting threshold\n+                scored_results.append(\n+                    ScoredDialogGenerations(\n+                        dialog=dg.dialog,\n+                        scored_generations=scored_messages\n+                    )\n+                )\n+        \n+        return RewardScoringResponse(scored_generations=scored_results)\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/reward_scoring/reward_scoring.py",
                "position": 24
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/apis/reward_scoring/reward_scoring.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " \n     dialog_generations: List[DialogGenerations]\n     model: str\n+    score_threshold: float = 0.0\n \n \n @json_schema_type\n"
                    },
                    {
                        "id": 2,
                        "content": "     def reward_score(\n         self,\n         dialog_generations: List[DialogGenerations],\n-        model: str,\n-    ) -> Union[RewardScoringResponse]: ...\n+        model: str, \n+        score_threshold: float = 0.0,\n+    ) -> Union[RewardScoringResponse]:\n+        if not dialog_generations:\n+            return RewardScoringResponse(scored_generations=[])\n+        \n+        if score_threshold >= 0:\n+            return RewardScoringResponse(\n+                scored_generations=[\n+                    ScoredDialogGenerations(dialog=[], scored_generations=[])\n+                ])\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "57",
        "type": "3",
        "number": 106,
        "head_branch": "error-018-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the pull request primarily focus on the `pgvector.py` file, specifically enhancing the `PGVectorMemoryAdapter` class. A new private dictionary named `_bank_metadata_cache` has been introduced to cache metadata related to memory banks. This cache is utilized across multiple methods, including `register_memory_bank`, `list_memory_banks`, `insert_documents`, and `query_documents`.\n\nIn the `register_memory_bank` method, after updating the memory bank model, the corresponding entry is stored in the `_bank_metadata_cache`, and a commit operation is added to persist changes to the database. The `list_memory_banks` method has been updated to return a list that merges entries from the `_bank_metadata_cache` with loaded models, ensuring the output reflects the latest state of memory banks.\n\nAdditionally, the `insert_documents` and `query_documents` methods now verify the existence of the bank ID in the `_bank_metadata_cache` before accessing the corresponding index. This enhancement ensures that only registered memory banks are considered valid, thereby improving error handling and maintaining the integrity of database interactions. Overall, these modifications enhance internal state management and operational reliability within the `PGVectorMemoryAdapter` class.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Add error handling for database commit operation**\n\nThe commit operation should be wrapped in a try-except block to handle potential database errors gracefully.\n\n```diff\n-        self._bank_metadata_cache[memory_bank.identifier] = memory_bank\n-\n-        index = BankWithIndex(\n-            bank=memory_bank,\n-            index=PGVectorIndex(memory_bank, ALL_MINILM_L6_V2_DIMENSION, self.cursor),\n-        )\n-        self.cache[memory_bank.identifier] = index\n-        self.conn.commit()\n+        try:\n+            self._bank_metadata_cache[memory_bank.identifier] = memory_bank\n+\n+            index = BankWithIndex(\n+                bank=memory_bank,\n+                index=PGVectorIndex(memory_bank, ALL_MINILM_L6_V2_DIMENSION, self.cursor),\n+            )\n+            self.cache[memory_bank.identifier] = index\n+            self.conn.commit()\n+        except psycopg2.Error as e:\n+            self.conn.rollback()\n+            raise RuntimeError(f\"Failed to register memory bank: {str(e)}\") from e\n```\n\n\nAlso applies to: 179-179\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/memory/pgvector/pgvector.py",
                "position": 12
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consolidate duplicate bank validation logic**\n\nThe bank validation logic is duplicated in both `insert_documents` and `query_documents`. Consider extracting this into a private helper method.\n\n```diff\n+    def _get_bank_index(self, bank_id: str) -> BankWithIndex:\n+        \"\"\"Get bank index from cache if bank exists.\"\"\"\n+        index = self.cache.get(bank_id) if bank_id in self._bank_metadata_cache else None\n+        if not index:\n+            raise ValueError(f\"Bank {bank_id} not found\")\n+        return index\n+\n     async def insert_documents(\n         self,\n         bank_id: str,\n         documents: List[MemoryBankDocument],\n         ttl_seconds: Optional[int] = None,\n     ) -> None:\n-        index = self.cache.get(bank_id) if bank_id in self._bank_metadata_cache else None\n-        if not index:\n-            raise ValueError(f\"Bank {bank_id} not found\")\n+        index = self._get_bank_index(bank_id)\n         await index.insert_documents(documents)\n\n     async def query_documents(\n         self,\n         bank_id: str,\n         query: InterleavedTextMedia,\n         params: Optional[Dict[str, Any]] = None,\n     ) -> QueryDocumentsResponse:\n-        index = self.cache.get(bank_id) if bank_id in self._bank_metadata_cache else None\n-        if not index:\n-            raise ValueError(f\"Bank {bank_id} not found\")\n+        index = self._get_bank_index(bank_id)\n         return await index.query_documents(query, params)\n```\n\n\nAlso applies to: 210-213\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/memory/pgvector/pgvector.py",
                "position": 40
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/memory/pgvector/pgvector.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "         self.cursor = None\n         self.conn = None\n         self.cache = {}\n+        self._bank_metadata_cache = {}\n \n     async def initialize(self) -> None:\n         try:\n"
                    },
                    {
                        "id": 2,
                        "content": "                 (memory_bank.identifier, memory_bank),\n             ],\n         )\n+        self._bank_metadata_cache[memory_bank.identifier] = memory_bank\n \n         index = BankWithIndex(\n             bank=memory_bank,\n             index=PGVectorIndex(memory_bank, ALL_MINILM_L6_V2_DIMENSION, self.cursor),\n         )\n         self.cache[memory_bank.identifier] = index\n+        self.conn.commit()\n \n     async def list_memory_banks(self) -> List[MemoryBankDef]:\n         banks = load_models(self.cursor, MemoryBankDef)\n"
                    },
                    {
                        "id": 3,
                        "content": "                     index=PGVectorIndex(bank, ALL_MINILM_L6_V2_DIMENSION, self.cursor),\n                 )\n                 self.cache[bank.identifier] = index\n-        return banks\n+        return [self._bank_metadata_cache.get(bank.identifier, bank) for bank in banks]\n \n     async def insert_documents(\n         self,\n"
                    },
                    {
                        "id": 4,
                        "content": "         documents: List[MemoryBankDocument],\n         ttl_seconds: Optional[int] = None,\n     ) -> None:\n-        index = self.cache.get(bank_id, None)\n+        index = self.cache.get(bank_id) if bank_id in self._bank_metadata_cache else None\n         if not index:\n             raise ValueError(f\"Bank {bank_id} not found\")\n \n"
                    },
                    {
                        "id": 5,
                        "content": "         query: InterleavedTextMedia,\n         params: Optional[Dict[str, Any]] = None,\n     ) -> QueryDocumentsResponse:\n-        index = self.cache.get(bank_id, None)\n+        index = self.cache.get(bank_id) if bank_id in self._bank_metadata_cache else None\n         if not index:\n             raise ValueError(f\"Bank {bank_id} not found\")\n \n"
                    }
                ]
            }
        ]
    }
]