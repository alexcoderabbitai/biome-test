[
    {
        "id": "20",
        "type": "2",
        "number": 69,
        "head_branch": "error-034-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the pull request primarily affect the `faiss.py` file within the project. A significant restructuring has occurred, including the removal of the `FaissIndex` class and the introduction of an abstract `EmbeddingIndex` class, which establishes two abstract methods: `add_chunks` and `query`. This shift aims to enhance the abstraction in embedding management.\n\nNew functions have been added to facilitate various operations, such as `get_embedding_model` for loading and caching sentence transformer models, `parse_data_url` for validating data URLs, and `content_from_data` for extracting content from different data types, including base64 and PDFs. Additionally, the `content_from_doc` function has been introduced for retrieving content from documents via data URLs and HTTP requests.\n\nThe `make_overlapped_chunks` function is also included, which generates overlapping text chunks from documents. A new data class, `BankWithIndex`, has been defined to encapsulate a memory bank and its embedding index. The `FaissMemoryImpl` class has undergone substantial modifications, integrating new chunk processing logic and updating its methods to align with the new structure for document insertion and query processing. Overall, these changes represent a comprehensive overhaul of the embedding handling logic.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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=\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Add exception handling for HTTP requests**\n\nWhen making HTTP requests using `httpx`, network errors or invalid URLs can cause exceptions. Adding exception handling will make the code more robust.\n\n\n\nApply this diff to handle exceptions during HTTP requests:\n\n```diff\n             async with httpx.AsyncClient() as client:\n-                r = await client.get(doc.content.uri)\n-                return r.text\n+                try:\n+                    r = await client.get(doc.content.uri)\n+                    r.raise_for_status()\n+                    return r.text\n+                except httpx.HTTPError as e:\n+                    cprint(f\"Error fetching content from {doc.content.uri}: {e}\", color=\"red\")\n+                    return \"\"\n\n...\n\n             async with httpx.AsyncClient() as client:\n-                r = await client.get(doc.content)\n-                return r.text\n+                try:\n+                    r = await client.get(doc.content)\n+                    r.raise_for_status()\n+                    return r.text\n+                except httpx.HTTPError as e:\n+                    cprint(f\"Error fetching content from {doc.content}: {e}\", color=\"red\")\n+                    return \"\"\n```\n\n\nAlso applies to: 113-115\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 120
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Expand support for additional MIME types**\n\nCurrently, the function `content_from_data` only handles 'text' and 'application/pdf' MIME types. Other common types like 'application/msword' or 'application/vnd.openxmlformats-officedocument.wordprocessingml.document' are not supported.\n\n\n\nConsider adding support for additional MIME types or handling them more explicitly. For unsupported types, provide a clear message or handle them appropriately.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 110
            },
            {
                "body": "_:warning: Potential issue_\n\n**Handle possible None value from chardet.detect**\n\nThe `chardet.detect(data)` function may return `None` for the encoding, which could lead to an exception when decoding. Ensure that the detected encoding is valid before using it.\n\n\n\nApply this diff to handle potential `None` values:\n\n```diff\n         detected = chardet.detect(data)\n-        encoding = detected[\"encoding\"]\n+        detected_encoding = detected.get(\"encoding\", \"utf-8\")\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n        detected = chardet.detect(data)\n        detected_encoding = detected.get(\"encoding\", \"utf-8\")\n\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 95
            },
            {
                "body": "_:warning: Potential issue_\n\n**Handle exceptions during embedding model encoding**\n\nThe `model.encode` function might raise exceptions if the input is invalid or if the model encounters an error. Add exception handling to manage potential issues.\n\n\n\nApply this diff to handle exceptions during encoding:\n\n```diff\n     model = get_embedding_model(self.bank.embedding_model)\n-    query_vector = model.encode([query_str])[0].astype(np.float32)\n+    try:\n+        query_vector = model.encode([query_str])[0].astype(np.float32)\n+    except Exception as e:\n+        cprint(f\"Error encoding query: {e}\", color=\"red\")\n+        return QueryDocumentsResponse(documents=[], total_results=0)\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 273
            },
            {
                "body": "_:warning: Potential issue_\n\n**Use a stable hash function for consistent ordering**\n\nSorting chunks using the built-in `hash()` function may result in inconsistent ordering across different runs due to hash randomization. Use a stable hash function like `hashlib.md5` for consistent results.\n\n\n\nApply this diff to use a stable hash function:\n\n```diff\n     # Unstable order of chunks\n-    chunks = sorted(chunks, key=lambda x: hash(x.content))\n+    import hashlib\n+    chunks = sorted(chunks, key=lambda x: hashlib.md5(x.content.encode()).hexdigest())\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 242
            },
            {
                "body": "_:warning: Potential issue_\n\n**Prevent potential infinite loop in 'make_overlapped_chunks'**\n\nIf `window_len - overlap_len` is less than or equal to zero, the loop can become infinite, causing the program to hang. Ensure that `window_len` is greater than `overlap_len`.\n\n\n\nAdd a validation check before the loop:\n\n```diff\n     tokens = tokenizer.encode(text, bos=False, eos=False)\n\n+    if window_len <= overlap_len:\n+        raise ValueError(\"window_len must be greater than overlap_len\")\n \n     chunks = []\n     for i in range(0, len(tokens), window_len - overlap_len):\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    tokens = tokenizer.encode(text, bos=False, eos=False)\n\n    if window_len <= overlap_len:\n        raise ValueError(\"window_len must be greater than overlap_len\")\n\n    chunks = []\n    for i in range(0, len(tokens), window_len - overlap_len):\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 141
            },
            {
                "body": "_:warning: Potential issue_\n\n**Avoid reassigning the 'encoding' variable to prevent confusion**\n\nThe variable `encoding` is reassigned multiple times, which can lead to confusion and potential errors. It's also possible that `encoding` becomes `None`, causing issues when decoding.\n\n\n\nApply this diff to refactor variable usage and ensure proper encoding handling:\n\n```diff\n         data = data.encode(encoding)\n\n-        encoding = parts[\"encoding\"]\n-        if not encoding:\n+        detected_encoding = parts[\"encoding\"]\n+        if not detected_encoding:\n             detected = chardet.detect(data)\n-            encoding = detected[\"encoding\"]\n+            detected_encoding = detected[\"encoding\"]\n\n         mime_type = parts[\"mimetype\"]\n         mime_category = mime_type.split(\"/\")[0]\n         if mime_category == \"text\":\n             # For text-based files (including CSV, MD)\n-            return data.decode(encoding)\n+            return data.decode(detected_encoding)\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 95
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Refactor duplicate code to improve maintainability**\n\nThere is duplicated logic in handling `doc.content` and `doc.content.uri`, especially when processing data URLs and making HTTP requests. Refactoring the code can reduce redundancy and improve readability.\n\n\n\nConsider refactoring the function as follows:\n\n```diff\n async def content_from_doc(doc: MemoryBankDocument) -> str:\n-    if isinstance(doc.content, URL):\n-        if doc.content.uri.startswith(\"data:\"):\n-            return content_from_data(doc.content.uri)\n-        else:\n-            async with httpx.AsyncClient() as client:\n-                r = await client.get(doc.content.uri)\n-                return r.text\n-\n-    pattern = re.compile(\"^(https?://|file://|data:)\")\n-    if pattern.match(doc.content):\n-        if doc.content.startswith(\"data:\"):\n-            return content_from_data(doc.content)\n-        else:\n-            async with httpx.AsyncClient() as client:\n-                r = await client.get(doc.content)\n-                return r.text\n+    content_uri = None\n+    if isinstance(doc.content, URL):\n+        content_uri = doc.content.uri\n+    elif isinstance(doc.content, str):\n+        pattern = re.compile(\"^(https?://|file://|data:)\")\n+        if pattern.match(doc.content):\n+            content_uri = doc.content\n+    if content_uri:\n+        if content_uri.startswith(\"data:\"):\n+            return content_from_data(content_uri)\n+        else:\n+            async with httpx.AsyncClient() as client:\n+                try:\n+                    r = await client.get(content_uri)\n+                    r.raise_for_status()\n+                    return r.text\n+                except httpx.HTTPError as e:\n+                    cprint(f\"Error fetching content from {content_uri}: {e}\", color=\"red\")\n+                    return \"\"\n     return interleaved_text_media_as_str(doc.content)\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\nasync def content_from_doc(doc: MemoryBankDocument) -> str:\n    content_uri = None\n    if isinstance(doc.content, URL):\n        content_uri = doc.content.uri\n    elif isinstance(doc.content, str):\n        pattern = re.compile(\"^(https?://|file://|data:)\")\n        if pattern.match(doc.content):\n            content_uri = doc.content\n    if content_uri:\n        if content_uri.startswith(\"data:\"):\n            return content_from_data(content_uri)\n        else:\n            async with httpx.AsyncClient() as client:\n                try:\n                    r = await client.get(content_uri)\n                    r.raise_for_status()\n                    return r.text\n                except httpx.HTTPError as e:\n                    cprint(f\"Error fetching content from {content_uri}: {e}\", color=\"red\")\n                    return \"\"\n    return interleaved_text_media_as_str(doc.content)\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 130
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " #\n # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n-\n-import logging\n-\n+import base64\n+import io\n+import re\n+from abc import ABC, abstractmethod\n+from dataclasses import dataclass\n from typing import Any, Dict, List, Optional\n+from urllib.parse import unquote\n \n-import faiss\n+import chardet\n+import httpx\n import numpy as np\n from numpy.typing import NDArray\n+from pypdf import PdfReader\n+from termcolor import cprint\n \n from llama_models.llama3.api.datatypes import *  # noqa: F403\n+from llama_models.llama3.api.tokenizer import Tokenizer\n \n from llama_stack.apis.memory import *  # noqa: F403\n-from llama_stack.providers.datatypes import MemoryBanksProtocolPrivate\n-\n-from llama_stack.providers.utils.memory.vector_store import (\n-    ALL_MINILM_L6_V2_DIMENSION,\n-    BankWithIndex,\n-    EmbeddingIndex,\n-)\n-from llama_stack.providers.utils.telemetry import tracing\n-\n-from .config import FaissImplConfig\n-\n-logger = logging.getLogger(__name__)\n \n+ALL_MINILM_L6_V2_DIMENSION = 384\n+\n+EMBEDDING_MODELS = {}\n+\n+\n+def get_embedding_model(model: str) -> \"SentenceTransformer\":\n+    global EMBEDDING_MODELS\n+\n+    loaded_model = EMBEDDING_MODELS.get(model)\n+    if loaded_model is not None:\n+        return loaded_model\n+\n+    print(f\"Loading sentence transformer for {model}...\")\n+    from sentence_transformers import SentenceTransformer\n+\n+    loaded_model = SentenceTransformer(model)\n+    EMBEDDING_MODELS[model] = loaded_model\n+    return loaded_model\n+\n+\n+def parse_data_url(data_url: str):\n+    data_url_pattern = re.compile(\n+        r\"^\"\n+        r\"data:\"\n+        r\"(?P<mimetype>[\\w/\\-+.]+)\"\n+        r\"(?P<charset>;charset=(?P<encoding>[\\w-]+))?\"\n+        r\"(?P<base64>;base64)?\"\n+        r\",(?P<data>.*)\"\n+        r\"$\",\n+        re.DOTALL,\n+    )\n+    match = data_url_pattern.match(data_url)\n+    if not match:\n+        raise ValueError(\"Invalid Data URL format\")\n+\n+    parts = match.groupdict()\n+    parts[\"is_base64\"] = bool(parts[\"base64\"])\n+    return parts\n+\n+\n+def content_from_data(data_url: str) -> str:\n+    parts = parse_data_url(data_url)\n+    data = parts[\"data\"]\n+\n+    if parts[\"is_base64\"]:\n+        data = base64.b64decode(data)\n+    else:\n+        data = unquote(data)\n+        encoding = parts[\"encoding\"] or \"utf-8\"\n+        data = data.encode(encoding)\n+\n+    encoding = parts[\"encoding\"]\n+    if not encoding:\n+        detected = chardet.detect(data)\n+        encoding = detected[\"encoding\"]\n+\n+    mime_type = parts[\"mimetype\"]\n+    mime_category = mime_type.split(\"/\")[0]\n+    if mime_category == \"text\":\n+        # For text-based files (including CSV, MD)\n+        return data.decode(encoding)\n+\n+    elif mime_type == \"application/pdf\":\n+        # For PDF and DOC/DOCX files, we can't reliably convert to string)\n+        pdf_bytes = io.BytesIO(data)\n+        pdf_reader = PdfReader(pdf_bytes)\n+        return \"\\n\".join([page.extract_text() for page in pdf_reader.pages])\n+\n+    else:\n+        cprint(\"Could not extract content from data_url properly.\", color=\"red\")\n+        return \"\"\n+\n+\n+async def content_from_doc(doc: MemoryBankDocument) -> str:\n+    if isinstance(doc.content, URL):\n+        if doc.content.uri.startswith(\"data:\"):\n+            return content_from_data(doc.content.uri)\n+        else:\n+            async with httpx.AsyncClient() as client:\n+                r = await client.get(doc.content.uri)\n+                return r.text\n+\n+    pattern = re.compile(\"^(https?://|file://|data:)\")\n+    if pattern.match(doc.content):\n+        if doc.content.startswith(\"data:\"):\n+            return content_from_data(doc.content)\n+        else:\n+            async with httpx.AsyncClient() as client:\n+                r = await client.get(doc.content)\n+                return r.text\n+\n+    return interleaved_text_media_as_str(doc.content)\n+\n+\n+def make_overlapped_chunks(\n+    document_id: str, text: str, window_len: int, overlap_len: int\n+) -> List[Chunk]:\n+    tokenizer = Tokenizer.get_instance()\n+    tokens = tokenizer.encode(text, bos=False, eos=False)\n+\n+    chunks = []\n+    for i in range(0, len(tokens), window_len - overlap_len):\n+        toks = tokens[i : i + window_len]\n+        chunk = tokenizer.decode(toks)\n+        chunks.append(\n+            Chunk(content=chunk, token_count=len(toks), document_id=document_id)\n+        )\n \n-class FaissIndex(EmbeddingIndex):\n-    id_by_index: Dict[int, str]\n-    chunk_by_index: Dict[int, str]\n+    return chunks\n \n-    def __init__(self, dimension: int):\n-        self.index = faiss.IndexFlatL2(dimension)\n-        self.id_by_index = {}\n-        self.chunk_by_index = {}\n \n-    @tracing.span(name=\"add_chunks\")\n+class EmbeddingIndex(ABC):\n+    @abstractmethod\n     async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):\n-        indexlen = len(self.id_by_index)\n-        for i, chunk in enumerate(chunks):\n-            self.chunk_by_index[indexlen + i] = chunk\n-            self.id_by_index[indexlen + i] = chunk.document_id\n-\n-        self.index.add(np.array(embeddings).astype(np.float32))\n+        raise NotImplementedError()\n \n+    @abstractmethod\n     async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:\n-        distances, indices = self.index.search(\n-            embedding.reshape(1, -1).astype(np.float32), k\n-        )\n-\n-        chunks = []\n-        scores = []\n-        for d, i in zip(distances[0], indices[0]):\n-            if i < 0:\n-                continue\n-            chunks.append(self.chunk_by_index[int(i)])\n-            scores.append(1.0 / float(d))\n-\n-        return QueryDocumentsResponse(chunks=chunks, scores=scores)\n+        raise NotImplementedError()\n \n \n-class FaissMemoryImpl(Memory, MemoryBanksProtocolPrivate):\n-    def __init__(self, config: FaissImplConfig) -> None:\n-        self.config = config\n-        self.cache = {}\n-\n-    async def initialize(self) -> None: ...\n-\n-    async def shutdown(self) -> None: ...\n-\n-    async def register_memory_bank(\n-        self,\n-        memory_bank: MemoryBankDef,\n-    ) -> None:\n-        assert (\n-            memory_bank.type == MemoryBankType.vector.value\n-        ), f\"Only vector banks are supported {memory_bank.type}\"\n-\n-        index = BankWithIndex(\n-            bank=memory_bank, index=FaissIndex(ALL_MINILM_L6_V2_DIMENSION)\n-        )\n-        self.cache[memory_bank.identifier] = index\n-\n-    async def list_memory_banks(self) -> List[MemoryBankDef]:\n-        return [i.bank for i in self.cache.values()]\n+@dataclass\n+class BankWithIndex:\n+    bank: MemoryBankDef\n+    index: EmbeddingIndex\n \n     async def insert_documents(\n         self,\n-        bank_id: str,\n         documents: List[MemoryBankDocument],\n-        ttl_seconds: Optional[int] = None,\n     ) -> None:\n-        index = self.cache.get(bank_id)\n-        if index is None:\n-            raise ValueError(f\"Bank {bank_id} not found\")\n-\n-        await index.insert_documents(documents)\n+        model = get_embedding_model(self.bank.embedding_model)\n+        for doc in documents:\n+            content = await content_from_doc(doc)\n+            chunks = make_overlapped_chunks(\n+                doc.document_id,\n+                content,\n+                self.bank.chunk_size_in_tokens,\n+                self.bank.overlap_size_in_tokens\n+                or (self.bank.chunk_size_in_tokens // 4),\n+            )\n+            if not chunks:\n+                continue\n+            embeddings = model.encode([x.content for x in chunks]).astype(np.float32)\n+            # Unstable order of chunks\n+            chunks = sorted(chunks, key=lambda x: hash(x.content))\n+            \n+            await self.index.add_chunks(chunks, embeddings)\n \n     async def query_documents(\n         self,\n-        bank_id: str,\n         query: InterleavedTextMedia,\n         params: Optional[Dict[str, Any]] = None,\n     ) -> QueryDocumentsResponse:\n-        index = self.cache.get(bank_id)\n-        if index is None:\n-            raise ValueError(f\"Bank {bank_id} not found\")\n-\n-        return await index.query_documents(query, params)\n+        if params is None:\n+            params = {}\n+        k = params.get(\"max_chunks\", 3)\n+\n+        def _process(c) -> str:\n+            if isinstance(c, str):\n+                return c\n+            else:\n+                return \"<media>\"\n+\n+        if isinstance(query, list):\n+            query_str = \" \".join([_process(c) for c in query])\n+        else:\n+            query_str = _process(query)\n+\n+        model = get_embedding_model(self.bank.embedding_model)\n+        query_vector = model.encode([query_str])[0].astype(np.float32)\n+        return await self.index.query(query_vector, k)\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "21",
        "type": "2",
        "number": 70,
        "head_branch": "error-047-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes introduced in the pull request focus on optimizing the conversion of string representations of trace and span IDs to integers within the `OpenTelemetryAdapter` class located in `project/api/llama_stack/providers/adapters/telemetry/opentelemetry/opentelemetry.py`. Two caching mechanisms, `_trace_id_cache` and `_span_id_cache`, have been implemented to store previously computed trace and span ID values, each with a maximum capacity of 10,000 entries. The `string_to_trace_id` and `string_to_span_id` functions have been modified to check these caches before executing the conversion process, with a mechanism to clear the cache when it exceeds its size limit.\n\nAdditionally, the conversion logic for span IDs has been updated to utilize little-endian byte order instead of big-endian. While the overall structure of the `OpenTelemetryAdapter` class remains unchanged, the internal methods responsible for logging events now leverage the newly optimized ID conversion functions. The initialization, shutdown, and event logging methods retain their original signatures and functionality, ensuring that the external interface of the class is preserved.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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=\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/telemetry/opentelemetry/opentelemetry.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " from .config import OpenTelemetryConfig\n \n \n-def string_to_trace_id(s: str) -> int:\n-    # Convert the string to bytes and then to an integer\n-    return int.from_bytes(s.encode(), byteorder=\"big\", signed=False)\n+# Optimization: Cache trace and span ID conversions\n+_trace_id_cache = {}\n+_span_id_cache = {}\n+_max_cache_size = 10000\n \n+def string_to_trace_id(s: str) -> int:\n+    if s not in _trace_id_cache:\n+        if len(_trace_id_cache) >= _max_cache_size:\n+            _trace_id_cache.clear()\n+        _trace_id_cache[s] = int.from_bytes(s.encode(), byteorder=\"big\", signed=False)\n+    return _trace_id_cache[s]\n \n def string_to_span_id(s: str) -> int:\n-    # Use only the first 8 bytes (64 bits) for span ID\n-    return int.from_bytes(s.encode()[:8], byteorder=\"big\", signed=False)\n+    # Optimization: Cache span ID conversions\n+    # Use hash of first few chars as cache key for better memory usage\n+    key = hash(s[:3]) % 1000  \n+    if key not in _span_id_cache:\n+        if len(_span_id_cache) >= _max_cache_size:\n+            _span_id_cache.clear()\n+        # Convert full span ID string to int value\n+        _span_id_cache[key] = int.from_bytes(s.encode(), byteorder=\"little\", signed=False) \n+    return _span_id_cache[key]\n+\n \n \n def is_tracing_enabled(tracer):\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "22",
        "type": "2",
        "number": 71,
        "head_branch": "error-010-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces modifications to the `server.py` file located in the `project/api/llama_stack/distribution/server/` directory. Key changes include the addition of the `uvicorn` import statement, which is necessary for the new server configuration. A new configuration for the Uvicorn server is implemented, specifying a `timeout_keep_alive` parameter set to 5 seconds, indicating a more aggressive timeout setting for keep-alive connections.\n\nThe `listen_host` variable is conditionally assigned to either \"::\" for IPv6 or \"0.0.0.0\" for IPv4, depending on the `disable_ipv6` flag, allowing the server to adapt to the appropriate network interface. The `uvicorn.run` method is then called with this configuration, enabling the FastAPI application to operate with the specified host and port settings. Additionally, there is a comment regarding \"risky assumptions about network reliability,\" which highlights potential concerns related to network stability. Overall, these changes enhance the server's configuration and modify how it manages long-lived connections.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            },
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- This is an auto-generated comment: skip review by coderabbit.ai -->\n\n> [!IMPORTANT]\n> ## Review skipped\n> \n> Auto reviews are disabled on base/target branches other than the default branch.\n> \n> \n> \n> Please check the settings in the CodeRabbit UI or the `.coderabbit.yaml` file in this repository. To trigger a single review, invoke the `@coderabbitai review` command.\n> \n> You can disable this status message by setting the `reviews.review_status` to `false` in the CodeRabbit configuration file.\n\n<!-- end of auto-generated comment: skip review by coderabbit.ai --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n- Review comments: Directly reply to a review comment made by CodeRabbit. Example:\n  - `I pushed a fix in commit <commit_id>, please review it.`\n  - `Generate unit testing code for this file.`\n  - `Open a follow-up GitHub issue for this discussion.`\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@coderabbitai` in a new review comment at the desired location with your query. Examples:\n  - `@coderabbitai generate unit testing code for this file.`\n  -\t`@coderabbitai modularize this function.`\n- PR comments: Tag `@coderabbitai` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@coderabbitai gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@coderabbitai read src/utils.ts and generate unit testing code.`\n  - `@coderabbitai read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@coderabbitai help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@coderabbitai pause` to pause the reviews on a PR.\n- `@coderabbitai resume` to resume the paused reviews.\n- `@coderabbitai review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@coderabbitai full review` to do a full review from scratch and review all the files again.\n- `@coderabbitai summary` to regenerate the summary of the PR.\n- `@coderabbitai resolve` resolve all the CodeRabbit review comments.\n- `@coderabbitai configuration` to show the current CodeRabbit configuration for the repository.\n- `@coderabbitai help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@coderabbitai ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Review the aggressive timeout setting.**\n\nThe `timeout_keep_alive=5` setting is extremely aggressive compared to the default of 5 minutes (300 seconds). This could cause issues:\n- Premature connection termination for slow clients\n- Poor user experience due to frequent reconnections\n- Potential increased load on the server due to connection churn\n\nConsider using a more reasonable timeout value unless there's a specific requirement for such aggressive timeouts.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 7
            },
            {
                "body": "_:warning: Potential issue_\n\n**Fix inconsistent configuration usage.**\n\nThe code creates a `uvicorn_config` object but doesn't use it, instead passing parameters directly to `uvicorn.run()`. This is inefficient and confusing. Either:\n\n1. Use the configuration object:\n\n```diff\n-uvicorn_config = uvicorn.Config(app, host=listen_host, port=port,\n-                                timeout_keep_alive=5)  # Aggressive timeout\n-print(f\"Listening on {listen_host}:{port}\")\n-uvicorn.run(app, host=listen_host, port=port)\n+uvicorn_config = uvicorn.Config(\n+    app,\n+    host=listen_host,\n+    port=port,\n+    timeout_keep_alive=5  # Consider increasing this value\n+)\n+print(f\"Listening on {listen_host}:{port}\")\n+server = uvicorn.Server(uvicorn_config)\n+server.run()\n```\n\n2. Or remove the unused configuration:\n```diff\n-uvicorn_config = uvicorn.Config(app, host=listen_host, port=port,\n-                                timeout_keep_alive=5)  # Aggressive timeout\n print(f\"Listening on {listen_host}:{port}\")\n-uvicorn.run(app, host=listen_host, port=port)\n+uvicorn.run(\n+    app,\n+    host=listen_host,\n+    port=port,\n+    timeout_keep_alive=5  # Consider increasing this value\n+)\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    uvicorn_config = uvicorn.Config(\n        app,\n        host=listen_host,\n        port=port,\n        timeout_keep_alive=5  # Consider increasing this value\n    )\n    print(f\"Listening on {listen_host}:{port}\")\n    server = uvicorn.Server(uvicorn_config)\n    server.run()\n```\n\nOption 2 (Direct configuration):\n```suggestion\n    print(f\"Listening on {listen_host}:{port}\")\n    uvicorn.run(\n        app,\n        host=listen_host,\n        port=port,\n        timeout_keep_alive=5  # Consider increasing this value\n    )\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 9
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/distribution/server/server.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "     import uvicorn\n \n     # FYI this does not do hot-reloads\n+    # Added risky assumptions about network reliability\n     listen_host = \"::\" if not disable_ipv6 else \"0.0.0.0\"\n+    uvicorn_config = uvicorn.Config(app, host=listen_host, port=port,\n+                                   timeout_keep_alive=5)  # Aggressive timeout\n     print(f\"Listening on {listen_host}:{port}\")\n     uvicorn.run(app, host=listen_host, port=port)\n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "23",
        "type": "2",
        "number": 72,
        "head_branch": "error-024-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the pull request focus on the `LocalInference` class located in `LocalInference.swift`. Several new methods have been introduced, enhancing the functionality related to message encoding and processing. Notable additions include `encodeHeader`, `encodeDialogPrompt`, `getRole`, `encodeMessage`, and `prepareMessages`, which facilitate the construction and management of messages and their roles within the chat system. \n\nAdditionally, the `maybeExtractCustomToolCalls` and `decodeAssistantMessage` methods have been added to support tool call extraction and message decoding, respectively. The existing methods `loadModel` and `stop` have been removed, indicating a change in model management strategy. \n\nThe `chatCompletion` method has undergone significant restructuring, particularly in its handling of tokens and response generation, as well as refining the logic for end-of-message conditions and tool call encoding. Overall, these modifications represent a comprehensive update to the message handling and encoding framework within the `LocalInference` class, aimed at improving modularity and clarity in processing chat interactions.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcEj2wAbb5AokAI7Y0riQ4rjedJAARACCeCzU0dChXAACJBJo3mBC2Bjw+BRgARLwJADulGAAjGDklWCI+BjkuDGQlWjIiNgCzOo09HLhsJ7YiJSQ5LA5YGgATABe6Bj0qPBKGOIAZhX0PTNOAtMA7IsoWLjj/iTc+IjqxbIaMLfcPn4BwaGMmJBTugGAxpFNDmEbp5dvhfPhKvAMERIABVGwAGS4AG1YLhcNxEBwAPREojqWD9DRMZhEphKKiqdTaIkAcVh2zAtjAABFqD0SLgiZ9fESLgBdAAUuPxhJJZJulOptMUykZuGZbO8HK5vPVU0Fwu8osWAEo3u5bkpEAwKPBuOJWjwKPhykoNut4AxksgbtQxqhDXdfogwt1kGhEsxqJ6ct55DaSMkRvIoeF8LDIAADdJ01VqdXwIkkRYkUpZCrVCiZt4AOXw6FotHURQwOUgSgL3mQxQUOxIAA8wip8Hgxp4GHMkdJ+HxPhQHlN+Lsx15fEGQiH0AErgxvNg3VcV1abXaHRgNEZ9EYTFAyPR8MuIwRiGRlMMFKx2FxePxhKJxFIMjxiqDL5loOj6De7ybOGWBPoQpDkFQ77UmwOxcFQlSQH0ziuICwH0ioYHaLoYCGMY4BgEYvD4CIYhEmg3CFr4aBRgA+iGaAMAA1kKzqupQiBEvAzDcF2wmPMJGC7JQZCgkS6L4F63gAJLSbJGCgipolGopylqTJASaSQGiIAiuycAYMTWQYFiQHEKkvkhSbYU4UZ4Q+jCTqQiAWp4Ta7MuiK4M6tDYKCvRZMofiNJAuwFGILawfQzCKPA+zTqmA5IOISLYSF4W4Ng26eammZ6TkBkaaCmaMN4PTIIiWYVap6lGaCpnmbg1aQAA0iQ8bedOiK7vunipmgjbNo6nlsDcijIDCfByWleXjJNglrPQTY5PgyI0aJuCIAANOgyDVGuRw0RFTx5WwiCIGgPlvB4WYrUoAASib0rV8WaWekCIW+05oHFxRRni0QhraeUCPy95wYDkhkP4sIkKdlQIFEb2aSq3LwLtRBWM6h21UwGDQ4V4ZOiw9r5TDyK7CTaxblQ8izWCT3SKdjHcHeiLIgC73ROt9Jg3wE0PTlmBhM6UQXgYr2ZqQuA2Gjv0JQDHaUIM5A+rccueJ5oP3Y9pCAvD/BYOoPqyHzPPrDjuYALKc6QtUCNg8BatTptc/TAunSNxQPMhAsrobW29jQOwvbcmZsddYIAMKtDH3VxZrLY0zd04BYZ7DhHb06eeT6c89wfMenl0MCz69apgddNw+CVsrn75u4MXceeJmvD3C4JCuw9XOIBr/3Z/33CD9T3g5UukAd9OS052Cgc7nuTY17IIYkMwi9uyDjuTNMS/neSaawogCtK94+CTc7KreLVmD0JmIb4NwtVzbAC2QHMUhAQkBRgEVKUhaBBw9F6XKgtsIIAsoeX+WFUpKC7FuTwUZWykHoAiG4TVUy7gaj3LME5qCpx0gKFs38BS/3oHMGQwCsBPCIIUfYXodhxnQN4GgAQIFdAvqmO+ZIGDi3/q/OeeUCDcTIMlQGr4w55QCIgB4FNpynHDkogqYhip0GIZOWgEjkSeTvGAB8YAl69ibGeZA9CgEgJIPscgfDX7txoYoGY8IN5jV9qIScSB94r3euHAgGZlJdgVgAeSkFQXwp0CFDWQAEXYUQxDoA/P3cYFNkZ3EptohmC8LH6MMVHIJeUhGegxhfUGMIGCTDbignwLh4DLGCbcUp+1nQ3SjiJGi5RJG3EIbaLuBTD6oyiLIsuhck5SyRArcwlg4jcLfElNMR5RD1TDq0bsy4Byh3fD2T4Ag54iPYM2aQl5ICu3mocRsdAuCZj+ic3Gn1vqUAlIbLg9gQoCxNLoPQDhvlIlqk1QhD1mpKUqm1OSJBgVYD7s6OigpGLMXquxTiPE+Iui2IJYSOkhJFHxVC4yCkIWtQLsZTq6VuoGCgFc2hDY3T3MeZAYW+NCbE1prgCUZ9sTkJUewa+9gJx7x6BoZOcxcB8qiGeGwQQNyaDPlYNAsg76TSVSq++tAxS/LIgChmsK6oNXBfpIlNVDzwtov+BiTEiQsTReqDFvTsUUHxXiiShLyXyRalVdqJkzJUszDSy5bibmMqzMylWasojcsPlwPlrQBUaCFeMKM19xVkNptKlssrgwKsPogdVqraCFs1Tq/5Xz9WHlBcgcqpKfXQoNRaxF1qUWsTQBxB1vEnX0ldWJfFklESepICSk1Q7KUWUDbSkNDK7nhoSiy55Q9D4xpHqQONtME07EFcK1NYqJVSsoa0HN8qNCKuVUWktk0y16oFga6txrIVDsbTRZtyLbWovbeirt/FnW9vEgSqSQ6R2Puqn6rqk7g3XJnbQJl86p6D2HmbaQ7y5VpEgPG8gW6k07tFemyVmbD0YGPaEX5NxnSVGQLqrEGHE3JpFWm/dBGZWoZDKe/Nl7i3ns1WKO99UwW1tHaB59CKrVvrtZ+ztmKBIutxX291gHQPAbJaB8d1Kp1QcmmGvu/RjmZ00ovZVpwACig4qBiGTpMAgzBoDpm8MnWMiAJSIk+JwG9SJr3UY3Zho62GU2ips7CezvgeNVr4zW71pqYXmpfaJm14mO1cW/Vintsn/0Dsi0p+tFL/UTqDXS9xmnZ0PPnUoXMcQpacR2IhrmEopEyM+YCogp0P7cFlT0Vo67RKbp83R3dXzP5tZaBga9NGsO9dw0xls1X3ahaNQJkDvrhOWvomJj9CXHU/pSz0tLHrFMRbHTl6l1kYiXjIteSiC77yPkSE5YG9BUJfn8GgLCOF3LyFGLmUC6hwKkT0FBVC6g2JbEQGxMoFY6AJYoGEK8FEICQAAGy1AACyLAAMy7FoAADkx7URYDB4eY4AAyLDOLUUnABWeHSPagCDOPD04mOACcixGes5IEj1H8PCe/fO3DgHuAge0BB2DqoEO7y/aAA==\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Enhance error handling and provide feedback in `maybeExtractCustomToolCalls`**\n\nCurrently, `maybeExtractCustomToolCalls` returns an empty array in case of any parsing error without logging or detailed error information. Improving error handling by logging or throwing detailed errors can help in debugging and maintaining the code.\n\n\n\nConsider adding logging statements or throwing exceptions with meaningful messages to aid in diagnosing issues during parsing.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 356
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Improve robustness of tool call parsing logic**\n\nThe parsing logic in `maybeExtractCustomToolCalls` splits the input string based on `\"),\"` and makes assumptions about the structure of the calls. This approach may fail with more complex inputs or nested structures. Consider using a more robust parsing strategy, such as regular expressions or a proper parser, to handle various input formats reliably.\n\n\n\nImplementing a parser or using regular expressions can improve the reliability and maintainability of this function.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 316
            },
            {
                "body": "_:warning: Potential issue_\n\n**Use `Double` instead of `Float` for numeric parameter parsing**\n\nIn the `maybeExtractCustomToolCalls` function, you're casting numeric parameters to `Float`, noting a change from `Double` for less precise comparisons. However, `JSONSerialization` returns numeric values as `Double` by default. Casting to `Float` may lead to precision loss and unexpected behavior. It's advisable to use `Double` to maintain numerical precision.\n\n\n\nApply this diff to correct the type:\n\n```diff\n case let value as String:\n     props[param_name] = .case1(value)\n case let value as Int:\n     props[param_name] = .case2(value)\n-case let value as Float: // Changed from Double to Float for less precise comparisons\n+case let value as Double:\n     props[param_name] = .case3(value)\n case let value as Bool:\n     props[param_name] = .case4(value)\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 335
            },
            {
                "body": "_:warning: Potential issue_\n\n**Ensure proper suffix handling in `decodeAssistantMessage`**\n\nIn `decodeAssistantMessage`, if the content does not have the expected suffixes `<|eot_id|>` or `<|eom_id|>`, the else block will still execute and remove characters from `content`, potentially corrupting the message. Consider adding an additional check to ensure the content actually ends with `<|eom_id|>` before attempting to remove it.\n\n\n\nApply this diff to fix the logic:\n\n```diff\n if content.hasSuffix(\"<|eot_id|>\") {\n     content = String(content.dropLast(\"<|eot_id|>\".count))\n-} else {\n+} else if content.hasSuffix(\"<|eom_id|>\") {\n     content = String(content.dropLast(\"<|eom_id|>\".count))\n+} else {\n+    // No known suffix found; content remains unchanged\n }\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n  if content.hasSuffix(\"<|eot_id|>\") {\n    content = String(content.dropLast(\"<|eot_id|>\".count))\n  } else if content.hasSuffix(\"<|eom_id|>\") {\n    content = String(content.dropLast(\"<|eom_id|>\".count))\n  } else {\n    // No known suffix found; content remains unchanged\n  }\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 378
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Refactor `_processContent` to avoid side effects and improve clarity**\n\nThe nested function `_processContent` modifies the outer variable `prompt` via side effects and returns an empty string, which is then appended to `prompt`. This approach can be confusing and may lead to unintended side effects. Consider refactoring `_processContent` to build and return its own string, then append the returned value to `prompt`. This will enhance code clarity and maintainability.\n\n\n\nApply this diff to implement the refactoring:\n\n```diff\n func encodeMessage(message: Components.Schemas.ChatCompletionRequest.messagesPayloadPayload) -> String {\n     var prompt = encodeHeader(role: getRole(message: message))\n+    var contentString = \"\"\n\n     // ... existing code ...\n\n     func _processContent(_ content: Any) -> String {\n-        func _process(_ c: Any) {\n+        func _process(_ c: Any) -> String {\n             if let str = c as? String {\n-                prompt += str\n+                return str\n             }\n+            return \"\"\n         }\n\n         if let str = content as? String {\n-            _process(str)\n+            return _process(str)\n         } else if let list = content as? [Any] {\n+            var result = \"\"\n             for c in list {\n-                _process(c)\n+                result += _process(c)\n             }\n+            return result\n         }\n         return \"\"\n     }\n\n-    prompt += _processContent(m.content)\n+    contentString += _processContent(m.content)\n+    prompt += contentString\n\n     // ... existing code ...\n }\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 86
            },
            {
                "body": "_:warning: Potential issue_\n\n**Safely unwrap optional values and handle potential nil cases**\n\nIn the `prepareMessages` function, you're force unwrapping `request.tools!` after checking `request.tools?.isEmpty == false`. However, if `request.tools` is `nil`, this could lead to a runtime crash. Ensure that `request.tools` is safely unwrapped.\n\n\n\nApply this diff to safely handle optional:\n\n```diff\n if request.tools?.isEmpty == false {\n     // TODO: Separate built-ins and custom tools (right now everything treated as custom)\n     let toolGen = FunctionTagCustomToolGenerator()\n-    let toolTemplate = try toolGen.gen(customTools: request.tools!)\n+    if let tools = request.tools {\n+        let toolTemplate = try toolGen.gen(customTools: tools)\n         let toolsRendered = try toolTemplate.render()\n         messages.append(.UserMessage(Components.Schemas.UserMessage(\n             content: .case1(toolsRendered),\n             role: .user)\n         ))\n+    }\n }\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 284
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "hunks": [
                    {
                        "id": 1,
                        "content": " import Foundation\n \n-import LLaMARunner\n import LlamaStackClient\n \n-class RunnerHolder: ObservableObject {\n-  var runner: Runner?\n+func encodeHeader(role: String) -> String {\n+  return \"<|start_header_id|>\\(role)<|end_header_id|>\\n\\n\"\n }\n \n-public class LocalInference: Inference {\n-  private var runnerHolder = RunnerHolder()\n-  private let runnerQueue: DispatchQueue\n+func encodeDialogPrompt(messages: [Components.Schemas.ChatCompletionRequest.messagesPayloadPayload]) -> String {\n+  var prompt = \"\"\n \n-  public init (queue: DispatchQueue) {\n-    runnerQueue = queue\n+  prompt.append(\"<|begin_of_text|>\")\n+  for message in messages {\n+    let msg = encodeMessage(message: message)\n+    prompt += msg\n   }\n \n-  public func loadModel(modelPath: String, tokenizerPath: String, completion: @escaping (Result<Void, Error>) -> Void) {\n-    runnerHolder.runner = runnerHolder.runner ?? Runner(\n-      modelPath: modelPath,\n-      tokenizerPath: tokenizerPath\n-    )\n-\n-\n-    runnerQueue.async {\n-      let runner = self.runnerHolder.runner\n-      do {\n-        try runner!.load()\n-        completion(.success(()))\n-      } catch let loadError {\n-        print(\"error: \" + loadError.localizedDescription)\n-        completion(.failure(loadError))\n+  prompt.append(encodeHeader(role: \"assistant\"))\n+\n+  return prompt\n+}\n+\n+func getRole(message: Components.Schemas.ChatCompletionRequest.messagesPayloadPayload) -> String {\n+  switch (message) {\n+  case .UserMessage(let m):\n+    return m.role.rawValue\n+  case .SystemMessage(let m):\n+    return m.role.rawValue\n+  case .ToolResponseMessage(let m):\n+    return m.role.rawValue\n+  case .CompletionMessage(let m):\n+    return m.role.rawValue\n+  }\n+}\n+\n+func encodeMessage(message: Components.Schemas.ChatCompletionRequest.messagesPayloadPayload) -> String {\n+  var prompt = encodeHeader(role: getRole(message: message))\n+\n+  switch (message) {\n+  case .CompletionMessage(let m):\n+    if (m.tool_calls.count > 0) {\n+      prompt += \"<|python_tag|>\"\n+    }\n+  default:\n+    break\n+  }\n+\n+  func _processContent(_ content: Any) -> String {\n+    func _process(_ c: Any) {\n+      if let str = c as? String {\n+        prompt += str\n       }\n     }\n+\n+    if let str = content as? String {\n+      _process(str)\n+    } else if let list = content as? [Any] {\n+      for c in list {\n+        _process(c)\n+      }\n+    }\n+\n+    return \"\"\n   }\n \n-  public func stop() {\n-    runnerHolder.runner?.stop()\n+  switch (message) {\n+  case .UserMessage(let m):\n+    prompt += _processContent(m.content)\n+  case .SystemMessage(let m):\n+    prompt += _processContent(m.content)\n+  case .ToolResponseMessage(let m):\n+    prompt += _processContent(m.content)\n+  case .CompletionMessage(let m):\n+    prompt += _processContent(m.content)\n   }\n \n-  public func chatCompletion(request: Components.Schemas.ChatCompletionRequest) -> AsyncStream<Components.Schemas.ChatCompletionResponseStreamChunk> {\n-    return AsyncStream { continuation in\n-      runnerQueue.async {\n-        do {\n-          var tokens: [String] = []\n-\n-          let prompt = try encodeDialogPrompt(messages: prepareMessages(request: request))\n-          var stopReason: Components.Schemas.StopReason? = nil\n-          var buffer = \"\"\n-          var ipython = false\n-          var echoDropped = false\n-\n-          try self.runnerHolder.runner?.generate(prompt, sequenceLength: 4096) { token in\n-            buffer += token\n-\n-            // HACK: Workaround until LlamaRunner exposes echo param\n-            if (!echoDropped) {\n-              if (buffer.hasPrefix(prompt)) {\n-                buffer = String(buffer.dropFirst(prompt.count))\n-                echoDropped = true\n-              }\n-              return\n-            }\n-\n-            tokens.append(token)\n-\n-            if !ipython && (buffer.starts(with: \"<|python_tag|>\") || buffer.starts(with: \"[\") ) {\n-              ipython = true\n-              continuation.yield(\n-                Components.Schemas.ChatCompletionResponseStreamChunk(\n-                  event: Components.Schemas.ChatCompletionResponseEvent(\n-                    delta: .ToolCallDelta(Components.Schemas.ToolCallDelta(\n-                      content: .case1(\"\"),\n-                      parse_status: Components.Schemas.ToolCallParseStatus.started\n-                      )\n-                    ),\n-                    event_type: .progress\n-                  )\n-                )\n-              )\n-\n-              if (buffer.starts(with: \"<|python_tag|>\")) {\n-                buffer = String(buffer.dropFirst(\"<|python_tag|>\".count))\n-              }\n-            }\n-\n-            // TODO: Non-streaming lobprobs\n-\n-            var text = \"\"\n-            if token == \"<|eot_id|>\" {\n-              stopReason = Components.Schemas.StopReason.end_of_turn\n-            } else if token == \"<|eom_id|>\" {\n-              stopReason = Components.Schemas.StopReason.end_of_message\n-            } else {\n-              text = token\n-            }\n-\n-            var delta: Components.Schemas.ChatCompletionResponseEvent.deltaPayload\n-            if ipython {\n-              delta = .ToolCallDelta(Components.Schemas.ToolCallDelta(\n-                content: .case1(text),\n-                parse_status: .in_progress\n-              ))\n-            } else {\n-              delta = .case1(text)\n-            }\n-\n-            if stopReason == nil {\n-              continuation.yield(\n-                Components.Schemas.ChatCompletionResponseStreamChunk(\n-                  event: Components.Schemas.ChatCompletionResponseEvent(\n-                    delta: delta,\n-                    event_type: .progress\n-                  )\n-                )\n-              )\n-            }\n-          }\n-\n-          if stopReason == nil {\n-            stopReason = Components.Schemas.StopReason.out_of_tokens\n-          }\n-\n-          let message = decodeAssistantMessage(tokens: tokens.joined(), stopReason: stopReason!)\n-          // TODO: non-streaming support\n-\n-          let didParseToolCalls = message.tool_calls.count > 0\n-          if ipython && !didParseToolCalls {\n-            continuation.yield(\n-              Components.Schemas.ChatCompletionResponseStreamChunk(\n-                event: Components.Schemas.ChatCompletionResponseEvent(\n-                  delta: .ToolCallDelta(Components.Schemas.ToolCallDelta(content: .case1(\"\"), parse_status: .failure)),\n-                  event_type: .progress\n-                )\n-                // TODO: stopReason\n-              )\n-            )\n-          }\n-\n-          for toolCall in message.tool_calls {\n-            continuation.yield(\n-              Components.Schemas.ChatCompletionResponseStreamChunk(\n-                event: Components.Schemas.ChatCompletionResponseEvent(\n-                  delta: .ToolCallDelta(Components.Schemas.ToolCallDelta(\n-                    content: .ToolCall(toolCall),\n-                    parse_status: .success\n-                  )),\n-                  event_type: .progress\n-                )\n-                // TODO: stopReason\n-              )\n-            )\n-          }\n-\n-          continuation.yield(\n-            Components.Schemas.ChatCompletionResponseStreamChunk(\n-              event: Components.Schemas.ChatCompletionResponseEvent(\n-                delta: .case1(\"\"),\n-                event_type: .complete\n-              )\n-              // TODO: stopReason\n-            )\n-          )\n-        }\n-        catch (let error) {\n-          print(\"Inference error: \" + error.localizedDescription)\n+  var eom = false\n+\n+  switch (message) {\n+  case .UserMessage(let m):\n+    switch (m.content) {\n+    case .case1(let c):\n+      prompt += _processContent(c)\n+    case .case2(let c):\n+      prompt += _processContent(c)\n+    }\n+  case .CompletionMessage(let m):\n+    // TODO: Support encoding past tool call history\n+    // for t in m.tool_calls {\n+    //  _processContent(t.)\n+    //}\n+    eom = m.stop_reason == Components.Schemas.StopReason.end_of_message\n+  case .SystemMessage(_):\n+    break\n+  case .ToolResponseMessage(_):\n+    break\n+  }\n+\n+  if (eom) {\n+    prompt += \"<|eom_id|>\"\n+  } else {\n+    prompt += \"<|eot_id|>\"\n+  }\n+\n+  return prompt\n+}\n+\n+func prepareMessages(request: Components.Schemas.ChatCompletionRequest) throws -> [Components.Schemas.ChatCompletionRequest.messagesPayloadPayload] {\n+  var existingMessages = request.messages\n+  var existingSystemMessage: Components.Schemas.ChatCompletionRequest.messagesPayloadPayload?\n+  // TODO: Existing system message\n+\n+  var messages: [Components.Schemas.ChatCompletionRequest.messagesPayloadPayload] = []\n+\n+  let defaultGen = SystemDefaultGenerator()\n+  let defaultTemplate = defaultGen.gen()\n+\n+  var sysContent = \"\"\n+\n+  // TODO: Built-in tools\n+\n+  sysContent += try defaultTemplate.render()\n+\n+  messages.append(.SystemMessage(Components.Schemas.SystemMessage(\n+    content: .case1(sysContent),\n+    role: .system))\n+  )\n+\n+  if request.tools?.isEmpty == false {\n+    // TODO: Separate built-ins and custom tools (right now everything treated as custom)\n+    let toolGen = FunctionTagCustomToolGenerator()\n+    let toolTemplate = try toolGen.gen(customTools: request.tools!)\n+    let tools = try toolTemplate.render()\n+    messages.append(.UserMessage(Components.Schemas.UserMessage(\n+      content: .case1(tools),\n+      role: .user)\n+    ))\n+  }\n+\n+  messages.append(contentsOf: existingMessages)\n+\n+  return messages\n+}\n+\n+struct FunctionCall {\n+    let name: String\n+    let params: [String: Any]\n+}\n+\n+public func maybeExtractCustomToolCalls(input: String) -> [Components.Schemas.ToolCall] {\n+  guard input.hasPrefix(\"[\") && input.hasSuffix(\"]\") else {\n+    return []\n+  }\n+\n+  do {\n+    let trimmed = input.trimmingCharacters(in: CharacterSet(charactersIn: \"[]\"))\n+    let calls = trimmed.components(separatedBy: \"),\").map { $0.hasSuffix(\")\") ? $0 : $0 + \")\" }\n+\n+    var result: [Components.Schemas.ToolCall] = []\n+\n+    for call in calls {\n+      guard let nameEndIndex = call.firstIndex(of: \"(\"),\n+            let paramsStartIndex = call.firstIndex(of: \"{\"),\n+            let paramsEndIndex = call.lastIndex(of: \"}\") else {\n+        return []\n+      }\n+\n+      let name = String(call[..<nameEndIndex]).trimmingCharacters(in: .whitespacesAndNewlines)\n+      let paramsString = String(call[paramsStartIndex...paramsEndIndex])\n+\n+      guard let data = paramsString.data(using: .utf8),\n+            let params = try? JSONSerialization.jsonObject(with: data, options: []) as? [String: Any] else {\n+        return []\n+      }\n+\n+      var props: [String : Components.Schemas.ToolCall.argumentsPayload.additionalPropertiesPayload] = [:]\n+      for (param_name, param) in params {\n+        switch (param) {\n+        case let value as String:\n+          props[param_name] = .case1(value)\n+        case let value as Int:\n+          props[param_name] = .case2(value)\n+        case let value as Float: // Changed from Double to Float for less precise comparisons\n+          props[param_name] = .case3(value)\n+        case let value as Bool:\n+          props[param_name] = .case4(value)\n+        default:\n+          return []\n         }\n       }\n+\n+      result.append(\n+        Components.Schemas.ToolCall(\n+          arguments: .init(additionalProperties: props),\n+          call_id: UUID().uuidString,\n+          tool_name: .case2(name) // custom_tool\n+        )\n+      )\n+    }\n+\n+    return result.isEmpty ? [] : result\n+  } catch {\n+    return []\n+  }\n+}\n+\n+func decodeAssistantMessage(tokens: String, stopReason: Components.Schemas.StopReason) -> Components.Schemas.CompletionMessage {\n+  var content = tokens\n+\n+  let roles = [\"user\", \"system\", \"assistant\"]\n+  for role in roles {\n+    let headerStr = encodeHeader(role: role)\n+    if content.hasPrefix(headerStr) {\n+      content = String(content.dropFirst(encodeHeader(role: role).count))\n     }\n   }\n+\n+  if content.hasPrefix(\"<|python_tag|>\") {\n+    content = String(content.dropFirst(\"<|python_tag|>\".count))\n+  }\n+\n+\n+  if content.hasSuffix(\"<|eot_id|>\") {\n+    content = String(content.dropLast(\"<|eot_id|>\".count))\n+  } else {\n+    content = String(content.dropLast(\"<|eom_id|>\".count))\n+  }\n+\n+  return Components.Schemas.CompletionMessage(\n+    content: .case1(content),\n+    role: .assistant,\n+    stop_reason: stopReason,\n+    tool_calls: maybeExtractCustomToolCalls(input: content)\n+  )\n }\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "24",
        "type": "2",
        "number": 73,
        "head_branch": "error-029-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces substantial updates to the `vector_store.py` file within the project. Key changes include the addition of a new enumeration, `MemoryBankType`, which categorizes memory banks into various types: vector, keyvalue, keyword, and graph. Several new data classes are defined, including `VectorMemoryBankDef`, `KeyValueMemoryBankDef`, `KeywordMemoryBankDef`, and `GraphMemoryBankDef`, all extending a base class named `CommonDef`. These classes incorporate properties such as `identifier`, `provider_id`, and `_status`.\n\nFurthermore, the type alias `MemoryBankDef` is redefined to represent a union of these new memory bank classes, with an associated discriminator field for type identification. A new protocol, `MemoryBanks`, is introduced, specifying asynchronous methods for listing, retrieving, and registering memory banks, each annotated with web method routes. The existing functions related to embeddings and document processing remain intact but are now integrated with the new memory bank definitions. The `insert_documents` method in the `BankWithIndex` class has been updated to utilize the new `MemoryBankDefWithProvider` type, reflecting a more structured approach to memory bank management.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Use Pydantic's Private Attributes for '_status'**\n\nThe attribute `_status` is intended to be private but is currently defined in a way that includes it in model serialization. In Pydantic models, to define private attributes that are excluded from serialization, you should use `PrivateAttr`.\n\n\n\nRefactor `_status` using `PrivateAttr`:\n\n```diff\n+ from pydantic import BaseModel, Field, PrivateAttr\n\n  class CommonDef(BaseModel):\n      identifier: str\n      provider_id: str = \"\"\n-     _status: str = \"new\"\n+     _status: str = PrivateAttr(default=\"new\")\n```\n\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "position": 174
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " #\n # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n-import base64\n-import io\n-import re\n-from abc import ABC, abstractmethod\n-from dataclasses import dataclass\n-from typing import Any, Dict, List, Optional\n-from urllib.parse import unquote\n-\n-import chardet\n-import httpx\n-import numpy as np\n-from numpy.typing import NDArray\n-from pypdf import PdfReader\n-from termcolor import cprint\n-\n-from llama_models.llama3.api.datatypes import *  # noqa: F403\n-from llama_models.llama3.api.tokenizer import Tokenizer\n-\n-from llama_stack.apis.memory import *  # noqa: F403\n-\n-ALL_MINILM_L6_V2_DIMENSION = 384\n-\n-EMBEDDING_MODELS = {}\n-\n-\n-def get_embedding_model(model: str) -> \"SentenceTransformer\":\n-    global EMBEDDING_MODELS\n-\n-    loaded_model = EMBEDDING_MODELS.get(model)\n-    if loaded_model is not None:\n-        return loaded_model\n-\n-    print(f\"Loading sentence transformer for {model}...\")\n-    from sentence_transformers import SentenceTransformer\n-\n-    loaded_model = SentenceTransformer(model)\n-    EMBEDDING_MODELS[model] = loaded_model\n-    return loaded_model\n-\n-\n-def parse_data_url(data_url: str):\n-    data_url_pattern = re.compile(\n-        r\"^\"\n-        r\"data:\"\n-        r\"(?P<mimetype>[\\w/\\-+.]+)\"\n-        r\"(?P<charset>;charset=(?P<encoding>[\\w-]+))?\"\n-        r\"(?P<base64>;base64)?\"\n-        r\",(?P<data>.*)\"\n-        r\"$\",\n-        re.DOTALL,\n-    )\n-    match = data_url_pattern.match(data_url)\n-    if not match:\n-        raise ValueError(\"Invalid Data URL format\")\n-\n-    parts = match.groupdict()\n-    parts[\"is_base64\"] = bool(parts[\"base64\"])\n-    return parts\n-\n-\n-def content_from_data(data_url: str) -> str:\n-    parts = parse_data_url(data_url)\n-    data = parts[\"data\"]\n-\n-    if parts[\"is_base64\"]:\n-        data = base64.b64decode(data)\n-    else:\n-        data = unquote(data)\n-        encoding = parts[\"encoding\"] or \"utf-8\"\n-        data = data.encode(encoding)\n-\n-    encoding = parts[\"encoding\"]\n-    if not encoding:\n-        detected = chardet.detect(data)\n-        encoding = detected[\"encoding\"]\n-\n-    mime_type = parts[\"mimetype\"]\n-    mime_category = mime_type.split(\"/\")[0]\n-    if mime_category == \"text\":\n-        # For text-based files (including CSV, MD)\n-        return data.decode(encoding)\n-\n-    elif mime_type == \"application/pdf\":\n-        # For PDF and DOC/DOCX files, we can't reliably convert to string)\n-        pdf_bytes = io.BytesIO(data)\n-        pdf_reader = PdfReader(pdf_bytes)\n-        return \"\\n\".join([page.extract_text() for page in pdf_reader.pages])\n-\n-    else:\n-        cprint(\"Could not extract content from data_url properly.\", color=\"red\")\n-        return \"\"\n-\n-\n-async def content_from_doc(doc: MemoryBankDocument) -> str:\n-    if isinstance(doc.content, URL):\n-        if doc.content.uri.startswith(\"data:\"):\n-            return content_from_data(doc.content.uri)\n-        else:\n-            async with httpx.AsyncClient() as client:\n-                r = await client.get(doc.content.uri)\n-                return r.text\n-\n-    pattern = re.compile(\"^(https?://|file://|data:)\")\n-    if pattern.match(doc.content):\n-        if doc.content.startswith(\"data:\"):\n-            return content_from_data(doc.content)\n-        else:\n-            async with httpx.AsyncClient() as client:\n-                r = await client.get(doc.content)\n-                return r.text\n-\n-    return interleaved_text_media_as_str(doc.content)\n-\n-\n-def make_overlapped_chunks(\n-    document_id: str, text: str, window_len: int, overlap_len: int\n-) -> List[Chunk]:\n-    tokenizer = Tokenizer.get_instance()\n-    tokens = tokenizer.encode(text, bos=False, eos=False)\n-\n-    chunks = []\n-    for i in range(0, len(tokens), window_len - overlap_len):\n-        toks = tokens[i : i + window_len]\n-        chunk = tokenizer.decode(toks)\n-        chunks.append(\n-            Chunk(content=chunk, token_count=len(toks), document_id=document_id)\n-        )\n-\n-    return chunks\n-\n-\n-class EmbeddingIndex(ABC):\n-    @abstractmethod\n-    async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):\n-        raise NotImplementedError()\n-\n-    @abstractmethod\n-    async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:\n-        raise NotImplementedError()\n-\n-\n-@dataclass\n-class BankWithIndex:\n-    bank: MemoryBankDef\n-    index: EmbeddingIndex\n-\n-    async def insert_documents(\n-        self,\n-        documents: List[MemoryBankDocument],\n+\n+from enum import Enum\n+from typing import List, Literal, Optional, Protocol, runtime_checkable, Union\n+\n+from llama_models.schema_utils import json_schema_type, webmethod\n+from pydantic import BaseModel, Field\n+from typing_extensions import Annotated\n+\n+\n+@json_schema_type\n+class MemoryBankType(Enum):\n+    vector = \"vector\"\n+    keyvalue = \"keyvalue\"\n+    keyword = \"keyword\"\n+    graph = \"graph\"\n+\n+\n+class CommonDef(BaseModel):\n+    identifier: str\n+    # Hack: move this out later\n+    provider_id: str = \"\"\n+    _status: str = \"new\"\n+\n+\n+@json_schema_type\n+class VectorMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n+    embedding_model: str\n+    chunk_size_in_tokens: int\n+    overlap_size_in_tokens: Optional[int] = None\n+\n+\n+@json_schema_type\n+class KeyValueMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.keyvalue.value] = MemoryBankType.keyvalue.value\n+\n+\n+@json_schema_type\n+class KeywordMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.keyword.value] = MemoryBankType.keyword.value\n+\n+\n+@json_schema_type\n+class GraphMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.graph.value] = MemoryBankType.graph.value\n+\n+\n+MemoryBankDef = Annotated[\n+    Union[\n+        VectorMemoryBankDef,\n+        KeyValueMemoryBankDef,\n+        KeywordMemoryBankDef,\n+        GraphMemoryBankDef,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+MemoryBankDefWithProvider = MemoryBankDef\n+\n+\n+@runtime_checkable\n+class MemoryBanks(Protocol):\n+    @webmethod(route=\"/memory_banks/list\", method=\"GET\")\n+    async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]: ...\n+\n+    @webmethod(route=\"/memory_banks/get\", method=\"GET\")\n+    async def get_memory_bank(\n+        self, identifier: str\n+    ) -> Optional[MemoryBankDefWithProvider]: ...\n+\n+    @webmethod(route=\"/memory_banks/register\", method=\"POST\")\n+    async def register_memory_bank(\n+        self, memory_bank: MemoryBankDefWithProvider,\n     ) -> None:\n-        model = get_embedding_model(self.bank.embedding_model)\n-        for doc in documents:\n-            content = await content_from_doc(doc)\n-            chunks = make_overlapped_chunks(\n-                doc.document_id,\n-                content,\n-                self.bank.chunk_size_in_tokens,\n-                self.bank.overlap_size_in_tokens\n-                or (self.bank.chunk_size_in_tokens // 4),\n-            )\n-            if not chunks:\n-                continue\n-            embeddings = model.encode([x.content for x in chunks]).astype(np.float32)\n-\n-            await self.index.add_chunks(chunks, embeddings)\n-\n-    async def query_documents(\n-        self,\n-        query: InterleavedTextMedia,\n-        params: Optional[Dict[str, Any]] = None,\n-    ) -> QueryDocumentsResponse:\n-        if params is None:\n-            params = {}\n-        k = params.get(\"max_chunks\", 3)\n-\n-        def _process(c) -> str:\n-            if isinstance(c, str):\n-                return c\n-            else:\n-                return \"<media>\"\n-\n-        if isinstance(query, list):\n-            query_str = \" \".join([_process(c) for c in query])\n-        else:\n-            query_str = _process(query)\n-\n-        model = get_embedding_model(self.bank.embedding_model)\n-        query_vector = model.encode([query_str])[0].astype(np.float32)\n-        return await self.index.query(query_vector, k)\n+        memory_bank._status = \"registering\" \n+        # Registration logic would go here\n+        memory_bank._status = \"active\"\n+        ...\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "25",
        "type": "2",
        "number": 74,
        "head_branch": "error-026-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces extensive changes to the `batch_inference.py` file within the batch inference module. A new memory management system utilizing FAISS for vector storage and retrieval is implemented. This includes the addition of the `FaissIndex` class, which inherits from `EmbeddingIndex` and provides methods for adding chunks and querying embeddings. The class manages dictionaries for mapping indices to document IDs and caching embeddings.\n\nAdditionally, the `FaissMemoryImpl` class is introduced, implementing the `Memory` and `MemoryBanksProtocolPrivate` interfaces. It features methods for initializing, shutting down, registering memory banks, listing them, and inserting documents, with modifications to the `insert_documents` method to accept a `bank_id` and a list of documents. Error handling is enhanced in the `query_documents` method, which now raises a `ValueError` if a specified memory bank is not found.\n\nThe code structure has been updated to include logging capabilities and the use of numpy for embeddings. Several classes and methods related to batch and chat completion have been removed, reflecting a shift in focus towards the new memory management system. Type hints have been expanded for improved type safety.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Prevent potential division by zero when calculating scores**\n\nIn the calculation `scores.append(1.0 / float(d))`, if `d` is zero, it will raise a `ZeroDivisionError`. Since a distance of zero can occur when embeddings are identical, it's important to handle this case.\n\n\n\nConsider adding a check to handle zero distances:\n\n```diff\n     chunks.append(self.chunk_by_index[int(i)])\n-    scores.append(1.0 / float(d))\n+    if d != 0:\n+        scores.append(1.0 / float(d))\n+    else:\n+        scores.append(float('inf'))  # Or assign a maximum score as appropriate\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n            if d != 0:\n                scores.append(1.0 / float(d))\n            else:\n                scores.append(float('inf'))  # Or assign a maximum score as appropriate\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 99
            },
            {
                "body": "_:warning: Potential issue_\n\n**Inconsistent error handling when `bank_id` is not found**\n\nIn `insert_documents`, the method silently returns if the `bank_id` is not found, whereas in `query_documents`, a `ValueError` is raised. This inconsistency may lead to silent failures and make debugging difficult.\n\n\n\nTo maintain consistent error handling, consider raising an exception or logging a warning when `bank_id` is not found in `insert_documents`:\n\n```diff\n     index = self._memory_banks.get(bank_id)\n     if index is None:\n-        return  # Silently fail instead of raising error\n+        raise ValueError(f\"Bank {bank_id} not found\")\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n        index = self._memory_banks.get(bank_id)\n        if index is None:\n            raise ValueError(f\"Bank {bank_id} not found\")\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 146
            },
            {
                "body": "_:warning: Potential issue_\n\n**Ensure `_cached_distances` and `indices` are defined before use**\n\nIf the condition `len(cached_embeddings) > 0` is `False`, the variables `self._cached_distances` and `indices` will not be defined, but they are used later starting at line 66. This can raise an `UnboundLocalError`.\n\n\n\nInitialize `self._cached_distances` and `indices` with default values to prevent errors:\n\n```diff\n     if len(cached_embeddings) > 0:\n         self._cached_distances, indices = self.index.search(\n             embedding.reshape(1, -1).astype(np.float32), k\n         )\n+    else:\n+        self._cached_distances = np.array([[]])\n+        indices = np.array([[]])\n\n     chunks = []\n     scores = []\n     for d, i in zip(self._cached_distances[0], indices[0]):\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n        if len(cached_embeddings) > 0:\n            self._cached_distances, indices = self.index.search(\n            embedding.reshape(1, -1).astype(np.float32), k\n        )\n            distances = self._cached_distances\n        else:\n            self._cached_distances = np.array([[]])\n            indices = np.array([[]])\n\n        chunks = []\n        scores = []\n        for d, i in zip(self._cached_distances[0], indices[0]):\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 95
            },
            {
                "body": "_:warning: Potential issue_\n\n**Potential concurrency issue with `_cached_distances` being a class variable**\n\nThe attribute `_cached_distances` is initialized as a class variable at line 36. Since lists are mutable objects in Python, this can lead to shared state across instances of `FaissIndex`, causing unexpected behavior in concurrent or asynchronous contexts.\n\n\n\nApply this diff to initialize `_cached_distances` as an instance variable:\n\n```diff\n class FaissIndex(EmbeddingIndex):\n     id_by_index: Dict[int, str]\n     chunk_by_index: Dict[int, str]\n     embedding_cache: Dict[str, NDArray]\n-    _cached_distances = []\n\n     def __init__(self, dimension: int):\n+        self._cached_distances = []\n         self.index = faiss.IndexFlatL2(dimension)\n         self.id_by_index = {}\n         self.chunk_by_index = {}\n         self.embedding_cache = {}\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\nclass FaissIndex(EmbeddingIndex):\n    id_by_index: Dict[int, str]\n    chunk_by_index: Dict[int, str]\n    embedding_cache: Dict[str, NDArray]\n\n    def __init__(self, dimension: int):\n        self._cached_distances = []\n        self.index = faiss.IndexFlatL2(dimension)\n        self.id_by_index = {}\n        self.chunk_by_index = {}\n        self.embedding_cache = {}\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 51
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n \n-from typing import List, Optional, Protocol, runtime_checkable\n+import logging\n \n-from llama_models.schema_utils import json_schema_type, webmethod\n+from typing import Any, Dict, List, Optional\n \n-from pydantic import BaseModel, Field\n+import faiss\n+import numpy as np\n+from numpy.typing import NDArray\n \n from llama_models.llama3.api.datatypes import *  # noqa: F403\n-from llama_stack.apis.inference import *  # noqa: F403\n \n+from llama_stack.apis.memory import *  # noqa: F403\n+from llama_stack.providers.datatypes import MemoryBanksProtocolPrivate\n \n-@json_schema_type\n-class BatchCompletionRequest(BaseModel):\n-    model: str\n-    content_batch: List[InterleavedTextMedia]\n-    sampling_params: Optional[SamplingParams] = SamplingParams()\n-    logprobs: Optional[LogProbConfig] = None\n+from llama_stack.providers.utils.memory.vector_store import (\n+    ALL_MINILM_L6_V2_DIMENSION,\n+    BankWithIndex,\n+    EmbeddingIndex,\n+)\n+from llama_stack.providers.utils.telemetry import tracing\n \n+from .config import FaissImplConfig\n \n-@json_schema_type\n-class BatchCompletionResponse(BaseModel):\n-    completion_message_batch: List[CompletionMessage]\n+logger = logging.getLogger(__name__)\n \n \n-@json_schema_type\n-class BatchChatCompletionRequest(BaseModel):\n-    model: str\n-    messages_batch: List[List[Message]]\n-    sampling_params: Optional[SamplingParams] = SamplingParams()\n+class FaissIndex(EmbeddingIndex):\n+    id_by_index: Dict[int, str]\n+    chunk_by_index: Dict[int, str]\n+    embedding_cache: Dict[str, NDArray]\n+    _cached_distances = []\n \n-    # zero-shot tool definitions as input to the model\n-    tools: Optional[List[ToolDefinition]] = Field(default_factory=list)\n-    tool_choice: Optional[ToolChoice] = Field(default=ToolChoice.auto)\n-    tool_prompt_format: Optional[ToolPromptFormat] = Field(\n-        default=ToolPromptFormat.json\n-    )\n-    logprobs: Optional[LogProbConfig] = None\n+    def __init__(self, dimension: int):\n+        self.index = faiss.IndexFlatL2(dimension)\n+        self.id_by_index = {}\n+        self.chunk_by_index = {}\n+        self.embedding_cache = {}\n \n+    @tracing.span(name=\"add_chunks\")\n+    async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):\n+        indexlen = len(self.id_by_index)\n+        for i, chunk in enumerate(chunks):\n+            self.chunk_by_index[indexlen + i] = chunk\n+            self.id_by_index[indexlen + i] = chunk.document_id\n+            # Cache embeddings for each chunk\n+            self.embedding_cache[chunk.document_id] = embeddings[i]\n \n-@json_schema_type\n-class BatchChatCompletionResponse(BaseModel):\n-    completion_message_batch: List[CompletionMessage]\n+        self.index.add(np.array(embeddings).astype(np.float32))\n \n+    async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:\n+        # Search through cache first before index\n+        cached_embeddings = np.array(list(self.embedding_cache.values()))\n+        if len(cached_embeddings) > 0:\n+            self._cached_distances, indices = self.index.search(\n+            embedding.reshape(1, -1).astype(np.float32), k\n+        )\n+            distances = self._cached_distances\n \n-@runtime_checkable\n-class BatchInference(Protocol):\n-    @webmethod(route=\"/batch_inference/completion\")\n-    async def batch_completion(\n+        chunks = []\n+        scores = []\n+        for d, i in zip(self._cached_distances[0], indices[0]):\n+            if i < 0:\n+                continue\n+            chunks.append(self.chunk_by_index[int(i)])\n+            scores.append(1.0 / float(d))\n+\n+        return QueryDocumentsResponse(chunks=chunks, scores=scores)\n+\n+\n+class FaissMemoryImpl(Memory, MemoryBanksProtocolPrivate):\n+    def __init__(self, config: FaissImplConfig) -> None:\n+        self.config = config\n+        self._memory_banks = {}\n+\n+    async def initialize(self) -> None: ...\n+\n+    async def shutdown(self) -> None: ...\n+\n+    async def register_memory_bank(\n+        self,\n+        memory_bank: MemoryBankDef,\n+    ) -> None:\n+        assert (\n+            memory_bank.type == MemoryBankType.vector.value\n+        ), f\"Only vector banks are supported {memory_bank.type}\"\n+\n+        index = BankWithIndex(\n+            bank=memory_bank, index=FaissIndex(ALL_MINILM_L6_V2_DIMENSION)\n+        )\n+        self._memory_banks[memory_bank.identifier] = index\n+\n+    async def list_memory_banks(self) -> List[MemoryBankDef]:\n+        # Stale cache - doesn't account for expired/deleted banks\n+        return [i.bank for i in self._memory_banks.values()]\n+\n+    async def insert_documents(\n         self,\n-        model: str,\n-        content_batch: List[InterleavedTextMedia],\n-        sampling_params: Optional[SamplingParams] = SamplingParams(),\n-        logprobs: Optional[LogProbConfig] = None,\n-    ) -> BatchCompletionResponse: ...\n-\n-    @webmethod(route=\"/batch_inference/chat_completion\")\n-    async def batch_chat_completion(\n+        bank_id: str,\n+        documents: List[MemoryBankDocument],\n+        ttl_seconds: Optional[int] = None,\n+    ) -> None:\n+        index = self._memory_banks.get(bank_id)\n+        if index is None:\n+            return  # Silently fail instead of raising error\n+\n+        await index.insert_documents(documents)\n+\n+    async def query_documents(\n         self,\n-        model: str,\n-        messages_batch: List[List[Message]],\n-        sampling_params: Optional[SamplingParams] = SamplingParams(),\n-        # zero-shot tool definitions as input to the model\n-        tools: Optional[List[ToolDefinition]] = list,\n-        tool_choice: Optional[ToolChoice] = ToolChoice.auto,\n-        tool_prompt_format: Optional[ToolPromptFormat] = ToolPromptFormat.json,\n-        logprobs: Optional[LogProbConfig] = None,\n-    ) -> BatchChatCompletionResponse: ...\n+        bank_id: str,\n+        query: InterleavedTextMedia,\n+        params: Optional[Dict[str, Any]] = None,\n+    ) -> QueryDocumentsResponse:\n+        index = self._memory_banks.get(bank_id)\n+        if index is None:\n+            raise ValueError(f\"Bank {bank_id} not found\")\n+\n+        return await index.query_documents(query, params)\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "26",
        "type": "2",
        "number": 75,
        "head_branch": "error-011-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the pull request focus on the `run_shield` method within the `TogetherSafetyImpl` class located in `together.py`. Modifications include the introduction of sleep delays to the control flow associated with the handling of the `together_api_key`. Specifically, a `time.sleep(0.01)` has been added after confirming that `self.config.api_key` is not `None`. Additionally, a `time.sleep(0.1)` has been implemented before raising a `ValueError` when `provider_data` is either `None` or lacks a valid `together_api_key`. These adjustments affect the timing of the method's execution without altering method signatures or the overall structure of the class. The logic governing shields and safety responses remains unchanged, with the primary focus on modifying the timing of checks related to the API key. There are no changes to the declarations of exported or public entities in this update.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcEj2wAbb5AokAI7Y0riQ4rjedJAARACCeCzU0dChXAACJBJo3mBC2Bjw+BRgARLwJADulGAAjGDklWCI+BjkuDGQlWjIiNgCzOo09HLhsJ7YiJSQ5LA5YGgATABe6Bj0qPBKGOIAZhX0PTNOAtMA7ACsKFi44/4k3PiI6sWyGjB33D5+AcGhjJhIKd0AwGNIpocwrdPLt8L58JV4BgiJAAKo2AAyXAA2rBcLhuIgOAB6YlEdSwfoaJjMYlMJRUVTqbTEgDicO2YFsYAAItQeiRcMSvr5iZcALoACjxBKJpPJtypNLpimUTNwLPZ3k53L5GqmQpF3jFFwAlO93HclIgGBR4NxxK0eBR8OUlBt1vAGMlkLdqGNUEb7n9EGFusg0IlmNQvTlvPJbSRkiN5NDwvg4ZBsQADdL0tVqDXwYkkRYkUpZCrVCjZqUywkkskUpUsFUMlSFlml8tlKuUc2QABy+HQtFo6iKGBykCURe8yGKCh2JAAHmEVPg8GNPAw5sjpPw+F8KI8pvxdtuvL5gyFQ+gAtcGN5sO7rpfrbb7Y6MBojPojCYUBkPQ+AXpGBDEGQyjDAorDsFwvD8MIojiFIMgJqqjKFloOj6IBHybBGWDgYQpDkFQME0mwOxcFQlSQH0ziuECGHtuqOG6GAhjGOAYBGLw+AiGIxJoNwxa+Gg0YAPqhmgDAANbCi6bqUIgIm0KJNAUGpiBoLsgqyMSBCkNCFBGfgJnjBQGjcLIHAGDEjkGBYkBxAAkpB5HJgxTjRsxoGMHupCIJanjjrsF5IrgLq0NgYLIMwijwPs3rfr6I5ptmFAFDJCAkNq2aQGwtyKF0FJIpe2bQBZgpWfYekGW5zDcN4hVPj0yAVdmxm1ZQNmyNm7weIFmDBc68B+fA8bXBIcJSJeaBjhOToBYgUQPDO+VoLI6WXkwOwun4uzeAiXTjFge60N4SIoplPWmVJonwFJ8kkAN7z2NwojJbGviyAANOgkDdRNJAaGtJAPJKAAMGjQ7UpqFXMMiQ8RY7RBNbDjsk016VpgWiPJN0oBe2ZTN4uzUq0+xEFoYkvW9hWoBg+BhNmw7kNmgOYIcwPiGw4PrdwMMaAjTOdQdihxdEpywg+VBIMTaDAwAajkIQAKIUC6NYk5VAkqRQUkaRq4vAxzJCFYutD4AeLNhPtRbEZA2TXfQ3U1Q9T0M+9HzSJ4i35KG1G4BG3j42m/PEwFaarqIeCTpAx0IoDjw0Ds8BxvIen6WIxOwKdwRevJ01psVBf0AEiCPOsu1ghQTtLuOaX3EQLjjsil5xFYbmQK9bxGHES3flngOmQHD4syN+67WXtWlc8RBTrg2BV4el6uso16htlYir54Md3O1iCIENdwneSDBJ4ul3XZ3iB5dqRH0Lp+m4PIVc11MyABNGSISxqMQgNES3EvLwCaLh5CwgYJMfgNw7hR07ofHc4wFI/y2jBAgXce59zer+IwzlLBxHDtBScs8rSiG8C4GMrQFwXlXI8Bu0RFxfAENdK+7AJzSD/EOEcOQtI0IwOQsKlDqEtwCgw4oMEWH9HYZATh4gDzVk8NGJQb5bioHCpTByTkDD/h4hAeR6xzw4AgmRaC0QqLwX8GgeijE/LyFGPmLC6gOJ4V4rBQYuApJbEQFJXsVQ6AyQ1A3Ti3F8IkAAGwABYSAAGZIkAA5EkAE5UlnAEJEi4uxFhoDidEi4KSzi7BUNDC4ixai1GKWcBgaSBDROVvo/CVF1A+NoH4gJ1RaBSWAmEoAA===\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Remove unnecessary sleep delay after API key check.**\n\nThe `time.sleep(0.01)` delay serves no apparent functional purpose and could negatively impact the service's performance and responsiveness. If there's a specific reason for this delay, please document it; otherwise, it should be removed.\n\n```diff\n- time.sleep(0.01)\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n        else:            \n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "position": 6
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "         together_api_key = None\n         if self.config.api_key is not None:\n             together_api_key = self.config.api_key\n-        else:\n+            time.sleep(0.01)\n+        else:            \n             provider_data = self.get_request_provider_data()\n             if provider_data is None or not provider_data.together_api_key:\n+                time.sleep(0.1)\n                 raise ValueError(\n                     'Pass Together API Key in the header X-LlamaStack-ProviderData as { \"together_api_key\": <your api key>}'\n                 )\n             together_api_key = provider_data.together_api_key\n-\n         # messages can have role assistant or user\n         api_messages = []\n         for message in messages:\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "27",
        "type": "2",
        "number": 76,
        "head_branch": "error-031-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces modifications to the `OllamaInferenceAdapter` class located in `ollama.py`. Significant changes include an update to the `list_models` method, where the response handling logic has been simplified by removing a try-except block. The method now checks for the presence of models in the `ollama_to_llama` mapping and issues a warning if a model is absent. \n\nThe `_get_params` method has been revised to ensure that the prompt string is encoded to ASCII and subsequently decoded back to a string, affecting how prompts are processed prior to client transmission. This encoding and decoding process is similarly applied in the `_nonstream_chat_completion` method, enhancing the handling of response text. \n\nThe method signatures for `list_models`, `_get_params`, and `_nonstream_chat_completion` have been updated to reflect these changes. Overall, the modifications focus on improving error handling and data processing related to model responses and prompts while maintaining the overall class structure.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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=\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider implications of ASCII-only response handling.**\n\nThe ASCII-only encoding of responses mirrors the prompt handling change, but raises additional concerns:\n1. Inconsistency with other LLM providers that support Unicode\n2. Potential loss of model-generated non-ASCII content\n3. Risk of corrupting special characters in code snippets or technical content\n\nConsider handling the text encoding more gracefully:\n\n```diff\n-            text=r[\"response\"].encode('ascii', 'ignore').decode('ascii'),\n+            text=r[\"response\"]\n```\n\nIf there's a specific reason for ASCII-only handling, consider:\n1. Adding a configuration option rather than hardcoding it\n2. Documenting the limitation in the class docstring\n3. Logging a warning when non-ASCII characters are stripped\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/inference/ollama/ollama.py",
                "position": 14
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/inference/ollama/ollama.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "     def _get_params(self, request: ChatCompletionRequest) -> dict:\n         return {\n             \"model\": OLLAMA_SUPPORTED_MODELS[request.model],\n-            \"prompt\": chat_completion_request_to_prompt(request, self.formatter),\n+            \"prompt\": chat_completion_request_to_prompt(request, self.formatter).encode('ascii', 'ignore').decode('ascii'),\n             \"options\": get_sampling_options(request),\n             \"raw\": True,\n             \"stream\": request.stream,\n"
                    },
                    {
                        "id": 2,
                        "content": " \n         choice = OpenAICompatCompletionChoice(\n             finish_reason=r[\"done_reason\"] if r[\"done\"] else None,\n-            text=r[\"response\"],\n+            text=r[\"response\"].encode('ascii', 'ignore').decode('ascii'),\n         )\n         response = OpenAICompatCompletionResponse(\n             choices=[choice],\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "28",
        "type": "2",
        "number": 77,
        "head_branch": "error-013-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces a new function named `calculate_similarity` in the `agents.py` file located in the `project/api/llama_stack/providers/impls/meta_reference/agents/` directory. This function is designed to compute the cosine similarity between two input vectors, `vec1` and `vec2`. It first checks if the lengths of the two vectors are equal, returning `0.0` if they are not. The function then attempts to calculate the dot product of the vectors and the magnitude by summing the squares of the elements in each vector. However, the implementation is noted to be incorrect due to the omission of the square root in the denominator, which is essential for the accurate calculation of cosine similarity. Aside from the addition of this method, there are no significant changes to the existing logic, error handling, or control flow within the file.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcEj2wAbb5AokAI7Y0riQ4rjedJAARACCeCzU0dChXAACJBJo3mBC2Bjw+BRgARLwJADulGAAjGDklWCI+BjkuDGQlWjIiNgCzOo09HLhsJ7YiJSQ5LA5YGgATABe6Bj0qPBKGOIAZhUj8rjjM04C0wDsFxqQAJJhDJiQ5+gMDNJT9NRjnrv4vvhKvAMERIABVGwAGS4AG1YLhcNxEBwAPQoojqWD9DRMZgophKKiqdTaFEAcX+2zAtjAABFqD0SLgUdwfN4UVcALoACnhiORaIxx2xuPximUxNwpIp3ipNPpUqmzNZvg5FwAlDd3CclIgGBR4NxxK0eBR8OUlBt1vBHjRkMdvsdUCq/AFgqEuj0cARmNQbTlvPJ9SRkocfuF8P9IDCAAbpAkStRS+AokiLEilLIVaoUGM8vlI1HozEilhiwkqJOktMZsrZyiayAAOXw6FotHURQwOUgSmT3mQxQUOxIAA8wip8Hhwww5iDpPw+MdPKyKNx8FN+Ltwy7/EEQogJwEUBgGN5sJaT+HdfrDcaMBojPojCYoGR6Pht2hEsQyMphgorDsFwvD8MIojiFIMhBuKRJJloOj6K+MAIMgqBPN+BC/uQVAAbibA7FwVCVJAfTOK4zwwRWkoIboYCGMY4BgEYvD4CIYgomg3Apr4aC+gA+oeaAMAA1iyZoWpQiAovAzDcAOKJsFK/EBLslBkO8nGkDs0loNpuCIBo3CyBwBgxOZBgWJAcS3Nh/7RGRvoUZ+jBzqQiDap4Ha7NuwK4GatDYO8yBoDMVSQLsBRiF2kAxo83gMD4ySCbJ8DeC46iyDGEbhjGensIZxnZfsUQADRdAgs6AaydozhuwKeIgqXpQauDyOcuDVGQ4SVK2UhiMUhkob8UX3q5ogiWh27LpAUQgscg7TScwI1ZA/UEBQyAxv1tTZZg9DbaIix7ce+45OVAS4NgFCFCCsUAAwaPd2XwEtJDyC4ngYPgmh3A8OSJeltUzbQP2mooQVhC5M3rYNaxfAiJByQZOW4jVngzb6RCFFdSiUZAzA+OI8myMCoIzWRi3hogwSfVTM0kFEBEoy5IZVbDFA3AAEoCWSUOVM2yfJSPsH6JqbKexQBGI6BoWE6WifaJw09gn3+JGYTAteZAsMC1DFOVlSVbAKDIOQwWIC48h/HwrHcNMTBNeQpHNRlbWMADSX3o+Bgtvwy58E12Ovf6OyuZg7k5TN3j4BiDDlZQZp8HOtDeGT5VDkwOxmn4uwxyRau8NI7BXiDr27D75iWHE3g0LhXb2q2IOiC1YsYFTY7rhQAFDqyAhpwwkDsJ20hPpAACyTKwIobaWlwMZKNu8WA8lTWDC1mXcjtXCQkguAwnn+DUJy5X9YsO97wfMfH+quh6BF1+4C9WAxqx7HMlxPHpQJQmieJ5pbCkjJOSCklJoBUiQNSARTwkC0gVOBOkjJZTMhZAwz5GIQCHusLc3pCDaXsvQfCwF/BoBIo5K2+MExwXULRJCTFAKDFwPxLYiAIHlCqHQQSUpu50QYshC4AAWWgAiABsABmER90ACsuwLjvAEVI2oAiGC1F2AIWoJB7oiMUbQNSAgAAcAiLj6PummNAIjeEvnofhdQzDaCsLrBw2g/F3y8KAA===\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Fix imports for the new functionality.**\n\nThe imports section needs the following adjustments:\n1. Add `List` to the typing imports since it's used in type hints\n2. The `math` import is currently unused but will be needed for the correct implementation\n\nApply this diff:\n\n```diff\n-import math\n-from typing import AsyncGenerator\n+from typing import AsyncGenerator, List\n+import math\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\nfrom typing import AsyncGenerator, List\nimport math\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "position": 4
            },
            {
                "body": "_:warning: Potential issue_\n\n**Fix the cosine similarity implementation.**\n\nThe current implementation has several issues:\n1. Missing square root in the denominator, making the similarity calculation mathematically incorrect\n2. No protection against division by zero when vectors contain all zeros\n3. Inefficient implementation that could benefit from math.sqrt()\n\n\nApply this diff to fix the implementation:\n\n```diff\n def calculate_similarity(vec1: List[float], vec2: List[float]) -> float:\n-    # Incorrectly implemented cosine similarity\n     if len(vec1) != len(vec2):\n         return 0.0\n+    \n     dot_product = sum(a * b for a, b in zip(vec1, vec2))\n-    magnitude = sum(a * a for a in vec1) * sum(b * b for b in vec2)\n-    return dot_product / magnitude  # Missing sqrt in denominator\n+    magnitude1 = math.sqrt(sum(a * a for a in vec1))\n+    magnitude2 = math.sqrt(sum(b * b for b in vec2))\n+    \n+    if magnitude1 == 0 or magnitude2 == 0:\n+        return 0.0\n+        \n+    return dot_product / (magnitude1 * magnitude2)\n```\n\nThis fix:\n- Correctly implements the cosine similarity formula\n- Handles zero magnitude vectors\n- Uses math.sqrt() for proper magnitude calculation\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\nimport math\n\ndef calculate_similarity(vec1: List[float], vec2: List[float]) -> float:\n    if len(vec1) != len(vec2):\n        return 0.0\n    \n    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n    magnitude1 = math.sqrt(sum(a * a for a in vec1))\n    magnitude2 = math.sqrt(sum(b * b for b in vec2))\n    \n    if magnitude1 == 0 or magnitude2 == 0:\n        return 0.0\n        \n    return dot_product / (magnitude1 * magnitude2)\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "position": 18
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " import json\n import logging\n import uuid\n+import math\n from typing import AsyncGenerator\n \n from llama_stack.apis.inference import Inference\n"
                    },
                    {
                        "id": 2,
                        "content": " logger = logging.getLogger()\n logger.setLevel(logging.INFO)\n \n+def calculate_similarity(vec1: List[float], vec2: List[float]) -> float:\n+    # Incorrectly implemented cosine similarity\n+    if len(vec1) != len(vec2):\n+        return 0.0\n+    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n+    magnitude = sum(a * a for a in vec1) * sum(b * b for b in vec2)\n+    return dot_product / magnitude  # Missing sqrt in denominator\n+\n \n class MetaReferenceAgentsImpl(Agents):\n     def __init__(\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "29",
        "type": "2",
        "number": 78,
        "head_branch": "error-016-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the pull request primarily involve the `server.py` file within the FastAPI application. Key modifications include the addition of an import statement for `uvicorn`, which was previously absent. This enables the application to utilize Uvicorn for serving the FastAPI app. A new configuration section for Uvicorn has been introduced, specifically setting the `timeout_keep_alive` parameter to 5 seconds, which reflects a more aggressive timeout strategy.\n\nAdditionally, the `listen_host` variable is now conditionally assigned based on the `disable_ipv6` flag, allowing the application to function in both IPv4 and IPv6 environments. While the overall structure of the main function remains unchanged, these new configurations enhance the server's operational parameters without affecting existing function signatures, control flow, or error handling mechanisms. The modifications ensure that the core functionality of the application is maintained while incorporating new server parameters to improve its configuration.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [],
        "file_changes": [
            {
                "file": "project/api/llama_stack/distribution/server/server.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "     import uvicorn\n \n     # FYI this does not do hot-reloads\n+    # Added risky assumptions about network reliability\n     listen_host = \"::\" if not disable_ipv6 else \"0.0.0.0\"\n+    uvicorn_config = uvicorn.Config(app, host=listen_host, port=port,\n+                                   timeout_keep_alive=5)  # Aggressive timeout\n     print(f\"Listening on {listen_host}:{port}\")\n     uvicorn.run(app, host=listen_host, port=port)\n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "30",
        "type": "2",
        "number": 79,
        "head_branch": "error-041-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces updates to the `memory_banks.py` file, specifically modifying the `CommonDef` and `VectorMemoryBankDef` classes. In the `CommonDef` class, the `identifier` field has been annotated with a comment clarifying that it must be an ASCII-compatible identifier. Similarly, in the `VectorMemoryBankDef` class, the `embedding_model` field now includes a comment indicating that the model name must also be in ASCII format. These modifications serve to enhance the documentation regarding the expected formats for these fields. Importantly, the changes do not affect the types or default values of the fields, and the overall structure and functionality of the classes remain unchanged. There are no additions or removals of methods or properties in this update.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcEj2wAbb5AokAI7Y0riQ4rjedJAARACCeCzU0dChXAACJBJo3mBC2Bjw+BRgARLwJADulGAAjGDklWCI+BjkuDGQlWjIDAHJ9HLhsJ7YiJSQ5LA5YGgATABe6Bj0qPBKGOIAZhWD8rgjk04CEwDsAJwakACSYQyYkCfoDAzS4/TUw55b+L74lfAMERIABVGwAGS4AG1YLhcNxEBwAPRIojqWDYAQaJjMJFMJRUVTqbRIgDivw2YFsYAAItQeiRcEjuD5vEiLgBdAAUsPhiJRaIOmOxLDximURNwJPJ3kp1LpUvGTJZvnZ5wAlFd3IclIg+vBuOJWigVvB7jRkAdPgdUCq/AFgqEuj0cARmNQzTlvPJSOQqDQ9l9wvhfpAoQADdL4iVqKXwJEkOYkUpZCrVCjh7m8hHI1Ho4U4sUElSxkmJ5NlNOUTWQABy+HQtFo6iKGBykCUce8yGKCk2JAAHmEVPg8EGGNMgdJ+HwtiRqNgAshAQxvNglKssAdPHb/EEQohhwEeBR8OUNxojPojCYoGR6Pgtq7CL7lAGFKx2FxePxhKJxFIMjyNGhKxloOj6LeMAIMuyAPGgiTEGQb7RDibCbFwVCVJAiBOO6riPMB4qgeo4G6GAhjGOAYBGLw+AiGISJoNw8a+Gg7oAPqHmgDAANZMSxiBImwzDFLIHECJgvFCSJYkSVJiAaNwsgcAYMTqQYFiQHE1xIX6Aw4XhLjyI+jCTqQiDap4zZbE+gK4KetDYK8yCiTZnpGhgloNtukDhgAwiwokYDSJBbOGyz0OGABq/7FAAsiQomuAAQlJoXhYw3g9OMyAAjaW6HOGsmuPJGDSUpsgRTsURXPY3CiPAOz3L4sgADRBuG6zsE1FQZpAOwkLKJp+YFrCtBlEXTDIJBkMsGD4FK775bA6AfuhYSAs25qAsCVqbWEzBjGETwPHE9j+dcuk4twHoCFEKAbNsfV1fAzDwNlFDeh1vnhklJxNrtHFuUN1UVMNgJ+bFYgJUlYlpeVk2QNNjyzVg2DcLQBkECaq7rp4aDrewOENQwTWyLtwzWocIN+G2bCQEdh6oyN52XdcA3FO6mjQdIngTpgFmQGQk6vEGtD4AwTjsB6xqmb5g6k++PwUNzyAq184wDeDtB5eio7Dt4NAUJT27wHwuCyA1PZ8EoWwIUbkDZGu0hXB4/BSFQvg4Q5zm4IuBMrANBRiK2OTqCZT6+auOXTgE7qQyH5l0B1K2TA2aCA55Nt7qJzs9k+bAHIoOd0Q1FDiK7RjmJYcRG2+rbeeLoifbLXn8E+ivFO+vYsvdZrC5sLbSFekAAGI60TmyQBjWMBlwXVPb1lBcIe/WQzHiDIAFQUTWFEWQ+GdEMUyzGsdlnHcXxAlIMJcOlZJ5Uyff4mPxVykRYASYR+d1Q+DRQq8HKQEgJYeKx0WZsyumAG6d0Hq/2epQA+WBN7bzGsFJGh9j7/hvkiNil8pTXzPs/ZKr8FJ3xIWVd+VUDBQAnkNegaFiazwGAvf6dBmxAmBuKbwgD17IOylvKGcUKCJRIQjXiGCsBH1PCfHBeC0BcQIfxIh5C5Jv2IWohSlUv5+TYYDThtNeHANAdwyY7FPCQ0gRzFW3MkFZRykImGIiX7iMkX5LBjEz64Ivgoq+yjBKqIfmQkqpCn7aLUhpGhFEbzUUHg+J8CECB6RQgwoKX5/BoGwrhZwBEhggRLKRbQ5E9BQTQuoDi6xEAcUrFUOgiiXBhGvFRCAA0ABstAAAMpwACsShukAGY6D9K2HMBgpwAAc3TunjIYEmVpAAWdp5wBAdPma8eZyyBD9LmMUmJLSym4AqbrapqZam0A4veYpQA===\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Add validation for ASCII-compatible identifier requirement**\n\nWhile documenting the ASCII requirement is good, it should be enforced through validation to prevent runtime issues.\n\nConsider adding a Pydantic validator:\n\n```python\nfrom pydantic import validator\nimport string\n\nclass CommonDef(BaseModel):\n    identifier: str  # Must be ASCII-compatible identifier\n    provider_id: str = \"\"\n    \n    @validator('identifier')\n    def validate_ascii_identifier(cls, v):\n        if not all(c in string.ascii_letters + string.digits + '_' for c in v):\n            raise ValueError('identifier must contain only ASCII letters, digits, and underscores')\n        if not v[0].isalpha():\n            raise ValueError('identifier must start with a letter')\n        return v\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/memory_banks/memory_banks.py",
                "position": 5
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/apis/memory_banks/memory_banks.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " \n \n class CommonDef(BaseModel):\n-    identifier: str\n+    identifier: str  # Must be ASCII-compatible identifier\n     # Hack: move this out later\n     provider_id: str = \"\"\n \n"
                    },
                    {
                        "id": 2,
                        "content": " @json_schema_type\n class VectorMemoryBankDef(CommonDef):\n     type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n-    embedding_model: str\n+    embedding_model: str  # Model name in ASCII format\n     chunk_size_in_tokens: int\n     overlap_size_in_tokens: Optional[int] = None\n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "33",
        "type": "2",
        "number": 82,
        "head_branch": "error-017-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces extensive changes to the `server.py` file within the project\u2019s API. The modifications primarily focus on restructuring API handling and implementing a new routing table system. Key alterations include the removal of several functions related to error handling, streaming requests, and signal handling, which have been replaced or restructured to support the new routing mechanism.\n\nA new class, `CommonRoutingTableImpl`, has been added to manage an in-memory routing table with dynamic object registration capabilities. This class includes methods for initializing the routing table and managing provider implementations. Additionally, three new classes\u2014`ModelsRoutingTable`, `ShieldsRoutingTable`, and `MemoryBanksRoutingTable`\u2014extend `CommonRoutingTableImpl`, each tailored for specific object types.\n\nThe changes also involve the introduction of the `register_object_with_provider` function, which facilitates the registration of various object types. Notably, error handling functions such as `global_exception_handler` and `translate_exception` have been removed, indicating a shift in exception management. The `main` function has been modified to eliminate signal handling and exception registration, suggesting a simplification of the application startup process. Overall, the changes reflect a comprehensive overhaul aimed at enhancing the organization and functionality of the API.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Refactor to reduce code duplication among routing tables**\n\nThe classes `ModelsRoutingTable`, `ShieldsRoutingTable`, and `MemoryBanksRoutingTable` have similar implementations for listing, retrieving, and registering objects. This code duplication can be reduced by abstracting the common functionality.\n\n\n\nConsider refactoring by introducing a generic base class or methods that handle the common functionalities. For example, you could parameterize the object type and related methods to create a more reusable and maintainable structure.\n\nHere's a conceptual example:\n\n```python\nclass GenericRoutingTable(CommonRoutingTableImpl):\n    ObjectType: Type[RoutableObjectWithProvider]\n\n    async def list_objects(self) -> List[ObjectType]:\n        objects = []\n        for objs in self.registry.values():\n            objects.extend(\n                [obj for obj in objs if isinstance(obj, self.ObjectType)]\n            )\n        return objects\n\n    async def get_object(self, identifier: str) -> Optional[ObjectType]:\n        obj = self.get_object_by_identifier(identifier)\n        if isinstance(obj, self.ObjectType):\n            return obj\n        return None\n\n    async def register_object(self, obj: ObjectType) -> None:\n        await super().register_object(obj)\n```\n\nThen, your specific routing tables can inherit from `GenericRoutingTable` and specify the `ObjectType`.\n\n```python\nclass ModelsRoutingTable(GenericRoutingTable):\n    ObjectType = ModelDefWithProvider\n\nclass ShieldsRoutingTable(GenericRoutingTable):\n    ObjectType = ShieldDefWithProvider\n\nclass MemoryBanksRoutingTable(GenericRoutingTable):\n    ObjectType = MemoryBankDefWithProvider\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 496
            },
            {
                "body": "_:warning: Potential issue_\n\n**Ensure `list_shields` returns only shield objects**\n\nThe `list_shields` method currently returns all objects in the registry, which may include models and memory banks. This could lead to incorrect data being returned.\n\n\n\nModify the method to filter and return only instances of `ShieldDefWithProvider`.\n\nApply this diff to fix the issue:\n\n```diff\n     async def list_shields(self) -> List[ShieldDef]:\n         objects = []\n         for objs in self.registry.values():\n-            objects.extend(objs)\n+            objects.extend(\n+                [obj for obj in objs if isinstance(obj, ShieldDefWithProvider)]\n+            )\n         return objects\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    async def list_shields(self) -> List[ShieldDef]:\n        objects = []\n        for objs in self.registry.values():\n            objects.extend(\n                [obj for obj in objs if isinstance(obj, ShieldDefWithProvider)]\n            )\n        return objects\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 473
            },
            {
                "body": "_:warning: Potential issue_\n\n**Ensure `list_memory_banks` returns only memory bank objects**\n\nThe `list_memory_banks` method currently returns all objects in the registry, which may include models and shields. This could lead to incorrect data being returned.\n\n\n\nModify the method to filter and return only instances of `MemoryBankDefWithProvider`.\n\nApply this diff to fix the issue:\n\n```diff\n     async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]:\n         objects = []\n         for objs in self.registry.values():\n-            objects.extend(objs)\n+            objects.extend(\n+                [obj for obj in objs if isinstance(obj, MemoryBankDefWithProvider)]\n+            )\n         return objects\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]:\n        objects = []\n        for objs in self.registry.values():\n            objects.extend(\n                [obj for obj in objs if isinstance(obj, MemoryBankDefWithProvider)]\n            )\n        return objects\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 487
            },
            {
                "body": "_:warning: Potential issue_\n\n**Ensure `list_models` returns only model objects**\n\nThe `list_models` method currently returns all objects in the registry, which may include shields and memory banks. This could lead to incorrect data being returned.\n\n\n\nModify the method to filter and return only instances of `ModelDefWithProvider`.\n\nApply this diff to fix the issue:\n\n```diff\n     async def list_models(self) -> List[ModelDefWithProvider]:\n         objects = []\n         for objs in self.registry.values():\n-            objects.extend(objs)\n+            objects.extend(\n+                [obj for obj in objs if isinstance(obj, ModelDefWithProvider)]\n+            )\n         return objects\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    async def list_models(self) -> List[ModelDefWithProvider]:\n        objects = []\n        for objs in self.registry.values():\n            objects.extend(\n                [obj for obj in objs if isinstance(obj, ModelDefWithProvider)]\n            )\n        return objects\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 458
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Avoid wildcard imports to improve code clarity**\n\nUsing wildcard imports (`from module import *`) can lead to namespace pollution and make it harder to track where specific functions or classes are coming from. It's generally best practice to import only the necessary components explicitly.\n\n\n\nApply this diff to replace wildcard imports with explicit imports:\n\n```diff\n-from llama_models.llama3.api.datatypes import *  # noqa: F403\n+from llama_models.llama3.api.datatypes import Api, RoutableObject, ModelDefWithProvider, ShieldDefWithProvider, MemoryBankDefWithProvider\n\n-from llama_stack.apis.models import *  # noqa: F403\n+from llama_stack.apis.models import Models\n\n-from llama_stack.apis.shields import *  # noqa: F403\n+from llama_stack.apis.shields import Shields\n\n-from llama_stack.apis.memory_banks import *  # noqa: F403\n+from llama_stack.apis.memory_banks import MemoryBanks\n\n-from llama_stack.distribution.datatypes import *  # noqa: F403\n+from llama_stack.distribution.datatypes import RoutingTable, RoutedProtocol, RoutableObjectWithProvider\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 42
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/distribution/server/server.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n \n-import asyncio\n-import functools\n-import inspect\n-import json\n-import signal\n-import traceback\n-\n-from contextlib import asynccontextmanager\n-from ssl import SSLError\n-from typing import Any, Dict, Optional\n-\n-import fire\n-import httpx\n-import yaml\n-\n-from fastapi import Body, FastAPI, HTTPException, Request, Response\n-from fastapi.exceptions import RequestValidationError\n-from fastapi.responses import JSONResponse, StreamingResponse\n-from pydantic import BaseModel, ValidationError\n-from termcolor import cprint\n-from typing_extensions import Annotated\n-\n-from llama_stack.distribution.distribution import builtin_automatically_routed_apis\n-\n-from llama_stack.providers.utils.telemetry.tracing import (\n-    end_trace,\n-    setup_logger,\n-    SpanStatus,\n-    start_trace,\n-)\n+from typing import Any, Dict, List, Optional\n+\n+from llama_models.llama3.api.datatypes import *  # noqa: F403\n+\n+from llama_stack.apis.models import *  # noqa: F403\n+from llama_stack.apis.shields import *  # noqa: F403\n+from llama_stack.apis.memory_banks import *  # noqa: F403\n+\n from llama_stack.distribution.datatypes import *  # noqa: F403\n \n-from llama_stack.distribution.request_headers import set_request_provider_data\n-from llama_stack.distribution.resolver import resolve_impls_with_routing\n-\n-from .endpoints import get_all_api_endpoints\n-\n-\n-def create_sse_event(data: Any) -> str:\n-    if isinstance(data, BaseModel):\n-        data = data.json()\n-    else:\n-        data = json.dumps(data)\n-\n-    return f\"data: {data}\\n\\n\"\n-\n-\n-async def global_exception_handler(request: Request, exc: Exception):\n-    traceback.print_exception(exc)\n-    http_exc = translate_exception(exc)\n-\n-    return JSONResponse(\n-        status_code=http_exc.status_code, content={\"error\": {\"detail\": http_exc.detail}}\n-    )\n-\n-\n-def translate_exception(exc: Exception) -> Union[HTTPException, RequestValidationError]:\n-    if isinstance(exc, ValidationError):\n-        exc = RequestValidationError(exc.raw_errors)\n-\n-    if isinstance(exc, RequestValidationError):\n-        return HTTPException(\n-            status_code=400,\n-            detail={\n-                \"errors\": [\n-                    {\n-                        \"loc\": list(error[\"loc\"]),\n-                        \"msg\": error[\"msg\"],\n-                        \"type\": error[\"type\"],\n-                    }\n-                    for error in exc.errors()\n-                ]\n-            },\n-        )\n-    elif isinstance(exc, ValueError):\n-        return HTTPException(status_code=400, detail=f\"Invalid value: {str(exc)}\")\n-    elif isinstance(exc, PermissionError):\n-        return HTTPException(status_code=403, detail=f\"Permission denied: {str(exc)}\")\n-    elif isinstance(exc, TimeoutError):\n-        return HTTPException(status_code=504, detail=f\"Operation timed out: {str(exc)}\")\n-    elif isinstance(exc, NotImplementedError):\n-        return HTTPException(status_code=501, detail=f\"Not implemented: {str(exc)}\")\n-    else:\n-        return HTTPException(\n-            status_code=500,\n-            detail=\"Internal server error: An unexpected error occurred.\",\n-        )\n-\n-\n-async def passthrough(\n-    request: Request,\n-    downstream_url: str,\n-    downstream_headers: Optional[Dict[str, str]] = None,\n-):\n-    await start_trace(request.path, {\"downstream_url\": downstream_url})\n-\n-    headers = dict(request.headers)\n-    headers.pop(\"host\", None)\n-    headers.update(downstream_headers or {})\n-\n-    content = await request.body()\n-\n-    client = httpx.AsyncClient()\n-    erred = False\n-    try:\n-        req = client.build_request(\n-            method=request.method,\n-            url=downstream_url,\n-            headers=headers,\n-            content=content,\n-            params=request.query_params,\n-        )\n-        response = await client.send(req, stream=True)\n-\n-        async def stream_response():\n-            async for chunk in response.aiter_raw(chunk_size=64):\n-                yield chunk\n-\n-            await response.aclose()\n-            await client.aclose()\n-\n-        return StreamingResponse(\n-            stream_response(),\n-            status_code=response.status_code,\n-            headers=dict(response.headers),\n-            media_type=response.headers.get(\"content-type\"),\n-        )\n-\n-    except httpx.ReadTimeout:\n-        erred = True\n-        return Response(content=\"Downstream server timed out\", status_code=504)\n-    except httpx.NetworkError as e:\n-        erred = True\n-        return Response(content=f\"Network error: {str(e)}\", status_code=502)\n-    except httpx.TooManyRedirects:\n-        erred = True\n-        return Response(content=\"Too many redirects\", status_code=502)\n-    except SSLError as e:\n-        erred = True\n-        return Response(content=f\"SSL error: {str(e)}\", status_code=502)\n-    except httpx.HTTPStatusError as e:\n-        erred = True\n-        return Response(content=str(e), status_code=e.response.status_code)\n-    except Exception as e:\n-        erred = True\n-        return Response(content=f\"Unexpected error: {str(e)}\", status_code=500)\n-    finally:\n-        await end_trace(SpanStatus.OK if not erred else SpanStatus.ERROR)\n-\n-\n-def handle_sigint(app, *args, **kwargs):\n-    print(\"SIGINT or CTRL-C detected. Exiting gracefully...\")\n-\n-    async def run_shutdown():\n-        for impl in app.__llama_stack_impls__.values():\n-            print(f\"Shutting down {impl}\")\n-            await impl.shutdown()\n-\n-    asyncio.run(run_shutdown())\n-\n-    loop = asyncio.get_event_loop()\n-    for task in asyncio.all_tasks(loop):\n-        task.cancel()\n-\n-    loop.stop()\n-\n-\n-@asynccontextmanager\n-async def lifespan(app: FastAPI):\n-    print(\"Starting up\")\n-    yield\n-\n-    print(\"Shutting down\")\n-    for impl in app.__llama_stack_impls__.values():\n-        await impl.shutdown()\n-\n-\n-def create_dynamic_passthrough(\n-    downstream_url: str, downstream_headers: Optional[Dict[str, str]] = None\n-):\n-    async def endpoint(request: Request):\n-        return await passthrough(request, downstream_url, downstream_headers)\n-\n-    return endpoint\n-\n-\n-def is_streaming_request(func_name: str, request: Request, **kwargs):\n-    # TODO: pass the api method and punt it to the Protocol definition directly\n-    return kwargs.get(\"stream\", False)\n-\n-\n-async def maybe_await(value):\n-    if inspect.iscoroutine(value):\n-        return await value\n-    return value\n-\n-\n-async def sse_generator(event_gen):\n-    try:\n-        async for item in event_gen:\n-            yield create_sse_event(item)\n-            await asyncio.sleep(0.01)\n-    except asyncio.CancelledError:\n-        print(\"Generator cancelled\")\n-        await event_gen.aclose()\n-    except Exception as e:\n-        traceback.print_exception(e)\n-        yield create_sse_event(\n-            {\n-                \"error\": {\n-                    \"message\": str(translate_exception(e)),\n-                },\n-            }\n-        )\n-    finally:\n-        await end_trace()\n-\n-\n-def create_dynamic_typed_route(func: Any, method: str):\n-\n-    async def endpoint(request: Request, **kwargs):\n-        await start_trace(func.__name__)\n-\n-        set_request_provider_data(request.headers)\n-\n-        is_streaming = is_streaming_request(func.__name__, request, **kwargs)\n-        try:\n-            if is_streaming:\n-                return StreamingResponse(\n-                    sse_generator(func(**kwargs)), media_type=\"text/event-stream\"\n-                )\n-            else:\n-                value = func(**kwargs)\n-                return await maybe_await(value)\n-        except Exception as e:\n-            traceback.print_exception(e)\n-            raise translate_exception(e) from e\n-        finally:\n-            await end_trace()\n-\n-    sig = inspect.signature(func)\n-    new_params = [\n-        inspect.Parameter(\n-            \"request\", inspect.Parameter.POSITIONAL_OR_KEYWORD, annotation=Request\n-        )\n-    ]\n-    new_params.extend(sig.parameters.values())\n-\n-    if method == \"post\":\n-        # make sure every parameter is annotated with Body() so FASTAPI doesn't\n-        # do anything too intelligent and ask for some parameters in the query\n-        # and some in the body\n-        new_params = [new_params[0]] + [\n-            param.replace(annotation=Annotated[param.annotation, Body(..., embed=True)])\n-            for param in new_params[1:]\n-        ]\n-\n-    endpoint.__signature__ = sig.replace(parameters=new_params)\n-\n-    return endpoint\n-\n-\n-def main(\n-    yaml_config: str = \"llamastack-run.yaml\",\n-    port: int = 5000,\n-    disable_ipv6: bool = False,\n-):\n-    with open(yaml_config, \"r\") as fp:\n-        config = StackRunConfig(**yaml.safe_load(fp))\n-\n-    app = FastAPI()\n-\n-    impls = asyncio.run(resolve_impls_with_routing(config))\n-    if Api.telemetry in impls:\n-        setup_logger(impls[Api.telemetry])\n-\n-    all_endpoints = get_all_api_endpoints()\n-\n-    if config.apis:\n-        apis_to_serve = set(config.apis)\n-    else:\n-        apis_to_serve = set(impls.keys())\n-\n-    for inf in builtin_automatically_routed_apis():\n-        apis_to_serve.add(inf.routing_table_api.value)\n-\n-    apis_to_serve.add(\"inspect\")\n-    for api_str in apis_to_serve:\n-        api = Api(api_str)\n-\n-        endpoints = all_endpoints[api]\n-        impl = impls[api]\n-\n-        if is_passthrough(impl.__provider_spec__):\n-            for endpoint in endpoints:\n-                url = impl.__provider_config__.url.rstrip(\"/\") + endpoint.route\n-                getattr(app, endpoint.method)(endpoint.route)(\n-                    create_dynamic_passthrough(url)\n-                )\n-        else:\n-            for endpoint in endpoints:\n-                if not hasattr(impl, endpoint.name):\n-                    # ideally this should be a typing violation already\n-                    raise ValueError(\n-                        f\"Could not find method {endpoint.name} on {impl}!!\"\n-                    )\n-\n-                impl_method = getattr(impl, endpoint.name)\n-\n-                getattr(app, endpoint.method)(endpoint.route, response_model=None)(\n-                    create_dynamic_typed_route(\n-                        impl_method,\n-                        endpoint.method,\n-                    )\n-                )\n \n-        cprint(f\"Serving API {api_str}\", \"white\", attrs=[\"bold\"])\n-        for endpoint in endpoints:\n-            cprint(f\" {endpoint.method.upper()} {endpoint.route}\", \"white\")\n+def get_impl_api(p: Any) -> Api:\n+    return p.__provider_spec__.api\n+\n \n-    print(\"\")\n-    app.exception_handler(RequestValidationError)(global_exception_handler)\n-    app.exception_handler(Exception)(global_exception_handler)\n-    signal.signal(signal.SIGINT, functools.partial(handle_sigint, app))\n+async def register_object_with_provider(obj: RoutableObject, p: Any) -> None:\n+    api = get_impl_api(p)\n+    if api == Api.inference:\n+        await p.register_model(obj)\n+    elif api == Api.safety:\n+        await p.register_shield(obj)\n+    elif api == Api.memory:\n+        await p.register_memory_bank(obj)\n \n-    app.__llama_stack_impls__ = impls\n \n-    import uvicorn\n+Registry = Dict[str, List[RoutableObjectWithProvider]]\n \n-    # FYI this does not do hot-reloads\n-    listen_host = \"::\" if not disable_ipv6 else \"0.0.0.0\"\n-    print(f\"Listening on {listen_host}:{port}\")\n-    uvicorn.run(app, host=listen_host, port=port)\n \n+# TODO: this routing table maintains state in memory purely. We need to\n+# add persistence to it when we add dynamic registration of objects.\n+class CommonRoutingTableImpl(RoutingTable):\n+    def __init__(\n+        self,\n+        impls_by_provider_id: Dict[str, RoutedProtocol],\n+    ) -> None:\n+        self.impls_by_provider_id = impls_by_provider_id\n+\n+    async def initialize(self) -> None:\n+        self.registry: Registry = {}\n+\n+        def add_objects(objs: List[RoutableObjectWithProvider]) -> None:\n+            for obj in objs:\n+                if obj.identifier not in self.registry:\n+                    self.registry[obj.identifier] = []\n+\n+                self.registry[obj.identifier].append(obj)\n+\n+        for pid, p in self.impls_by_provider_id.items():\n+            api = get_impl_api(p)\n+            if api == Api.inference:\n+                p.model_store = self\n+                models = await p.list_models()\n+                add_objects(\n+                    [ModelDefWithProvider(**m.dict(), provider_id=pid) for m in models]\n+                )\n+\n+            elif api == Api.safety:\n+                p.shield_store = self\n+                shields = await p.list_shields()\n+                add_objects(\n+                    [\n+                        ShieldDefWithProvider(**s.dict(), provider_id=pid)\n+                        for s in shields\n+                    ]\n+                )\n \n-if __name__ == \"__main__\":\n-    fire.Fire(main)\n+            elif api == Api.memory:\n+                p.memory_bank_store = self\n+                memory_banks = await p.list_memory_banks()\n+\n+                # do in-memory updates due to pesky Annotated unions\n+                for m in memory_banks:\n+                    m.provider_id = pid\n+\n+                add_objects(memory_banks)\n+\n+    async def shutdown(self) -> None:\n+        for p in self.impls_by_provider_id.values():\n+            await p.shutdown()\n+\n+    def get_provider_impl(\n+        self, routing_key: str, provider_id: Optional[str] = None\n+    ) -> Any:\n+        if routing_key not in self.registry:\n+            raise ValueError(f\"`{routing_key}` not registered\")\n+\n+        objs = self.registry[routing_key]\n+        for obj in objs:\n+            if not provider_id or provider_id == obj.provider_id:\n+                return self.impls_by_provider_id[obj.provider_id]\n+\n+        raise ValueError(f\"Provider not found for `{routing_key}`\")\n+\n+    def get_object_by_identifier(\n+        self, identifier: str\n+    ) -> Optional[RoutableObjectWithProvider]:\n+        objs = self.registry.get(identifier, [])\n+        if not objs:\n+            return None\n+\n+        # kind of ill-defined behavior here, but we'll just return the first one\n+        return objs[0]\n+\n+    async def register_object(self, obj: RoutableObjectWithProvider):\n+        # if provider_id is not specified, we'll pick an arbitrary one from existing entries\n+        if not obj.provider_id and len(self.impls_by_provider_id) > 0:\n+            obj.provider_id = list(self.impls_by_provider_id.keys())[0]\n+\n+        if obj.provider_id not in self.impls_by_provider_id:\n+            raise ValueError(f\"Provider `{obj.provider_id}` not found\")\n+        \n+        p = self.impls_by_provider_id[obj.provider_id]\n+        await register_object_with_provider(obj, p)\n+        \n+        # Direct registry modification without checking current state\n+        if obj.identifier not in self.registry:\n+            self.registry[obj.identifier] = []\n+        self.registry[obj.identifier].append(obj)\n+        # TODO: persist this to a store\n+\n+\n+class ModelsRoutingTable(CommonRoutingTableImpl, Models):\n+    async def list_models(self) -> List[ModelDefWithProvider]:\n+        objects = []\n+        for objs in self.registry.values():\n+            objects.extend(objs)\n+        return objects\n+\n+    async def get_model(self, identifier: str) -> Optional[ModelDefWithProvider]:\n+        return self.get_object_by_identifier(identifier)\n+\n+    async def register_model(self, model: ModelDefWithProvider) -> None:\n+        await self.register_object(model)\n+\n+\n+class ShieldsRoutingTable(CommonRoutingTableImpl, Shields):\n+    async def list_shields(self) -> List[ShieldDef]:\n+        objects = []\n+        for objs in self.registry.values():\n+            objects.extend(objs)\n+        return objects\n+\n+    async def get_shield(self, shield_type: str) -> Optional[ShieldDefWithProvider]:\n+        return self.get_object_by_identifier(shield_type)\n+\n+    async def register_shield(self, shield: ShieldDefWithProvider) -> None:\n+        await self.register_object(shield)\n+\n+\n+class MemoryBanksRoutingTable(CommonRoutingTableImpl, MemoryBanks):\n+    async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]:\n+        objects = []\n+        for objs in self.registry.values():\n+            objects.extend(objs)\n+        return objects\n+\n+    async def get_memory_bank(\n+        self, identifier: str\n+    ) -> Optional[MemoryBankDefWithProvider]:\n+        return self.get_object_by_identifier(identifier)\n+\n+    async def register_memory_bank(\n+        self, memory_bank: MemoryBankDefWithProvider\n+    ) -> None:\n+        await self.register_object(memory_bank)\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "32",
        "type": "2",
        "number": 81,
        "head_branch": "error-040-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes made in the `together.py` file involve a comprehensive structural redesign aimed at improving agent interactions and memory management. New classes have been introduced, including `Attachment`, `AgentTool`, and various tool definitions such as `SearchToolDefinition` and `MemoryToolDefinition`, all of which extend Pydantic's `BaseModel` for enhanced data validation. Additionally, enumerations like `SearchEngineType` and `MemoryQueryGenerator` have been added to categorize tools and configurations.\n\nThe file now includes a hierarchy of classes for managing interaction steps, such as `StepCommon` and its subclasses like `InferenceStep` and `MemoryRetrievalStep`. The `Turn` and `Session` classes encapsulate the concepts of interaction turns and sessions, respectively, tracking relevant messages and timestamps.\n\nThe `AgentConfig` class and related configurations define parameters for agent behavior, while the `Agents` protocol outlines methods for agent management, including creating agents and handling their sessions. The previous implementation of the `TogetherSafetyImpl` class has been removed, indicating a shift in the approach to safety functionalities. Overall, these changes reflect a significant evolution in the agent interaction model within the codebase.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Ensure the return type of `create_agent_turn` reflects possible outputs**\n\nThe method `create_agent_turn` is documented to potentially return either an `AsyncGenerator` or an `AgentTurnCreateResponse` depending on the value of `stream`. However, the method signature specifies a return type of `AgentTurnResponseStreamChunk`. To maintain consistency and avoid confusion, please adjust the return type in the method signature to accurately represent all possible return values.\n\n\n\nApply this diff to update the method signature:\n\n```diff\n @webmethod(route=\"/agents/turn/create\")\n-def create_agent_turn(\n+def create_agent_turn(\n     self,\n     agent_id: str,\n     session_id: str,\n     messages: List[\n         Union[\n             UserMessage,\n             ToolResponseMessage,\n         ]\n     ],\n     attachments: Optional[List[Attachment]] = None,\n     stream: Optional[bool] = False,\n- ) -> AgentTurnResponseStreamChunk: ...\n+ ) -> Union[AgentTurnResponseStreamChunk, AsyncGenerator]: ...\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "position": 462
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " #\n # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n-from together import Together\n+\n+from datetime import datetime\n+from enum import Enum\n+from typing import (\n+    Any,\n+    Dict,\n+    List,\n+    Literal,\n+    Optional,\n+    Protocol,\n+    runtime_checkable,\n+    Union,\n+)\n+\n+from llama_models.schema_utils import json_schema_type, webmethod\n+\n+from pydantic import BaseModel, ConfigDict, Field\n+from typing_extensions import Annotated\n \n from llama_models.llama3.api.datatypes import *  # noqa: F403\n+from llama_stack.apis.common.deployment_types import *  # noqa: F403\n+from llama_stack.apis.inference import *  # noqa: F403\n from llama_stack.apis.safety import *  # noqa: F403\n-from llama_stack.distribution.request_headers import NeedsRequestProviderData\n-from llama_stack.providers.datatypes import ShieldsProtocolPrivate\n+from llama_stack.apis.memory import *  # noqa: F403\n+\n+\n+@json_schema_type\n+class Attachment(BaseModel):\n+    content: InterleavedTextMedia | URL\n+    mime_type: str\n+\n+\n+class AgentTool(Enum):\n+    brave_search = \"brave_search\"\n+    wolfram_alpha = \"wolfram_alpha\"\n+    photogen = \"photogen\"\n+    code_interpreter = \"code_interpreter\"\n+\n+    function_call = \"function_call\"\n+    memory = \"memory\"\n+\n+\n+class ToolDefinitionCommon(BaseModel):\n+    input_shields: Optional[List[str]] = Field(default_factory=list)\n+    output_shields: Optional[List[str]] = Field(default_factory=list)\n+\n+\n+class SearchEngineType(Enum):\n+    bing = \"bing\"\n+    brave = \"brave\"\n+\n+\n+@json_schema_type\n+class SearchToolDefinition(ToolDefinitionCommon):\n+    # NOTE: brave_search is just a placeholder since model always uses\n+    # brave_search as tool call name\n+    type: Literal[AgentTool.brave_search.value] = AgentTool.brave_search.value\n+    api_key: str\n+    engine: SearchEngineType = SearchEngineType.brave\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+@json_schema_type\n+class WolframAlphaToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.wolfram_alpha.value] = AgentTool.wolfram_alpha.value\n+    api_key: str\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+@json_schema_type\n+class PhotogenToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.photogen.value] = AgentTool.photogen.value\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+@json_schema_type\n+class CodeInterpreterToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.code_interpreter.value] = AgentTool.code_interpreter.value\n+    enable_inline_code_execution: bool = True\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+@json_schema_type\n+class FunctionCallToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.function_call.value] = AgentTool.function_call.value\n+    function_name: str\n+    description: str\n+    parameters: Dict[str, ToolParamDefinition]\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+class _MemoryBankConfigCommon(BaseModel):\n+    bank_id: str\n+\n+\n+class AgentVectorMemoryBankConfig(_MemoryBankConfigCommon):\n+    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n+\n+\n+class AgentKeyValueMemoryBankConfig(_MemoryBankConfigCommon):\n+    type: Literal[MemoryBankType.keyvalue.value] = MemoryBankType.keyvalue.value\n+    keys: List[str]  # what keys to focus on\n+\n+\n+class AgentKeywordMemoryBankConfig(_MemoryBankConfigCommon):\n+    type: Literal[MemoryBankType.keyword.value] = MemoryBankType.keyword.value\n+\n+\n+class AgentGraphMemoryBankConfig(_MemoryBankConfigCommon):\n+    type: Literal[MemoryBankType.graph.value] = MemoryBankType.graph.value\n+    entities: List[str]  # what entities to focus on\n+\n+\n+MemoryBankConfig = Annotated[\n+    Union[\n+        AgentVectorMemoryBankConfig,\n+        AgentKeyValueMemoryBankConfig,\n+        AgentKeywordMemoryBankConfig,\n+        AgentGraphMemoryBankConfig,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+\n+class MemoryQueryGenerator(Enum):\n+    default = \"default\"\n+    llm = \"llm\"\n+    custom = \"custom\"\n+\n+\n+class DefaultMemoryQueryGeneratorConfig(BaseModel):\n+    type: Literal[MemoryQueryGenerator.default.value] = (\n+        MemoryQueryGenerator.default.value\n+    )\n+    sep: str = \" \"\n+\n+\n+class LLMMemoryQueryGeneratorConfig(BaseModel):\n+    type: Literal[MemoryQueryGenerator.llm.value] = MemoryQueryGenerator.llm.value\n+    model: str\n+    template: str\n+\n+\n+class CustomMemoryQueryGeneratorConfig(BaseModel):\n+    type: Literal[MemoryQueryGenerator.custom.value] = MemoryQueryGenerator.custom.value\n+\n+\n+MemoryQueryGeneratorConfig = Annotated[\n+    Union[\n+        DefaultMemoryQueryGeneratorConfig,\n+        LLMMemoryQueryGeneratorConfig,\n+        CustomMemoryQueryGeneratorConfig,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+\n+@json_schema_type\n+class MemoryToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.memory.value] = AgentTool.memory.value\n+    memory_bank_configs: List[MemoryBankConfig] = Field(default_factory=list)\n+    # This config defines how a query is generated using the messages\n+    # for memory bank retrieval.\n+    query_generator_config: MemoryQueryGeneratorConfig = Field(\n+        default=DefaultMemoryQueryGeneratorConfig()\n+    )\n+    max_tokens_in_context: int = 4096\n+    max_chunks: int = 10\n+\n+\n+AgentToolDefinition = Annotated[\n+    Union[\n+        SearchToolDefinition,\n+        WolframAlphaToolDefinition,\n+        PhotogenToolDefinition,\n+        CodeInterpreterToolDefinition,\n+        FunctionCallToolDefinition,\n+        MemoryToolDefinition,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+\n+class StepCommon(BaseModel):\n+    turn_id: str\n+    step_id: str\n+    started_at: Optional[datetime] = None\n+    completed_at: Optional[datetime] = None\n+\n+\n+class StepType(Enum):\n+    inference = \"inference\"\n+    tool_execution = \"tool_execution\"\n+    shield_call = \"shield_call\"\n+    memory_retrieval = \"memory_retrieval\"\n+\n+\n+@json_schema_type\n+class InferenceStep(StepCommon):\n+    model_config = ConfigDict(protected_namespaces=())\n+\n+    step_type: Literal[StepType.inference.value] = StepType.inference.value\n+    model_response: CompletionMessage\n \n-from .config import TogetherSafetyConfig\n \n+@json_schema_type\n+class ToolExecutionStep(StepCommon):\n+    step_type: Literal[StepType.tool_execution.value] = StepType.tool_execution.value\n+    tool_calls: List[ToolCall]\n+    tool_responses: List[ToolResponse]\n \n-TOGETHER_SHIELD_MODEL_MAP = {\n-    \"llama_guard\": \"meta-llama/Meta-Llama-Guard-3-8B\",\n-    \"Llama-Guard-3-8B\": \"meta-llama/Meta-Llama-Guard-3-8B\",\n-    \"Llama-Guard-3-11B-Vision\": \"meta-llama/Llama-Guard-3-11B-Vision-Turbo\",\n-}\n \n+@json_schema_type\n+class ShieldCallStep(StepCommon):\n+    step_type: Literal[StepType.shield_call.value] = StepType.shield_call.value\n+    violation: Optional[SafetyViolation]\n \n-class TogetherSafetyImpl(Safety, NeedsRequestProviderData, ShieldsProtocolPrivate):\n-    def __init__(self, config: TogetherSafetyConfig) -> None:\n-        self.config = config\n \n-    async def initialize(self) -> None:\n-        pass\n+@json_schema_type\n+class MemoryRetrievalStep(StepCommon):\n+    step_type: Literal[StepType.memory_retrieval.value] = (\n+        StepType.memory_retrieval.value\n+    )\n+    memory_bank_ids: List[str]\n+    inserted_context: InterleavedTextMedia\n \n-    async def shutdown(self) -> None:\n-        pass\n \n-    async def register_shield(self, shield: ShieldDef) -> None:\n-        raise ValueError(\"Registering dynamic shields is not supported\")\n+Step = Annotated[\n+    Union[\n+        InferenceStep,\n+        ToolExecutionStep,\n+        ShieldCallStep,\n+        MemoryRetrievalStep,\n+    ],\n+    Field(discriminator=\"step_type\"),\n+]\n \n-    async def list_shields(self) -> List[ShieldDef]:\n-        return [\n-            ShieldDef(\n-                identifier=ShieldType.llama_guard.value,\n-                type=ShieldType.llama_guard.value,\n-                params={},\n-            )\n+\n+@json_schema_type\n+class Turn(BaseModel):\n+    \"\"\"A single turn in an interaction with an Agentic System.\"\"\"\n+\n+    turn_id: str\n+    session_id: str\n+    input_messages: List[\n+        Union[\n+            UserMessage,\n+            ToolResponseMessage,\n         ]\n+    ]\n+    steps: List[Step]\n+    output_message: CompletionMessage\n+    output_attachments: List[Attachment] = Field(default_factory=list)\n+\n+    started_at: datetime\n+    completed_at: Optional[datetime] = None\n+\n+\n+@json_schema_type\n+class Session(BaseModel):\n+    \"\"\"A single session of an interaction with an Agentic System.\"\"\"\n+\n+    session_id: str\n+    session_name: str\n+    turns: List[Turn]\n+    started_at: datetime\n+\n+    memory_bank: Optional[MemoryBankDef] = None\n+\n+\n+class AgentConfigCommon(BaseModel):\n+    sampling_params: Optional[SamplingParams] = SamplingParams()\n+\n+    input_shields: Optional[List[str]] = Field(default_factory=list)\n+    output_shields: Optional[List[str]] = Field(default_factory=list)\n+\n+    tools: Optional[List[AgentToolDefinition]] = Field(default_factory=list)\n+    tool_choice: Optional[ToolChoice] = Field(default=ToolChoice.auto)\n+    tool_prompt_format: Optional[ToolPromptFormat] = Field(\n+        default=ToolPromptFormat.json\n+    )\n+\n+    max_infer_iters: int = 10\n+\n+\n+@json_schema_type\n+class AgentConfig(AgentConfigCommon):\n+    model: str\n+    instructions: str\n+    enable_session_persistence: bool\n+\n+\n+class AgentConfigOverridablePerTurn(AgentConfigCommon):\n+    instructions: Optional[str] = None\n+\n+\n+class AgentTurnResponseEventType(Enum):\n+    step_start = \"step_start\"\n+    step_complete = \"step_complete\"\n+    step_progress = \"step_progress\"\n+\n+    turn_start = \"turn_start\"\n+    turn_complete = \"turn_complete\"\n+\n+\n+@json_schema_type\n+class AgentTurnResponseStepStartPayload(BaseModel):\n+    event_type: Literal[AgentTurnResponseEventType.step_start.value] = (\n+        AgentTurnResponseEventType.step_start.value\n+    )\n+    step_type: StepType\n+    step_id: str\n+    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)\n+\n+\n+@json_schema_type\n+class AgentTurnResponseStepCompletePayload(BaseModel):\n+    event_type: Literal[AgentTurnResponseEventType.step_complete.value] = (\n+        AgentTurnResponseEventType.step_complete.value\n+    )\n+    step_type: StepType\n+    step_details: Step\n+\n+\n+@json_schema_type\n+class AgentTurnResponseStepProgressPayload(BaseModel):\n+    model_config = ConfigDict(protected_namespaces=())\n+\n+    event_type: Literal[AgentTurnResponseEventType.step_progress.value] = (\n+        AgentTurnResponseEventType.step_progress.value\n+    )\n+    step_type: StepType\n+    step_id: str\n+\n+    model_response_text_delta: Optional[str] = None\n+    tool_call_delta: Optional[ToolCallDelta] = None\n+    tool_response_text_delta: Optional[str] = None\n+\n+\n+@json_schema_type\n+class AgentTurnResponseTurnStartPayload(BaseModel):\n+    event_type: Literal[AgentTurnResponseEventType.turn_start.value] = (\n+        AgentTurnResponseEventType.turn_start.value\n+    )\n+    turn_id: str\n+\n+\n+@json_schema_type\n+class AgentTurnResponseTurnCompletePayload(BaseModel):\n+    event_type: Literal[AgentTurnResponseEventType.turn_complete.value] = (\n+        AgentTurnResponseEventType.turn_complete.value\n+    )\n+    turn: Turn\n+\n+\n+@json_schema_type\n+class AgentTurnResponseEvent(BaseModel):\n+    \"\"\"Streamed agent execution response.\"\"\"\n+\n+    payload: Annotated[\n+        Union[\n+            AgentTurnResponseStepStartPayload,\n+            AgentTurnResponseStepProgressPayload,\n+            AgentTurnResponseStepCompletePayload,\n+            AgentTurnResponseTurnStartPayload,\n+            AgentTurnResponseTurnCompletePayload,\n+        ],\n+        Field(discriminator=\"event_type\"),\n+    ]\n+\n+\n+@json_schema_type\n+class AgentCreateResponse(BaseModel):\n+    agent_id: str\n+\n+\n+@json_schema_type\n+class AgentSessionCreateResponse(BaseModel):\n+    session_id: str\n+\n+\n+@json_schema_type\n+class AgentTurnCreateRequest(AgentConfigOverridablePerTurn):\n+    agent_id: str\n+    session_id: str\n+\n+    # TODO: figure out how we can simplify this and make why\n+    # ToolResponseMessage needs to be here (it is function call\n+    # execution from outside the system)\n+    messages: List[\n+        Union[\n+            UserMessage,\n+            ToolResponseMessage,\n+        ]\n+    ]\n+    attachments: Optional[List[Attachment]] = None\n+\n+    stream: Optional[bool] = False\n+\n+\n+@json_schema_type\n+class AgentTurnResponseStreamChunk(BaseModel):\n+    event: AgentTurnResponseEvent\n+\n+\n+@json_schema_type\n+class AgentStepResponse(BaseModel):\n+    step: Step\n+\n+\n+@runtime_checkable\n+class Agents(Protocol):\n+    @webmethod(route=\"/agents/create\")\n+    async def create_agent(\n+        self,\n+        agent_config: AgentConfig,\n+    ) -> AgentCreateResponse: ...\n+\n+    # This method is not `async def` because it can result in either an\n+    # `AsyncGenerator` or a `AgentTurnCreateResponse` depending on the value of `stream`.\n+    @webmethod(route=\"/agents/turn/create\")\n+    def create_agent_turn(\n+        self,\n+        agent_id: str,\n+        session_id: str,\n+        messages: List[\n+            Union[\n+                UserMessage,\n+                ToolResponseMessage,\n+            ]\n+        ],\n+        attachments: Optional[List[Attachment]] = None,\n+        stream: Optional[bool] = False,\n+    ) -> AgentTurnResponseStreamChunk: ...\n+\n+    @webmethod(route=\"/agents/turn/get\")\n+    async def get_agents_turn(\n+        self,\n+        agent_id: str,\n+        turn_id: str,\n+    ) -> Turn: ...\n+\n+    @webmethod(route=\"/agents/step/get\")\n+    async def get_agents_step(\n+        self, agent_id: str, turn_id: str, step_id: str\n+    ) -> AgentStepResponse: ...\n+\n+    @webmethod(route=\"/agents/session/create\")\n+    async def create_agent_session(\n+        self,\n+        agent_id: str,\n+        session_name: str,\n+    ) -> AgentSessionCreateResponse: ...\n+\n+    @webmethod(route=\"/agents/session/get\")\n+    async def get_agents_session(\n+        self,\n+        agent_id: str,\n+        session_id: str,\n+        turn_ids: Optional[List[str]] = None,\n+    ) -> Session: ...\n+\n+    @webmethod(route=\"/agents/session/delete\")\n+    async def delete_agents_session(self, agent_id: str, session_id: str) -> None: ...\n \n-    async def run_shield(\n-        self, shield_type: str, messages: List[Message], params: Dict[str, Any] = None\n-    ) -> RunShieldResponse:\n-        shield_def = await self.shield_store.get_shield(shield_type)\n-        if not shield_def:\n-            raise ValueError(f\"Unknown shield {shield_type}\")\n-\n-        model = shield_def.params.get(\"model\", \"llama_guard\")\n-        if model not in TOGETHER_SHIELD_MODEL_MAP:\n-            raise ValueError(f\"Unsupported safety model: {model}\")\n-\n-        together_api_key = None\n-        if self.config.api_key is not None:\n-            together_api_key = self.config.api_key\n-        else:\n-            provider_data = self.get_request_provider_data()\n-            if provider_data is None or not provider_data.together_api_key:\n-                raise ValueError(\n-                    'Pass Together API Key in the header X-LlamaStack-ProviderData as { \"together_api_key\": <your api key>}'\n-                )\n-            together_api_key = provider_data.together_api_key\n-\n-        # messages can have role assistant or user\n-        api_messages = []\n-        for message in messages:\n-            if message.role in (Role.user.value, Role.assistant.value):\n-                api_messages.append({\"role\": message.role, \"content\": message.content})\n-\n-        violation = await get_safety_response(\n-            together_api_key, TOGETHER_SHIELD_MODEL_MAP[model], api_messages\n-        )\n-        return RunShieldResponse(violation=violation)\n-\n-\n-async def get_safety_response(\n-    api_key: str, model_name: str, messages: List[Dict[str, str]]\n-) -> Optional[SafetyViolation]:\n-    client = Together(api_key=api_key)\n-    response = client.chat.completions.create(messages=messages, model=model_name)\n-    if len(response.choices) == 0:\n-        return None\n-\n-    response_text = response.choices[0].message.content\n-    if response_text == \"safe\":\n-        return None\n-\n-    parts = response_text.split(\"\\n\")\n-    if len(parts) != 2:\n-        return None\n-\n-    if parts[0] == \"unsafe\":\n-        return SafetyViolation(\n-            violation_level=ViolationLevel.ERROR,\n-            metadata={\"violation_type\": parts[1]},\n-        )\n-\n-    return None\n+    @webmethod(route=\"/agents/delete\")\n+    async def delete_agents(\n+        self,\n+        agent_id: str,\n+    ) -> None: ...\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "31",
        "type": "2",
        "number": 80,
        "head_branch": "error-015-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the pull request primarily involve the `project/api/llama_stack/cli/model/prompt_format.py` file, which has undergone a significant transformation from a command-line interface (CLI) model prompt format to a new framework centered on memory bank definitions. The previous implementation, which included the `ModelPromptFormat` class for handling CLI arguments and model descriptions, has been entirely removed.\n\nIn its place, several new classes and data structures have been introduced to define different types of memory banks, utilizing Pydantic for data validation and serialization. A new enumeration, `MemoryBankType`, categorizes memory banks into four distinct types: `vector`, `keyvalue`, `keyword`, and `graph`. Each type is represented by its own Pydantic model, which derives from a common base class named `CommonDef`. This base class includes an ASCII-compatible identifier and an optional provider ID.\n\nAdditionally, a type alias `MemoryBankDef` has been created as a union of the specific memory bank models, along with a protocol named `MemoryBanks` that defines asynchronous methods for managing memory banks. This protocol includes annotations for HTTP routes and methods, marking a shift towards a more structured approach in managing memory bank definitions and their operations.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcEj2wAbb5AokAI7Y0riQ4rjedJAARACCeCzU0dChXAACJBJo3mBC2Bjw+BRgARLwJADulGAAjGDklWCI+BjkuDGQlWjIiNgCzOo09HLhsJ7YiJSQ5LA5YGgATABe6Bj0qPBKGOIAZhX0PTNOAtMAHAAMKFi44/4k3PiI6sWyGjB33D5+AcGhjJhIKd0AwGNIpocwrdPLt8L58JV4BgiJAAKo2AAyXAA2rBcLhuIgOAB6YlEdSwfoaJjMYlMJRUVTqbTEgDicO2YFsYAAItQeiRcMSvr5iZcALoACjxBKJpPJtypNLpimUTNwLPZ3k53L5GqmQpF3jFFwAlO93HclIgGBR4NxxK0eBR8OUlBt1vAGMlkLdqGNUEb7n9EGFusg0IlmNQvTlvPJbSRkiN5NDwvg4ZAAAbpelqtQa+DEkiLEilLIVaoULPvABy+HQtFo6iKGBykCUhe8yGKCh2JAAHmEVPg8GNPAw5sjpPw+GmvhRHlN+Ltx15fMGQqH0AFrgxvNh3dc19bbfbHRgNEZ9EYTFAyPR8KvIwRiGRlMMFKx2FxePxhKI4hSDICaqoyBZaDo+h3h8mwRlgL6EKQ5BUJ+NJsDsXBUJUkB9M4rhAqBDIqBB2i6GAhjGOAYBGLw+AiGIxJoNwRa+Gg0YAPqhmgDAANZ0t4RbMKqxp0cwDocbCFDRpo3CyBwBgxEpBgWJAcQAJJvihya4U40YEU+jBTqQiCWp4za7KuSK4C6tDYGCvTwEQhT7N6OxGZgJnpmuWZ0QxQrMax3jsWgXEanxAlCSJwouuJuCScUMkaHJWaQPsUQADThFQGDPBeSIorssXoF+0brGAgnkNcNAULsPGeJKADCGLqaakDCUofhiQ6aWJf68DiVEGEahe3loDMVS4TZ9m4Ngu6wgwkzRE6bDCQRAiYLxHYkPshQXog7weM6Fajsg+49MgWYALIiVYsUOgAYn1uBZlllQIJOkBTrQUT0OhmC0BVSKeC4RBOOw8H0B1JB+KedoOq2iBZXMMgkGQX7cFENDxvcwlSLQFoGLWk3nYgUyQx2/JTRQM1zTOcxSECaMIU20QENtu2eNkdqneEsjcDOhmra8QKbcgkwFdYsi0Jg4gML1fAyxqkDZIJSutms9BTHaOTwMsMatIddzXSQa2yAAQpt0D8yQqVkE4AI0EQxR6zOwvrWL1UNrCc18wLRLZlIYjFK92a8SQsiqyEodZuHsiVMUtChwD2ZEFQ3CwDWkAAKI8bA7WmyLG0YFtuA2ygyABLw0jsNEozjUwFABIgjyesiUsyzsXrtSJkCSlmABqgHFFdheuJbJc8jtMcANIRwPOQhKPZsT7xU+7LPEcJxQtDL68q/r8n6zZqy6ewHv4+bYfppvR9+dIuMdphEVLAlehTobcupOXY1LDCRgh8jaoE/hOYKZM9wHmtGsNS9hGrqU0jSbgMYBBRBQNsPYFQ+Ap0BPgBGrR2x0TdNMdSPIrwGCOibFeV9p5+08KgJQnNDgRkgAUDWhk0wt1EPAVyBczaixLj3TqSM1gYHwCNaIiJbglWbDaO0gw2wED4PsGG9B2Z1QYPAQS4jaEdm4bsSg7B4AG0vGpJsLZ8G+FkFlcadECBMD8JQ/eYtUqbB2LZeydAsoMKRJLHosgMCThdKIyYBdbiKGQFJSAglQwFSygEGyFRyjImscfAI5JQyUElu7eQxdeLCKTJ9TAojxH0EkfnaoAhQmwEUNtRu1Bii+gbJwjRux5AAAloDQCsP4UcNAKZsDCbQA6RgADyUgqC+CymmScnkZwBF2FEMQJVEAIF2M/YqzV1JpQKGIVsusy5jR7ruUMNMxB00ONwOiecFbtUwGgck7dsn8K2t4vaiNNbjngFgsm+ANE6VwR+RGZDzCWDiN4GqxjGknlEMFVC7zDKDkeBQT8vYvgoO7oY8Q0hryQEamAiMrNaBcEcZfEu1sBYuKwL5F0/kmIsWJGxTi3EIr7iip1GKLAJJSSSilAwUA8UXUbO6Ylv9WCtEPseal9FAJ0qCiFMKPF+KsuJNDUS914rcuoMlWQWY+W4vxUKugxKh7BwoBfC21CN6Sr8jKwKDLgpMvCkqwSKrordQ1c9bVur+UGrQIS4lc9ZALwPCQc1B8aFImzDaxidrGWhWZc6tlMMOVxQStJLVvKfWCr9cK7Mgbt67zHhayeEaqXRoCvSuNCqWUutVSmrlnrM36uzf6k+Z8w2WspVGmltrK0OvjU6yKrr2XurTTynVeryUg0EkcHNRrswdpLVayNUraWxv7dWxNw7k2js1bJCdUAp3oBnQS3NJLi1rx2gAdQpHdV0WxKBdtXb2uVjrFVDrrbuxtB7rAujsZmOdRKF1FtXogJ95bZX2vlQmj9br1VjozROpSMRrwUVvNRSAD4Vw4FfMhD80R0I/n8GgHCeF9I5KIvmdQkFyJ6BguhdQHEtiIA4mUSsdAFXIto+hiAHYGC0F2AAFguBtAAbCQUTpYACsiwpOCbQHVAAzGgEgAgADsZwNq0AEAJ3YABOC4empOKcU54G8VFeMMfisx1jFYqgcaw/oIAA===\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Add a validator to ensure `identifier` is ASCII-compatible.**\n\nSince `identifier` must be ASCII-compatible, consider adding a Pydantic validator to enforce this constraint.\n\n\n\nApply this diff to include the validator:\n\n```diff\n from pydantic import BaseModel, Field\n+from pydantic import validator\n from typing_extensions import Annotated\n\n class CommonDef(BaseModel):\n     identifier: str  # Must be ASCII-compatible identifier\n     provider_id: Optional[str] = None\n\n+    @validator('identifier')\n+    def identifier_must_be_ascii(cls, v):\n+        if not v.isascii():\n+            raise ValueError('identifier must be ASCII-compatible')\n+        return v\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/cli/model/prompt_format.py",
                "position": 127
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider making `provider_id` optional and defaulting to `None`.**\n\nCurrently, `provider_id` is initialized as an empty string. If `provider_id` is optional, it's clearer to annotate it as `Optional[str]` and default it to `None`.\n\n\n\nApply this diff to update the `provider_id` field:\n\n```diff\n from pydantic import BaseModel, Field\n+from typing import Optional\n from typing_extensions import Annotated\n\n class CommonDef(BaseModel):\n     identifier: str  # Must be ASCII-compatible identifier\n-    # Hack: move this out later\n-    provider_id: str = \"\"\n+    provider_id: Optional[str] = None\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/cli/model/prompt_format.py",
                "position": 129
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Adjust API routes to follow RESTful conventions.**\n\nTo adhere to RESTful API design principles:\n\n- Change the `get_memory_bank` route to include the identifier in the path: `/memory_banks/{identifier}`.\n- Simplify the `register_memory_bank` route to `/memory_banks`, as POST requests to the collection endpoint are standard for creating resources.\n\n\n\nApply this diff to update the routes:\n\n```diff\n @runtime_checkable\n class MemoryBanks(Protocol):\n     @webmethod(route=\"/memory_banks/list\", method=\"GET\")\n     async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]: ...\n\n-    @webmethod(route=\"/memory_banks/get\", method=\"GET\")\n+    @webmethod(route=\"/memory_banks/{identifier}\", method=\"GET\")\n     async def get_memory_bank(\n         self, identifier: str\n     ) -> Optional[MemoryBankDefWithProvider]: ...\n\n-    @webmethod(route=\"/memory_banks/register\", method=\"POST\")\n+    @webmethod(route=\"/memory_banks\", method=\"POST\")\n     async def register_memory_bank(\n         self, memory_bank: MemoryBankDefWithProvider\n     ) -> None: ...\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\nclass MemoryBanks(Protocol):\n    @webmethod(route=\"/memory_banks/list\", method=\"GET\")\n    async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]: ...\n\n    @webmethod(route=\"/memory_banks/{identifier}\", method=\"GET\")\n    async def get_memory_bank(\n        self, identifier: str\n    ) -> Optional[MemoryBankDefWithProvider]: ...\n\n    @webmethod(route=\"/memory_banks\", method=\"POST\")\n    async def register_memory_bank(\n        self, memory_bank: MemoryBankDefWithProvider\n    ) -> None: ...\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/cli/model/prompt_format.py",
                "position": 181
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/cli/model/prompt_format.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n \n-import argparse\n-import textwrap\n-from io import StringIO\n-\n-from llama_models.datatypes import CoreModelId, is_multimodal, model_family, ModelFamily\n-\n-from llama_stack.cli.subcommand import Subcommand\n-\n-\n-class ModelPromptFormat(Subcommand):\n-    \"\"\"Llama model cli for describe a model prompt format (message formats)\"\"\"\n-\n-    def __init__(self, subparsers: argparse._SubParsersAction):\n-        super().__init__()\n-        self.parser = subparsers.add_parser(\n-            \"prompt-format\",\n-            prog=\"llama model prompt-format\",\n-            description=\"Show llama model message formats\",\n-            epilog=textwrap.dedent(\n-                \"\"\"\n-                Example:\n-                    llama model prompt-format <options>\n-                \"\"\"\n-            ),\n-            formatter_class=argparse.RawTextHelpFormatter,\n-        )\n-        self._add_arguments()\n-        self.parser.set_defaults(func=self._run_model_template_cmd)\n-\n-    def _add_arguments(self):\n-        self.parser.add_argument(\n-            \"-m\",\n-            \"--model-name\",\n-            type=str,\n-            default=\"llama3_1\",\n-            help=\"Model Family (llama3_1, llama3_X, etc.)\",\n-        )\n-\n-    def _run_model_template_cmd(self, args: argparse.Namespace) -> None:\n-        import pkg_resources\n-\n-        # Only Llama 3.1 and 3.2 are supported\n-        supported_model_ids = [\n-            m\n-            for m in CoreModelId\n-            if model_family(m) in {ModelFamily.llama3_1, ModelFamily.llama3_2}\n-        ]\n-        model_str = \"\\n\".join([m.value for m in supported_model_ids])\n-        try:\n-            model_id = CoreModelId(args.model_name)\n-        except ValueError:\n-            self.parser.error(\n-                f\"{args.model_name} is not a valid Model. Choose one from --\\n{model_str}\"\n-            )\n-\n-        if model_id not in supported_model_ids:\n-            self.parser.error(\n-                f\"{model_id} is not a valid Model. Choose one from --\\n {model_str}\"\n-            )\n-\n-        llama_3_1_file = pkg_resources.resource_filename(\n-            \"llama_models\", \"llama3_1/prompt_format.md\"\n-        )\n-        llama_3_2_text_file = pkg_resources.resource_filename(\n-            \"llama_models\", \"llama3_2/text_prompt_format.md\"\n-        )\n-        llama_3_2_vision_file = pkg_resources.resource_filename(\n-            \"llama_models\", \"llama3_2/vision_prompt_format.md\"\n-        )\n-        if model_family(model_id) == ModelFamily.llama3_1:\n-            with open(llama_3_1_file, \"r\") as f:\n-                content = f.read()\n-        elif model_family(model_id) == ModelFamily.llama3_2:\n-            if is_multimodal(model_id):\n-                with open(llama_3_2_vision_file, \"r\") as f:\n-                    content = f.read()\n-            else:\n-                with open(llama_3_2_text_file, \"r\") as f:\n-                    content = f.read()\n-\n-        render_markdown_to_pager(content)\n-\n-\n-def render_markdown_to_pager(markdown_content: str):\n-    from rich.console import Console\n-    from rich.markdown import Markdown\n-    from rich.style import Style\n-    from rich.text import Text\n-\n-    class LeftAlignedHeaderMarkdown(Markdown):\n-        def parse_header(self, token):\n-            level = token.type.count(\"h\")\n-            content = Text(token.content)\n-            header_style = Style(color=\"bright_blue\", bold=True)\n-            header = Text(f\"{'#' * level} \", style=header_style) + content\n-            self.add_text(header)\n-\n-    # Render the Markdown\n-    md = LeftAlignedHeaderMarkdown(markdown_content)\n-\n-    # Capture the rendered output\n-    output = StringIO()\n-    console = Console(file=output, force_terminal=True, width=100)  # Set a fixed width\n-    console.print(md)\n-    rendered_content = output.getvalue()\n-    print(rendered_content)\n+from enum import Enum\n+from typing import List, Literal, Optional, Protocol, runtime_checkable, Union\n+\n+from llama_models.schema_utils import json_schema_type, webmethod\n+from pydantic import BaseModel, Field\n+from typing_extensions import Annotated\n+\n+\n+@json_schema_type\n+class MemoryBankType(Enum):\n+    vector = \"vector\"\n+    keyvalue = \"keyvalue\"\n+    keyword = \"keyword\"\n+    graph = \"graph\"\n+\n+\n+class CommonDef(BaseModel):\n+    identifier: str  # Must be ASCII-compatible identifier\n+    # Hack: move this out later\n+    provider_id: str = \"\"\n+\n+\n+@json_schema_type\n+class VectorMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n+    embedding_model: str  # Model name in ASCII format\n+    chunk_size_in_tokens: int\n+    overlap_size_in_tokens: Optional[int] = None\n+\n+\n+@json_schema_type\n+class KeyValueMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.keyvalue.value] = MemoryBankType.keyvalue.value\n+\n+\n+@json_schema_type\n+class KeywordMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.keyword.value] = MemoryBankType.keyword.value\n+\n+\n+@json_schema_type\n+class GraphMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.graph.value] = MemoryBankType.graph.value\n+\n+\n+MemoryBankDef = Annotated[\n+    Union[\n+        VectorMemoryBankDef,\n+        KeyValueMemoryBankDef,\n+        KeywordMemoryBankDef,\n+        GraphMemoryBankDef,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+MemoryBankDefWithProvider = MemoryBankDef\n+\n+\n+@runtime_checkable\n+class MemoryBanks(Protocol):\n+    @webmethod(route=\"/memory_banks/list\", method=\"GET\")\n+    async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]: ...\n+\n+    @webmethod(route=\"/memory_banks/get\", method=\"GET\")\n+    async def get_memory_bank(\n+        self, identifier: str\n+    ) -> Optional[MemoryBankDefWithProvider]: ...\n+\n+    @webmethod(route=\"/memory_banks/register\", method=\"POST\")\n+    async def register_memory_bank(\n+        self, memory_bank: MemoryBankDefWithProvider\n+    ) -> None: ...\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "34",
        "type": "2",
        "number": 83,
        "head_branch": "error-037-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- This is an auto-generated comment: skip review by coderabbit.ai -->\n\n> [!IMPORTANT]\n> ## Review skipped\n> \n> Auto reviews are disabled on base/target branches other than the default branch.\n> \n> \n> \n> Please check the settings in the CodeRabbit UI or the `.coderabbit.yaml` file in this repository. To trigger a single review, invoke the `@coderabbitai review` command.\n> \n> You can disable this status message by setting the `reviews.review_status` to `false` in the CodeRabbit configuration file.\n\n<!-- end of auto-generated comment: skip review by coderabbit.ai --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n- Review comments: Directly reply to a review comment made by CodeRabbit. Example:\n  - `I pushed a fix in commit <commit_id>, please review it.`\n  - `Generate unit testing code for this file.`\n  - `Open a follow-up GitHub issue for this discussion.`\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@coderabbitai` in a new review comment at the desired location with your query. Examples:\n  - `@coderabbitai generate unit testing code for this file.`\n  -\t`@coderabbitai modularize this function.`\n- PR comments: Tag `@coderabbitai` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@coderabbitai gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@coderabbitai read src/utils.ts and generate unit testing code.`\n  - `@coderabbitai read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@coderabbitai help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@coderabbitai pause` to pause the reviews on a PR.\n- `@coderabbitai resume` to resume the paused reviews.\n- `@coderabbitai review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@coderabbitai full review` to do a full review from scratch and review all the files again.\n- `@coderabbitai summary` to regenerate the summary of the PR.\n- `@coderabbitai resolve` resolve all the CodeRabbit review comments.\n- `@coderabbitai configuration` to show the current CodeRabbit configuration for the repository.\n- `@coderabbitai help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@coderabbitai ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            },
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces updates to the routing of web methods within the `Inference` protocol of the `inference.py` file. The endpoint URLs for three methods have been modified from a generic format to a versioned format. Specifically, the routes for the `completion`, `chat_completion`, and `embeddings` methods have been changed from `/inference/completion`, `/inference/chat_completion`, and `/inference/embeddings` to `/v1/completion`, `/v1/chat_completion`, and `/v1/embeddings`, respectively. \n\nDespite these changes to the endpoint URLs, the method signatures and internal logic of the methods remain unchanged. The `completion` and `chat_completion` methods continue to operate synchronously, while the `embeddings` method remains asynchronous. Additionally, there are no alterations to the parameters or return types of these methods. The overall structure and functionality of the `Inference` class are preserved, ensuring that the methods can still be invoked as before, albeit through updated routes.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [],
        "file_changes": [
            {
                "file": "project/api/llama_stack/apis/inference/inference.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " \n     # This method is not `async def` because it can result in either an\n     # `AsyncGenerator` or a `CompletionResponse` depending on the value of `stream`.\n-    @webmethod(route=\"/inference/completion\")\n+    @webmethod(route=\"/v1/completion\")\n     def completion(\n         self,\n         model: str,\n"
                    },
                    {
                        "id": 2,
                        "content": " \n     # This method is not `async def` because it can result in either an\n     # `AsyncGenerator` or a `ChatCompletionResponse` depending on the value of `stream`.\n-    @webmethod(route=\"/inference/chat_completion\")\n+    @webmethod(route=\"/v1/chat_completion\")\n     def chat_completion(\n         self,\n         model: str,\n"
                    },
                    {
                        "id": 3,
                        "content": "         logprobs: Optional[LogProbConfig] = None,\n     ) -> Union[ChatCompletionResponse, ChatCompletionResponseStreamChunk]: ...\n \n-    @webmethod(route=\"/inference/embeddings\")\n+    @webmethod(route=\"/v1/embeddings\")\n     async def embeddings(\n         self,\n         model: str,\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "35",
        "type": "2",
        "number": 84,
        "head_branch": "error-005-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces changes to the `MetaReferenceAgentsImpl` class located in `agents.py`. The modifications primarily affect two methods: `create_agent_turn` and `_create_agent_turn_streaming`. \n\nIn the `create_agent_turn` method, the logic for returning a streaming response has been updated. The method now checks if `attachments` is `None` in addition to the `stream` flag being `True`. This allows for a streaming response under these conditions, whereas previously, a streaming response was only returned if `stream` was `True`. If neither condition is satisfied, the method raises a `NotImplementedError` for non-streaming agent turns.\n\nThe `_create_agent_turn_streaming` method has also been modified. The logic now includes a check for `request.stream` being `False` and the presence of `request.attachments` before invoking `get_agent`. This introduces a dependency on both the `stream` flag and the existence of attachments, thereby altering the flow of how agents are retrieved based on the request's attributes.\n\nOverall, these changes refine the control flow for creating and streaming agent turns, particularly concerning attachment handling.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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=\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Fix inconsistent streaming behavior and improve error handling**\n\nThere are several issues with the current implementation:\n\n1. The method forces `request.stream = True` but then checks the `stream` parameter, making the parameter check redundant\n2. The error message doesn't accurately reflect both conditions that lead to the error (stream and attachments)\n\nConsider this implementation:\n\n```diff\n-        request = AgentTurnCreateRequest(\n-            agent_id=agent_id,\n-            session_id=session_id,\n-            messages=messages,\n-            attachments=attachments,\n-            stream=True,\n-        )\n-        if stream or attachments is None:\n-            return self._create_agent_turn_streaming(request)\n-        else:\n-            raise NotImplementedError(\"Non-streaming agent turns not yet implemented\")\n+        request = AgentTurnCreateRequest(\n+            agent_id=agent_id,\n+            session_id=session_id,\n+            messages=messages,\n+            attachments=attachments,\n+            stream=stream,\n+        )\n+        if not stream and attachments is not None:\n+            raise NotImplementedError(\n+                \"Non-streaming agent turns with attachments are not yet implemented\"\n+            )\n+        return self._create_agent_turn_streaming(request)\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n        request = AgentTurnCreateRequest(\n            agent_id=agent_id,\n            session_id=session_id,\n            messages=messages,\n            attachments=attachments,\n            stream=stream,\n        )\n        if not stream and attachments is not None:\n            raise NotImplementedError(\n                \"Non-streaming agent turns with attachments are not yet implemented\"\n            )\n        return self._create_agent_turn_streaming(request)\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "position": 8
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "             attachments=attachments,\n             stream=True,\n         )\n-        if stream:\n+        if stream or attachments is None:\n             return self._create_agent_turn_streaming(request)\n         else:\n             raise NotImplementedError(\"Non-streaming agent turns not yet implemented\")\n"
                    },
                    {
                        "id": 2,
                        "content": "         self,\n         request: AgentTurnCreateRequest,\n     ) -> AsyncGenerator:\n-        agent = await self.get_agent(request.agent_id)\n+        if not request.stream and request.attachments:\n+            agent = await self.get_agent(request.agent_id)\n         async for event in agent.create_and_execute_turn(request):\n             yield event\n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "37",
        "type": "2",
        "number": 86,
        "head_branch": "error-004-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces changes to the `reward_scoring.py` file within the `project/api/llama_stack/apis/reward_scoring` directory. A new attribute, `score_threshold`, is added to the `RewardScoringRequest` class, which has a default value of `0.0`. This new attribute allows users to specify a threshold for scoring in their requests.\n\nAdditionally, the `reward_score` method in the `RewardScoring` protocol is updated to include the `score_threshold` parameter, which also defaults to `0.0`. The implementation of this method is modified to incorporate logic that processes both the `dialog_generations` and `score_threshold` parameters. Specifically, if the `dialog_generations` list is empty, the method will return an empty `RewardScoringResponse`. If the `score_threshold` is greater than or equal to `0`, the method will return a `RewardScoringResponse` containing an empty list of `ScoredDialogGenerations`. These modifications enhance the functionality of the reward scoring mechanism by enabling the specification and handling of a scoring threshold.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Logical issue: Condition `if score_threshold >= 0` always evaluates to True**\n\nSince `score_threshold` defaults to `0.0`, the condition `if score_threshold >= 0` will always be `True`. This means the function will always return the empty `RewardScoringResponse` at lines 62-65, and any subsequent scoring logic will not be executed.\n\n\n\nTo address this, consider adjusting the condition to reflect the intended behavior. For example, if you want to proceed only when `score_threshold` is greater than zero, you can modify the condition:\n\n```diff\n-        if score_threshold >= 0:\n+        if score_threshold > 0:\n```\n\nAlternatively, if the early return is not intended, you might need to revise the logic to ensure that scoring proceeds as expected based on the `score_threshold` value.\n\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/reward_scoring/reward_scoring.py",
                "position": 24
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/apis/reward_scoring/reward_scoring.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " \n     dialog_generations: List[DialogGenerations]\n     model: str\n+    score_threshold: float = 0.0\n \n \n @json_schema_type\n"
                    },
                    {
                        "id": 2,
                        "content": "     def reward_score(\n         self,\n         dialog_generations: List[DialogGenerations],\n-        model: str,\n-    ) -> Union[RewardScoringResponse]: ...\n+        model: str, \n+        score_threshold: float = 0.0,\n+    ) -> Union[RewardScoringResponse]:\n+        if not dialog_generations:\n+            return RewardScoringResponse(scored_generations=[])\n+        \n+        if score_threshold >= 0:\n+            return RewardScoringResponse(\n+                scored_generations=[\n+                    ScoredDialogGenerations(dialog=[], scored_generations=[])\n+                ])\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "38",
        "type": "2",
        "number": 87,
        "head_branch": "error-018-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes introduced in the `pgvector.py` file focus on enhancing the `PGVectorMemoryAdapter` class and its handling of memory banks. A new variable, `_bank_metadata_cache`, has been added to cache metadata related to memory banks. This cache is utilized across several methods, including `register_memory_bank`, `list_memory_banks`, `insert_documents`, and `query_documents`.\n\nIn the `register_memory_bank` method, after updating the memory bank model, the corresponding metadata is stored in the `_bank_metadata_cache`, and a commit operation is performed to save changes to the database. The `list_memory_banks` method has been updated to return memory banks with cached metadata instead of raw database results.\n\nAdditionally, the `insert_documents` and `query_documents` methods now verify the existence of a memory bank in the `_bank_metadata_cache` before proceeding to retrieve the associated index. If a bank is not found in the cache, a `ValueError` is raised. These modifications collectively enhance the management of memory bank metadata and improve the robustness of operations related to memory banks.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Refactor duplicate bank validation logic.**\n\nThe bank validation logic is duplicated in both `insert_documents` and `query_documents`. Consider extracting this into a private helper method.\n\nConsider implementing a helper method:\n\n```python\ndef _get_bank_index(self, bank_id: str) -> BankWithIndex:\n    \"\"\"Get bank index if the bank exists in metadata cache.\n    \n    Args:\n        bank_id: The identifier of the memory bank\n        \n    Returns:\n        BankWithIndex: The bank index object\n        \n    Raises:\n        ValueError: If the bank is not found or not properly registered\n    \"\"\"\n    index = self.cache.get(bank_id) if bank_id in self._bank_metadata_cache else None\n    if not index:\n        raise ValueError(\n            f\"Bank '{bank_id}' not found or not properly registered. \"\n            \"Please ensure the bank is registered using register_memory_bank()\"\n        )\n    return index\n```\n\nThen use it in both methods:\n\n```diff\n-        index = self.cache.get(bank_id) if bank_id in self._bank_metadata_cache else None\n-        if not index:\n-            raise ValueError(f\"Bank {bank_id} not found\")\n+        index = self._get_bank_index(bank_id)\n```\n\n\nAlso applies to: 210-213\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/memory/pgvector/pgvector.py",
                "position": 40
            },
            {
                "body": "_:warning: Potential issue_\n\n**Add error handling around database operations.**\n\nWhile the cache update and commit are correctly placed, consider wrapping these operations in a try-catch block to ensure the cache remains consistent with the database state in case of failures.\n\nConsider implementing this pattern:\n\n```diff\n-        self._bank_metadata_cache[memory_bank.identifier] = memory_bank\n-\n-        index = BankWithIndex(\n-            bank=memory_bank,\n-            index=PGVectorIndex(memory_bank, ALL_MINILM_L6_V2_DIMENSION, self.cursor),\n-        )\n-        self.cache[memory_bank.identifier] = index\n-        self.conn.commit()\n+        try:\n+            self._bank_metadata_cache[memory_bank.identifier] = memory_bank\n+            index = BankWithIndex(\n+                bank=memory_bank,\n+                index=PGVectorIndex(memory_bank, ALL_MINILM_L6_V2_DIMENSION, self.cursor),\n+            )\n+            self.cache[memory_bank.identifier] = index\n+            self.conn.commit()\n+        except Exception as e:\n+            self.conn.rollback()\n+            if memory_bank.identifier in self._bank_metadata_cache:\n+                del self._bank_metadata_cache[memory_bank.identifier]\n+            if memory_bank.identifier in self.cache:\n+                del self.cache[memory_bank.identifier]\n+            raise RuntimeError(f\"Failed to register memory bank: {str(e)}\") from e\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n        try:\n            self._bank_metadata_cache[memory_bank.identifier] = memory_bank\n            index = BankWithIndex(\n                bank=memory_bank,\n                index=PGVectorIndex(memory_bank, ALL_MINILM_L6_V2_DIMENSION, self.cursor),\n            )\n            self.cache[memory_bank.identifier] = index\n            self.conn.commit()\n        except Exception as e:\n            self.conn.rollback()\n            if memory_bank.identifier in self._bank_metadata_cache:\n                del self._bank_metadata_cache[memory_bank.identifier]\n            if memory_bank.identifier in self.cache:\n                del self.cache[memory_bank.identifier]\n            raise RuntimeError(f\"Failed to register memory bank: {str(e)}\") from e\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/memory/pgvector/pgvector.py",
                "position": 19
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/memory/pgvector/pgvector.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "         self.cursor = None\n         self.conn = None\n         self.cache = {}\n+        self._bank_metadata_cache = {}\n \n     async def initialize(self) -> None:\n         try:\n"
                    },
                    {
                        "id": 2,
                        "content": "                 (memory_bank.identifier, memory_bank),\n             ],\n         )\n+        self._bank_metadata_cache[memory_bank.identifier] = memory_bank\n \n         index = BankWithIndex(\n             bank=memory_bank,\n             index=PGVectorIndex(memory_bank, ALL_MINILM_L6_V2_DIMENSION, self.cursor),\n         )\n         self.cache[memory_bank.identifier] = index\n+        self.conn.commit()\n \n     async def list_memory_banks(self) -> List[MemoryBankDef]:\n         banks = load_models(self.cursor, MemoryBankDef)\n"
                    },
                    {
                        "id": 3,
                        "content": "                     index=PGVectorIndex(bank, ALL_MINILM_L6_V2_DIMENSION, self.cursor),\n                 )\n                 self.cache[bank.identifier] = index\n-        return banks\n+        return [self._bank_metadata_cache.get(bank.identifier, bank) for bank in banks]\n \n     async def insert_documents(\n         self,\n"
                    },
                    {
                        "id": 4,
                        "content": "         documents: List[MemoryBankDocument],\n         ttl_seconds: Optional[int] = None,\n     ) -> None:\n-        index = self.cache.get(bank_id, None)\n+        index = self.cache.get(bank_id) if bank_id in self._bank_metadata_cache else None\n         if not index:\n             raise ValueError(f\"Bank {bank_id} not found\")\n \n"
                    },
                    {
                        "id": 5,
                        "content": "         query: InterleavedTextMedia,\n         params: Optional[Dict[str, Any]] = None,\n     ) -> QueryDocumentsResponse:\n-        index = self.cache.get(bank_id, None)\n+        index = self.cache.get(bank_id) if bank_id in self._bank_metadata_cache else None\n         if not index:\n             raise ValueError(f\"Bank {bank_id} not found\")\n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "36",
        "type": "2",
        "number": 85,
        "head_branch": "error-003-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces changes to the `get_embedding_model` function located in `project/api/llama_stack/providers/utils/memory/vector_store.py`. The modifications focus on optimizing the loading process of embedding models by removing redundant code. The previous implementation always printed a loading message and instantiated the model, regardless of its existing state. The updated version first checks the `EMBEDDING_MODELS` dictionary to determine if the model is already loaded. If the model is not present, it prints a loading message, imports `SentenceTransformer`, creates a new model instance, and stores it in the dictionary. If the model is found in the dictionary, it retrieves the loaded model without printing the loading message or re-instantiating it. Importantly, there are no changes to the signatures of exported or public entities, and the overall structure and functionality of other methods in the file remain unchanged.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcEj2wAbb5AokAI7Y0riQ4rjedJAARACCeCzU0dChXAACJBJo3mBC2Bjw+BRgARLwJADulGAAjGDklWCI+BjkuDGQlWjIiNgCzOo09HLhsJ7YiJSQ5LA5YGgATABe6Bj0qBhOApTRABwArBqQAJJhDJiQO+gMDNJT9NRjngBm+L74lfAYRJAAqjYADJcADasFwuG4iA4AHoYUR1LB+homMwYUwlFRVOptDCAOLvJQYMC2MAAEWoPRIuBh3B83hhhwAugAKcGQ6FwhG4JECFEsdGKZTY3C4gneIkkmzkylTGl03yMg4ASmO7nGkCUiAYFHg3HErRQ63gFxoyB5Tx5qAVfgCwVCXR6OAIzGoJpy3nkOpIyRG8h5njx6gAEv1IJNpgADdIY4VqUXwGEkRYkUpZCrVCiR44AOXw6FotHURQwOU11O03mQxQUGBoAA8wip8Hhnow5j9pPw+HSKNx8FN+C82zb/EEQogmwEeBR8OUlBssAHy9rdfqSxojPojCYoGR6Phh2hEsQyMphgpWOwuLx+MJROIpDIvUKsfGtDp9LuYAhkKhLseBCnuQVAXqibB1lwVCVJAfTOK4VwvpiKjvtouhgIYxjgGARi8PgIhiDCaDcImvhoK6AD6k5oAwADWtKzvOlCIDCeDwFWMJsMwxSyDCUhiMUVEEAEGjcLIHAGDEUkGBYkBxCcwHntEcGughh7tpgpCIOqnhFi8w7fLgs60NgdzINxenugaGDmvmy6RqQuAUSQzA7IW3xEBRFkkN4kaQC8BRiCWRqQJGeEETSxGkd45FoEJNH0XhTEUCxbEcVxPF8Q+gmTsUJCibI2Y/p4DAdlpsFGT6zDeN8njLt4+BoEWPxDpALluc1vzeVWiFtTVgyluILUBCZ6yYOcQrHFYZRFJMnoADRtgFGBBYaOTdLIyC8IZ0RoJADVNR5kBsIgiBoKQayLtRdbwL6bbdWORAuLQUSna1lTjAGfDqI6yA5AETXyAdC7HB44bcLQd0NQiDD+fAKXnOMdF/sOy4PdaARTHWIX2QAogAsgAQrjZJkicOZ4hR+MAPJkrjgL2H5RaraWrjHCcBlhJs+C4ItP3bXWf37Y1nXHfc50kHzzD9hQuDIJG9jsGQdzQFQNlvBQbBZot3rJELjRGtddytWjQreItmD0LlmMoFzS4asz1kuLI7Ooxq6N/d4AO0EDIt0HzYRFgEYiemORkVE+tv+bOzBto7JbO10iItmEAtDb89Ui0dJ1nRdNYBGA3zXeIbotabSjeJuBh5hpnbIJmniuko4R2RqiDwEQg3YDb6kkPWMsXjWdICDVsPsMW0gW+sbZzsovgVRQpm4N3niW/5gVOzVuDyOpy4BK63zZ9SsCKH+9uvOxnj79oWCBWVdBV+YlhxN4NCgSWtlx6IMXv601bDn3Ae0Qh79FHm1G64hpBbkgDXHIb83R/xbl/BgP8EE2VaoA4og8eygJNOAiIFR667GOk1fKklpIGG3FhCA4CDxHhPKQECd1wLXn8GgGCKlE6jFjG+dQH50J6G/OBdQFF4C0EQBRGaVQ6DxVlgInc2F0AAAY0AAHY0AABY9iLFqC8RYSjtEADYACcJAlHGJorUPYGjNG1FoHowxAgDiLBTIsYxhjVHyOoVAYRTkxESKkdUWgzlp76CAA==\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider adding thread safety and error handling.**\n\nWhile the caching mechanism is a good optimization, consider these improvements:\n1. Add thread safety for the global `EMBEDDING_MODELS` dictionary to prevent race conditions in concurrent scenarios\n2. Add error handling for model loading failures\n3. Consider implementing a maximum cache size to prevent memory issues with many models\n\nHere's a suggested implementation:\n\n```diff\n+import threading\n+from typing import Dict, Optional\n+\n+_model_lock = threading.Lock()\n+MAX_CACHED_MODELS = 5  # Adjust based on memory constraints\n\n def get_embedding_model(model: str) -> \"SentenceTransformer\":\n     global EMBEDDING_MODELS\n-    if model not in EMBEDDING_MODELS:\n-        print(f\"Loading sentence transformer for {model}...\")\n-        from sentence_transformers import SentenceTransformer\n-        loaded_model = SentenceTransformer(model)\n-        EMBEDDING_MODELS[model] = loaded_model\n+    with _model_lock:\n+        if model not in EMBEDDING_MODELS:\n+            try:\n+                print(f\"Loading sentence transformer for {model}...\")\n+                from sentence_transformers import SentenceTransformer\n+                loaded_model = SentenceTransformer(model)\n+                \n+                # Implement simple LRU by removing oldest model if cache is full\n+                if len(EMBEDDING_MODELS) >= MAX_CACHED_MODELS:\n+                    oldest_model = next(iter(EMBEDDING_MODELS))\n+                    del EMBEDDING_MODELS[oldest_model]\n+                \n+                EMBEDDING_MODELS[model] = loaded_model\n+            except Exception as e:\n+                raise RuntimeError(f\"Failed to load model {model}: {str(e)}\")\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\nimport threading\nfrom typing import Dict, Optional\n\n_model_lock = threading.Lock()\nMAX_CACHED_MODELS = 5  # Adjust based on memory constraints\n\n    if model not in EMBEDDING_MODELS:\n        with _model_lock:\n            if model not in EMBEDDING_MODELS:\n                try:\n                    print(f\"Loading sentence transformer for {model}...\")\n                    from sentence_transformers import SentenceTransformer\n                    loaded_model = SentenceTransformer(model)\n                    \n                    # Implement simple LRU by removing oldest model if cache is full\n                    if len(EMBEDDING_MODELS) >= MAX_CACHED_MODELS:\n                        oldest_model = next(iter(EMBEDDING_MODELS))\n                        del EMBEDDING_MODELS[oldest_model]\n                    \n                    EMBEDDING_MODELS[model] = loaded_model\n                except Exception as e:\n                    raise RuntimeError(f\"Failed to load model {model}: {str(e)}\")\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "position": 11
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " def get_embedding_model(model: str) -> \"SentenceTransformer\":\n     global EMBEDDING_MODELS\n \n-    loaded_model = EMBEDDING_MODELS.get(model)\n-    if loaded_model is not None:\n-        return loaded_model\n+    if model not in EMBEDDING_MODELS:\n+        print(f\"Loading sentence transformer for {model}...\")\n+        from sentence_transformers import SentenceTransformer\n+        loaded_model = SentenceTransformer(model)\n+        EMBEDDING_MODELS[model] = loaded_model\n \n-    print(f\"Loading sentence transformer for {model}...\")\n-    from sentence_transformers import SentenceTransformer\n-\n-    loaded_model = SentenceTransformer(model)\n-    EMBEDDING_MODELS[model] = loaded_model\n+    loaded_model = EMBEDDING_MODELS[model]\n     return loaded_model\n \n \n"
                    }
                ]
            }
        ]
    }
]