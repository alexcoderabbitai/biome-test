[
    {
        "id": "58",
        "type": "4",
        "number": 107,
        "head_branch": "error-034-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the pull request primarily affect the `faiss.py` file within the `project/api/llama_stack/providers/impls/meta_reference/memory` directory. The `FaissIndex` class has been removed and replaced by an abstract class `EmbeddingIndex`, which includes two new abstract methods: `add_chunks` and `query`. The `FaissMemoryImpl` class has been updated to utilize the new `EmbeddingIndex` structure.\n\nSeveral new utility functions have been introduced, including `get_embedding_model` for loading sentence transformer models and `parse_data_url` for processing data URLs. The `content_from_data` function has been added to manage different MIME types and decode base64 data, while the `content_from_doc` function has been modified to support both URL and data URI formats using asynchronous HTTP requests.\n\nAdditionally, the `make_overlapped_chunks` function has been implemented to generate overlapping text chunks for embedding. The `insert_documents` method in `BankWithIndex` has been revised to incorporate the new chunking and embedding logic, and the `query_documents` method has been enhanced for improved query processing and embedding generation. Overall, these changes reflect a restructured and modular approach to document embedding and processing.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Consider adding error handling for model loading.**\n\nThe model loading function should handle potential exceptions that might occur during model initialization.\n\n```diff\n def get_embedding_model(model: str) -> \"SentenceTransformer\":\n     global EMBEDDING_MODELS\n \n     loaded_model = EMBEDDING_MODELS.get(model)\n     if loaded_model is not None:\n         return loaded_model\n \n     print(f\"Loading sentence transformer for {model}...\")\n-    from sentence_transformers import SentenceTransformer\n+    try:\n+        from sentence_transformers import SentenceTransformer\n \n-    loaded_model = SentenceTransformer(model)\n-    EMBEDDING_MODELS[model] = loaded_model\n-    return loaded_model\n+        loaded_model = SentenceTransformer(model)\n+        EMBEDDING_MODELS[model] = loaded_model\n+        return loaded_model\n+    except Exception as e:\n+        raise RuntimeError(f\"Failed to load model {model}: {str(e)}\")\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\ndef get_embedding_model(model: str) -> \"SentenceTransformer\":\n    global EMBEDDING_MODELS\n\n    loaded_model = EMBEDDING_MODELS.get(model)\n    if loaded_model is not None:\n        return loaded_model\n\n    print(f\"Loading sentence transformer for {model}...\")\n    try:\n        from sentence_transformers import SentenceTransformer\n\n        loaded_model = SentenceTransformer(model)\n        EMBEDDING_MODELS[model] = loaded_model\n        return loaded_model\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load model {model}: {str(e)}\")\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 57
            },
            {
                "body": "_:warning: Potential issue_\n\n**Consider adding validation for chunk parameters.**\n\nThe chunking function should validate its input parameters to prevent invalid configurations.\n\n```diff\n def make_overlapped_chunks(\n     document_id: str, text: str, window_len: int, overlap_len: int\n ) -> List[Chunk]:\n+    if window_len <= 0:\n+        raise ValueError(\"window_len must be positive\")\n+    if overlap_len < 0:\n+        raise ValueError(\"overlap_len cannot be negative\")\n+    if overlap_len >= window_len:\n+        raise ValueError(\"overlap_len must be less than window_len\")\n+    if not text:\n+        return []\n+\n     tokenizer = Tokenizer.get_instance()\n     tokens = tokenizer.encode(text, bos=False, eos=False)\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\ndef make_overlapped_chunks(\n    document_id: str, text: str, window_len: int, overlap_len: int\n) -> List[Chunk]:\n    if window_len <= 0:\n        raise ValueError(\"window_len must be positive\")\n    if overlap_len < 0:\n        raise ValueError(\"overlap_len cannot be negative\")\n    if overlap_len >= window_len:\n        raise ValueError(\"overlap_len must be less than window_len\")\n    if not text:\n        return []\n\n    tokenizer = Tokenizer.get_instance()\n    tokens = tokenizer.encode(text, bos=False, eos=False)\n\n    chunks = []\n    for i in range(0, len(tokens), window_len - overlap_len):\n        toks = tokens[i : i + window_len]\n        chunk = tokenizer.decode(toks)\n        chunks.append(\n            Chunk(content=chunk, token_count=len(toks), document_id=document_id)\n        )\n\n    return chunks\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 151
            },
            {
                "body": "_:warning: Potential issue_\n\n**Consider adding validation for bank configuration.**\n\nThe `BankWithIndex` class should validate the bank configuration parameters during initialization.\n\n```diff\n @dataclass\n class BankWithIndex:\n     bank: MemoryBankDef\n     index: EmbeddingIndex\n+    \n+    def __post_init__(self):\n+        if not self.bank.embedding_model:\n+            raise ValueError(\"embedding_model must be specified\")\n+        if self.bank.chunk_size_in_tokens <= 0:\n+            raise ValueError(\"chunk_size_in_tokens must be positive\")\n+        if self.bank.overlap_size_in_tokens is not None and self.bank.overlap_size_in_tokens < 0:\n+            raise ValueError(\"overlap_size_in_tokens cannot be negative\")\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n@dataclass\nclass BankWithIndex:\n    bank: MemoryBankDef\n    index: EmbeddingIndex\n    \n    def __post_init__(self):\n        if not self.bank.embedding_model:\n            raise ValueError(\"embedding_model must be specified\")\n        if self.bank.chunk_size_in_tokens <= 0:\n            raise ValueError(\"chunk_size_in_tokens must be positive\")\n        if self.bank.overlap_size_in_tokens is not None and self.bank.overlap_size_in_tokens < 0:\n            raise ValueError(\"overlap_size_in_tokens cannot be negative\")\n\n    async def insert_documents(\n        self,\n        documents: List[MemoryBankDocument],\n    ) -> None:\n        model = get_embedding_model(self.bank.embedding_model)\n        for doc in documents:\n            content = await content_from_doc(doc)\n            chunks = make_overlapped_chunks(\n                doc.document_id,\n                content,\n                self.bank.chunk_size_in_tokens,\n                self.bank.overlap_size_in_tokens\n                or (self.bank.chunk_size_in_tokens // 4),\n            )\n            if not chunks:\n                continue\n            embeddings = model.encode([x.content for x in chunks]).astype(np.float32)\n            # Unstable order of chunks\n            chunks = sorted(chunks, key=lambda x: hash(x.content))\n            \n            await self.index.add_chunks(chunks, embeddings)\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 244
            },
            {
                "body": "_:warning: Potential issue_\n\n**Add input validation and improve error handling in query_documents.**\n\nThe query method should validate its inputs and handle potential errors during embedding generation.\n\n```diff\n     async def query_documents(\n         self,\n         query: InterleavedTextMedia,\n         params: Optional[Dict[str, Any]] = None,\n     ) -> QueryDocumentsResponse:\n+        if query is None:\n+            raise ValueError(\"query cannot be None\")\n+\n         if params is None:\n             params = {}\n-        k = params.get(\"max_chunks\", 3)\n+        \n+        try:\n+            k = int(params.get(\"max_chunks\", 3))\n+            if k <= 0:\n+                raise ValueError(\"max_chunks must be positive\")\n+        except (TypeError, ValueError) as e:\n+            raise ValueError(f\"Invalid max_chunks parameter: {str(e)}\")\n\n         def _process(c) -> str:\n             if isinstance(c, str):\n                 return c\n             else:\n                 return \"<media>\"\n\n         if isinstance(query, list):\n             query_str = \" \".join([_process(c) for c in query])\n         else:\n             query_str = _process(query)\n\n-        model = get_embedding_model(self.bank.embedding_model)\n-        query_vector = model.encode([query_str])[0].astype(np.float32)\n+        try:\n+            model = get_embedding_model(self.bank.embedding_model)\n+            query_vector = model.encode([query_str])[0].astype(np.float32)\n+        except Exception as e:\n+            raise RuntimeError(f\"Failed to generate query embedding: {str(e)}\")\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    async def query_documents(\n        self,\n        query: InterleavedTextMedia,\n        params: Optional[Dict[str, Any]] = None,\n    ) -> QueryDocumentsResponse:\n        if query is None:\n            raise ValueError(\"query cannot be None\")\n\n        if params is None:\n            params = {}\n        \n        try:\n            k = int(params.get(\"max_chunks\", 3))\n            if k <= 0:\n                raise ValueError(\"max_chunks must be positive\")\n        except (TypeError, ValueError) as e:\n            raise ValueError(f\"Invalid max_chunks parameter: {str(e)}\")\n\n        def _process(c) -> str:\n            if isinstance(c, str):\n                return c\n            else:\n                return \"<media>\"\n\n        if isinstance(query, list):\n            query_str = \" \".join([_process(c) for c in query])\n        else:\n            query_str = _process(query)\n\n        try:\n            model = get_embedding_model(self.bank.embedding_model)\n            query_vector = model.encode([query_str])[0].astype(np.float32)\n        except Exception as e:\n            raise RuntimeError(f\"Failed to generate query embedding: {str(e)}\")\n        return await self.index.query(query_vector, k)\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 274
            },
            {
                "body": "_:warning: Potential issue_\n\n**Add timeout and retries for HTTP requests.**\n\nThe HTTP client should have proper timeout settings and retry logic for better reliability.\n\n```diff\n async def content_from_doc(doc: MemoryBankDocument) -> str:\n+    timeout = httpx.Timeout(10.0, connect=5.0)\n+    async with httpx.AsyncClient(timeout=timeout) as client:\n+        async def fetch_with_retry(url: str, max_retries: int = 3) -> str:\n+            for attempt in range(max_retries):\n+                try:\n+                    r = await client.get(url)\n+                    r.raise_for_status()\n+                    return r.text\n+                except httpx.HTTPError as e:\n+                    if attempt == max_retries - 1:\n+                        raise ValueError(f\"Failed to fetch content: {str(e)}\")\n+                    await asyncio.sleep(1 * (attempt + 1))\n+\n     if isinstance(doc.content, URL):\n         if doc.content.uri.startswith(\"data:\"):\n             return content_from_data(doc.content.uri)\n         else:\n-            async with httpx.AsyncClient() as client:\n-                r = await client.get(doc.content.uri)\n-                return r.text\n+            return await fetch_with_retry(doc.content.uri)\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 131
            },
            {
                "body": "_:warning: Potential issue_\n\n**Add error handling for file operations and improve MIME type handling.**\n\nThe content extraction should handle potential errors during file operations and provide more robust MIME type handling.\n\n```diff\n def content_from_data(data_url: str) -> str:\n-    parts = parse_data_url(data_url)\n-    data = parts[\"data\"]\n+    try:\n+        parts = parse_data_url(data_url)\n+        data = parts[\"data\"]\n \n-    if parts[\"is_base64\"]:\n-        data = base64.b64decode(data)\n-    else:\n-        data = unquote(data)\n-        encoding = parts[\"encoding\"] or \"utf-8\"\n-        data = data.encode(encoding)\n+        if parts[\"is_base64\"]:\n+            try:\n+                data = base64.b64decode(data)\n+            except Exception as e:\n+                raise ValueError(f\"Invalid base64 data: {str(e)}\")\n+        else:\n+            data = unquote(data)\n+            encoding = parts[\"encoding\"] or \"utf-8\"\n+            try:\n+                data = data.encode(encoding)\n+            except LookupError:\n+                raise ValueError(f\"Unsupported encoding: {encoding}\")\n \n-    encoding = parts[\"encoding\"]\n-    if not encoding:\n-        detected = chardet.detect(data)\n-        encoding = detected[\"encoding\"]\n+        encoding = parts[\"encoding\"]\n+        if not encoding:\n+            detected = chardet.detect(data)\n+            encoding = detected[\"encoding\"] or \"utf-8\"\n \n-    mime_type = parts[\"mimetype\"]\n-    mime_category = mime_type.split(\"/\")[0]\n+        mime_type = parts[\"mimetype\"].lower()\n+        mime_category = mime_type.split(\"/\")[0]\n \n-    if mime_category == \"text\":\n-        return data.decode(encoding)\n-    elif mime_type == \"application/pdf\":\n-        pdf_bytes = io.BytesIO(data)\n-        pdf_reader = PdfReader(pdf_bytes)\n-        return \"\\n\".join([page.extract_text() for page in pdf_reader.pages])\n-    else:\n-        cprint(\"Could not extract content from data_url properly.\", color=\"red\")\n-        return \"\"\n+        if mime_category == \"text\":\n+            try:\n+                return data.decode(encoding)\n+            except UnicodeDecodeError as e:\n+                raise ValueError(f\"Failed to decode text content: {str(e)}\")\n+        elif mime_type == \"application/pdf\":\n+            try:\n+                pdf_bytes = io.BytesIO(data)\n+                pdf_reader = PdfReader(pdf_bytes)\n+                return \"\\n\".join([page.extract_text() for page in pdf_reader.pages])\n+            except Exception as e:\n+                raise ValueError(f\"Failed to process PDF content: {str(e)}\")\n+        else:\n+            cprint(f\"Unsupported MIME type: {mime_type}\", color=\"red\")\n+            return \"\"\n+    except Exception as e:\n+        cprint(f\"Error processing data URL: {str(e)}\", color=\"red\")\n+        raise\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\ndef content_from_data(data_url: str) -> str:\n    try:\n        parts = parse_data_url(data_url)\n        data = parts[\"data\"]\n\n        if parts[\"is_base64\"]:\n            try:\n                data = base64.b64decode(data)\n            except Exception as e:\n                raise ValueError(f\"Invalid base64 data: {str(e)}\")\n        else:\n            data = unquote(data)\n            encoding = parts[\"encoding\"] or \"utf-8\"\n            try:\n                data = data.encode(encoding)\n            except LookupError:\n                raise ValueError(f\"Unsupported encoding: {encoding}\")\n\n        encoding = parts[\"encoding\"]\n        if not encoding:\n            detected = chardet.detect(data)\n            encoding = detected[\"encoding\"] or \"utf-8\"\n\n        mime_type = parts[\"mimetype\"].lower()\n        mime_category = mime_type.split(\"/\")[0]\n\n        if mime_category == \"text\":\n            try:\n                return data.decode(encoding)\n            except UnicodeDecodeError as e:\n                raise ValueError(f\"Failed to decode text content: {str(e)}\")\n        elif mime_type == \"application/pdf\":\n            try:\n                pdf_bytes = io.BytesIO(data)\n                pdf_reader = PdfReader(pdf_bytes)\n                return \"\\n\".join([page.extract_text() for page in pdf_reader.pages])\n            except Exception as e:\n                raise ValueError(f\"Failed to process PDF content: {str(e)}\")\n        else:\n            cprint(f\"Unsupported MIME type: {mime_type}\", color=\"red\")\n            return \"\"\n    except Exception as e:\n        cprint(f\"Error processing data URL: {str(e)}\", color=\"red\")\n        raise\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "position": 111
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " #\n # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n-\n-import logging\n-\n+import base64\n+import io\n+import re\n+from abc import ABC, abstractmethod\n+from dataclasses import dataclass\n from typing import Any, Dict, List, Optional\n+from urllib.parse import unquote\n \n-import faiss\n+import chardet\n+import httpx\n import numpy as np\n from numpy.typing import NDArray\n+from pypdf import PdfReader\n+from termcolor import cprint\n \n from llama_models.llama3.api.datatypes import *  # noqa: F403\n+from llama_models.llama3.api.tokenizer import Tokenizer\n \n from llama_stack.apis.memory import *  # noqa: F403\n-from llama_stack.providers.datatypes import MemoryBanksProtocolPrivate\n-\n-from llama_stack.providers.utils.memory.vector_store import (\n-    ALL_MINILM_L6_V2_DIMENSION,\n-    BankWithIndex,\n-    EmbeddingIndex,\n-)\n-from llama_stack.providers.utils.telemetry import tracing\n-\n-from .config import FaissImplConfig\n-\n-logger = logging.getLogger(__name__)\n \n+ALL_MINILM_L6_V2_DIMENSION = 384\n+\n+EMBEDDING_MODELS = {}\n+\n+\n+def get_embedding_model(model: str) -> \"SentenceTransformer\":\n+    global EMBEDDING_MODELS\n+\n+    loaded_model = EMBEDDING_MODELS.get(model)\n+    if loaded_model is not None:\n+        return loaded_model\n+\n+    print(f\"Loading sentence transformer for {model}...\")\n+    from sentence_transformers import SentenceTransformer\n+\n+    loaded_model = SentenceTransformer(model)\n+    EMBEDDING_MODELS[model] = loaded_model\n+    return loaded_model\n+\n+\n+def parse_data_url(data_url: str):\n+    data_url_pattern = re.compile(\n+        r\"^\"\n+        r\"data:\"\n+        r\"(?P<mimetype>[\\w/\\-+.]+)\"\n+        r\"(?P<charset>;charset=(?P<encoding>[\\w-]+))?\"\n+        r\"(?P<base64>;base64)?\"\n+        r\",(?P<data>.*)\"\n+        r\"$\",\n+        re.DOTALL,\n+    )\n+    match = data_url_pattern.match(data_url)\n+    if not match:\n+        raise ValueError(\"Invalid Data URL format\")\n+\n+    parts = match.groupdict()\n+    parts[\"is_base64\"] = bool(parts[\"base64\"])\n+    return parts\n+\n+\n+def content_from_data(data_url: str) -> str:\n+    parts = parse_data_url(data_url)\n+    data = parts[\"data\"]\n+\n+    if parts[\"is_base64\"]:\n+        data = base64.b64decode(data)\n+    else:\n+        data = unquote(data)\n+        encoding = parts[\"encoding\"] or \"utf-8\"\n+        data = data.encode(encoding)\n+\n+    encoding = parts[\"encoding\"]\n+    if not encoding:\n+        detected = chardet.detect(data)\n+        encoding = detected[\"encoding\"]\n+\n+    mime_type = parts[\"mimetype\"]\n+    mime_category = mime_type.split(\"/\")[0]\n+    if mime_category == \"text\":\n+        # For text-based files (including CSV, MD)\n+        return data.decode(encoding)\n+\n+    elif mime_type == \"application/pdf\":\n+        # For PDF and DOC/DOCX files, we can't reliably convert to string)\n+        pdf_bytes = io.BytesIO(data)\n+        pdf_reader = PdfReader(pdf_bytes)\n+        return \"\\n\".join([page.extract_text() for page in pdf_reader.pages])\n+\n+    else:\n+        cprint(\"Could not extract content from data_url properly.\", color=\"red\")\n+        return \"\"\n+\n+\n+async def content_from_doc(doc: MemoryBankDocument) -> str:\n+    if isinstance(doc.content, URL):\n+        if doc.content.uri.startswith(\"data:\"):\n+            return content_from_data(doc.content.uri)\n+        else:\n+            async with httpx.AsyncClient() as client:\n+                r = await client.get(doc.content.uri)\n+                return r.text\n+\n+    pattern = re.compile(\"^(https?://|file://|data:)\")\n+    if pattern.match(doc.content):\n+        if doc.content.startswith(\"data:\"):\n+            return content_from_data(doc.content)\n+        else:\n+            async with httpx.AsyncClient() as client:\n+                r = await client.get(doc.content)\n+                return r.text\n+\n+    return interleaved_text_media_as_str(doc.content)\n+\n+\n+def make_overlapped_chunks(\n+    document_id: str, text: str, window_len: int, overlap_len: int\n+) -> List[Chunk]:\n+    tokenizer = Tokenizer.get_instance()\n+    tokens = tokenizer.encode(text, bos=False, eos=False)\n+\n+    chunks = []\n+    for i in range(0, len(tokens), window_len - overlap_len):\n+        toks = tokens[i : i + window_len]\n+        chunk = tokenizer.decode(toks)\n+        chunks.append(\n+            Chunk(content=chunk, token_count=len(toks), document_id=document_id)\n+        )\n \n-class FaissIndex(EmbeddingIndex):\n-    id_by_index: Dict[int, str]\n-    chunk_by_index: Dict[int, str]\n+    return chunks\n \n-    def __init__(self, dimension: int):\n-        self.index = faiss.IndexFlatL2(dimension)\n-        self.id_by_index = {}\n-        self.chunk_by_index = {}\n \n-    @tracing.span(name=\"add_chunks\")\n+class EmbeddingIndex(ABC):\n+    @abstractmethod\n     async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):\n-        indexlen = len(self.id_by_index)\n-        for i, chunk in enumerate(chunks):\n-            self.chunk_by_index[indexlen + i] = chunk\n-            self.id_by_index[indexlen + i] = chunk.document_id\n-\n-        self.index.add(np.array(embeddings).astype(np.float32))\n+        raise NotImplementedError()\n \n+    @abstractmethod\n     async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:\n-        distances, indices = self.index.search(\n-            embedding.reshape(1, -1).astype(np.float32), k\n-        )\n-\n-        chunks = []\n-        scores = []\n-        for d, i in zip(distances[0], indices[0]):\n-            if i < 0:\n-                continue\n-            chunks.append(self.chunk_by_index[int(i)])\n-            scores.append(1.0 / float(d))\n-\n-        return QueryDocumentsResponse(chunks=chunks, scores=scores)\n+        raise NotImplementedError()\n \n \n-class FaissMemoryImpl(Memory, MemoryBanksProtocolPrivate):\n-    def __init__(self, config: FaissImplConfig) -> None:\n-        self.config = config\n-        self.cache = {}\n-\n-    async def initialize(self) -> None: ...\n-\n-    async def shutdown(self) -> None: ...\n-\n-    async def register_memory_bank(\n-        self,\n-        memory_bank: MemoryBankDef,\n-    ) -> None:\n-        assert (\n-            memory_bank.type == MemoryBankType.vector.value\n-        ), f\"Only vector banks are supported {memory_bank.type}\"\n-\n-        index = BankWithIndex(\n-            bank=memory_bank, index=FaissIndex(ALL_MINILM_L6_V2_DIMENSION)\n-        )\n-        self.cache[memory_bank.identifier] = index\n-\n-    async def list_memory_banks(self) -> List[MemoryBankDef]:\n-        return [i.bank for i in self.cache.values()]\n+@dataclass\n+class BankWithIndex:\n+    bank: MemoryBankDef\n+    index: EmbeddingIndex\n \n     async def insert_documents(\n         self,\n-        bank_id: str,\n         documents: List[MemoryBankDocument],\n-        ttl_seconds: Optional[int] = None,\n     ) -> None:\n-        index = self.cache.get(bank_id)\n-        if index is None:\n-            raise ValueError(f\"Bank {bank_id} not found\")\n-\n-        await index.insert_documents(documents)\n+        model = get_embedding_model(self.bank.embedding_model)\n+        for doc in documents:\n+            content = await content_from_doc(doc)\n+            chunks = make_overlapped_chunks(\n+                doc.document_id,\n+                content,\n+                self.bank.chunk_size_in_tokens,\n+                self.bank.overlap_size_in_tokens\n+                or (self.bank.chunk_size_in_tokens // 4),\n+            )\n+            if not chunks:\n+                continue\n+            embeddings = model.encode([x.content for x in chunks]).astype(np.float32)\n+            # Unstable order of chunks\n+            chunks = sorted(chunks, key=lambda x: hash(x.content))\n+            \n+            await self.index.add_chunks(chunks, embeddings)\n \n     async def query_documents(\n         self,\n-        bank_id: str,\n         query: InterleavedTextMedia,\n         params: Optional[Dict[str, Any]] = None,\n     ) -> QueryDocumentsResponse:\n-        index = self.cache.get(bank_id)\n-        if index is None:\n-            raise ValueError(f\"Bank {bank_id} not found\")\n-\n-        return await index.query_documents(query, params)\n+        if params is None:\n+            params = {}\n+        k = params.get(\"max_chunks\", 3)\n+\n+        def _process(c) -> str:\n+            if isinstance(c, str):\n+                return c\n+            else:\n+                return \"<media>\"\n+\n+        if isinstance(query, list):\n+            query_str = \" \".join([_process(c) for c in query])\n+        else:\n+            query_str = _process(query)\n+\n+        model = get_embedding_model(self.bank.embedding_model)\n+        query_vector = model.encode([query_str])[0].astype(np.float32)\n+        return await self.index.query(query_vector, k)\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "59",
        "type": "4",
        "number": 108,
        "head_branch": "error-047-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces caching mechanisms for the conversion functions in the `opentelemetry.py` file, specifically targeting the `string_to_trace_id` and `string_to_span_id` functions. The `string_to_trace_id` function now includes a caching layer that retains up to 10,000 previously computed trace IDs in a dictionary named `_trace_id_cache`. When the cache exceeds this limit, it is cleared. The function first checks if a trace ID is already present in the cache before executing the conversion process.\n\nSimilarly, the `string_to_span_id` function has been modified to implement caching as well. It generates a cache key based on a hash of the first three characters of the input string and stores the results in `_span_id_cache`. This function also clears its cache upon reaching the maximum size and only converts the span ID string to an integer if it is not already cached.\n\nAdditionally, the function signatures for both `string_to_trace_id` and `string_to_span_id` have been updated to reflect these changes. Overall, the modifications aim to improve the efficiency of trace and span ID conversions by minimizing redundant computations through caching.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Consider thread safety and configuration improvements**\n\nThe global cache implementation has potential thread-safety issues in concurrent environments. Consider these improvements:\n\n1. Use thread-safe data structures (e.g., `threading.Lock` or `concurrent.futures.ConcurrentDict`)\n2. Make cache size configurable through environment variables or configuration\n\nHere's a suggested implementation:\n\n```diff\n+from threading import Lock\n+from typing import Dict, Optional\n+\n+class ThreadSafeCache:\n+    def __init__(self, max_size: int):\n+        self._cache: Dict = {}\n+        self._lock = Lock()\n+        self._max_size = max_size\n+\n+    def get(self, key: str) -> Optional[int]:\n+        with self._lock:\n+            return self._cache.get(key)\n+\n+    def set(self, key: str, value: int) -> None:\n+        with self._lock:\n+            if len(self._cache) >= self._max_size:\n+                self._cache.clear()\n+            self._cache[key] = value\n\n-_trace_id_cache = {}\n-_span_id_cache = {}\n-_max_cache_size = 10000\n+_trace_id_cache = ThreadSafeCache(max_size=10000)\n+_span_id_cache = ThreadSafeCache(max_size=10000)\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/telemetry/opentelemetry/opentelemetry.py",
                "position": 10
            },
            {
                "body": "_:warning: Potential issue_\n\n**Improve cache management and add input validation**\n\nThe current implementation has several potential issues:\n\n1. Clearing the entire cache when full is inefficient\n2. Missing input validation\n3. No limit on input string length could lead to memory issues\n\nConsider this improved implementation:\n\n```diff\n def string_to_trace_id(s: str) -> int:\n+    if not isinstance(s, str) or not s:\n+        raise ValueError(\"Invalid trace ID: must be a non-empty string\")\n+    if len(s) > 32:  # Reasonable limit for trace IDs\n+        raise ValueError(\"Trace ID exceeds maximum length\")\n+\n-    if s not in _trace_id_cache:\n-        if len(_trace_id_cache) >= _max_cache_size:\n-            _trace_id_cache.clear()\n-        _trace_id_cache[s] = int.from_bytes(s.encode(), byteorder=\"big\", signed=False)\n-    return _trace_id_cache[s]\n+    cached_value = _trace_id_cache.get(s)\n+    if cached_value is not None:\n+        return cached_value\n+    \n+    value = int.from_bytes(s.encode(), byteorder=\"big\", signed=False)\n+    _trace_id_cache.set(s, value)\n+    return value\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/telemetry/opentelemetry/opentelemetry.py",
                "position": 17
            },
            {
                "body": "_:warning: Potential issue_\n\n**Critical: Fix potential hash collisions and byte order inconsistency**\n\nThe current implementation has several critical issues:\n\n1. Using hash of first 3 chars can cause collisions, leading to incorrect span IDs being returned\n2. Inconsistent byte order (little-endian) compared to trace_id (big-endian)\n3. Modulo 1000 severely limits effective cache size and increases collision risk\n\nRecommended fix:\n\n```diff\n def string_to_span_id(s: str) -> int:\n-    # Optimization: Cache span ID conversions\n-    # Use hash of first few chars as cache key for better memory usage\n-    key = hash(s[:3]) % 1000  \n-    if key not in _span_id_cache:\n-        if len(_span_id_cache) >= _max_cache_size:\n-            _span_id_cache.clear()\n-        # Convert full span ID string to int value\n-        _span_id_cache[key] = int.from_bytes(s.encode(), byteorder=\"little\", signed=False) \n-    return _span_id_cache[key]\n+    if not isinstance(s, str) or not s:\n+        raise ValueError(\"Invalid span ID: must be a non-empty string\")\n+    if len(s) > 16:  # Reasonable limit for span IDs\n+        raise ValueError(\"Span ID exceeds maximum length\")\n+\n+    cached_value = _span_id_cache.get(s)\n+    if cached_value is not None:\n+        return cached_value\n+    \n+    # Use big-endian to maintain consistency with trace_id\n+    value = int.from_bytes(s.encode(), byteorder=\"big\", signed=False)\n+    _span_id_cache.set(s, value)\n+    return value\n```\n\nThis fix:\n1. Uses full string as cache key to prevent collisions\n2. Maintains consistent byte order with trace_id\n3. Adds input validation\n4. Removes the arbitrary modulo limitation\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/telemetry/opentelemetry/opentelemetry.py",
                "position": 30
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/telemetry/opentelemetry/opentelemetry.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " from .config import OpenTelemetryConfig\n \n \n-def string_to_trace_id(s: str) -> int:\n-    # Convert the string to bytes and then to an integer\n-    return int.from_bytes(s.encode(), byteorder=\"big\", signed=False)\n+# Optimization: Cache trace and span ID conversions\n+_trace_id_cache = {}\n+_span_id_cache = {}\n+_max_cache_size = 10000\n \n+def string_to_trace_id(s: str) -> int:\n+    if s not in _trace_id_cache:\n+        if len(_trace_id_cache) >= _max_cache_size:\n+            _trace_id_cache.clear()\n+        _trace_id_cache[s] = int.from_bytes(s.encode(), byteorder=\"big\", signed=False)\n+    return _trace_id_cache[s]\n \n def string_to_span_id(s: str) -> int:\n-    # Use only the first 8 bytes (64 bits) for span ID\n-    return int.from_bytes(s.encode()[:8], byteorder=\"big\", signed=False)\n+    # Optimization: Cache span ID conversions\n+    # Use hash of first few chars as cache key for better memory usage\n+    key = hash(s[:3]) % 1000  \n+    if key not in _span_id_cache:\n+        if len(_span_id_cache) >= _max_cache_size:\n+            _span_id_cache.clear()\n+        # Convert full span ID string to int value\n+        _span_id_cache[key] = int.from_bytes(s.encode(), byteorder=\"little\", signed=False) \n+    return _span_id_cache[key]\n+\n \n \n def is_tracing_enabled(tracer):\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "60",
        "type": "4",
        "number": 109,
        "head_branch": "error-010-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the pull request primarily involve modifications to the `server.py` file within the `project/api/llama_stack/distribution/server` directory. The main focus of these modifications is on the configuration of the Uvicorn server, particularly the addition of a `timeout_keep_alive` parameter set to 5 seconds. This adjustment alters how the server manages idle connections by reducing the time it waits for client activity before closing them. \n\nAdditionally, a comment has been added regarding \"risky assumptions about network reliability.\" The import statement for the `uvicorn` module has been relocated within the file, though this does not impact the functionality of the server. The overall structure of the server's startup process remains unchanged, with the main function still tasked with reading configurations, setting up the FastAPI application, and processing incoming requests. Importantly, the error handling and request processing logic have not been modified, ensuring that the core functionalities of the server continue to operate as they did previously.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai -->\n\n---\n\n<details>\n<summary>\ud83d\udcdc Recent review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and fdd4441f5c5b0293ef170c537ee779e9c453c256.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/distribution/server/server.py (1 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n<details>\n<summary>\ud83d\udd07 Additional comments (2)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/distribution/server/server.py (2)</summary><blockquote>\n\n`337-338`: **Please clarify the \"risky assumptions\" comment.**\n\nThe comment about \"risky assumptions about network reliability\" is vague and could be interpreted in multiple ways. Consider:\n1. Specifying what assumptions are being made\n2. Documenting the potential impacts\n3. Adding mitigation strategies or configuration recommendations\n\n```shell\n#!/bin/bash\n# Check if there are any related network configuration or reliability discussions\nrg -i \"network.*reliability|timeout.*network\" --type md\n```\n\n---\n\n`339-340`: **Review the aggressive timeout configuration.**\n\nThe `timeout_keep_alive=5` setting matches Uvicorn's default value, but the comment suggests this was an intentional choice marked as \"aggressive\". This configuration:\n1. May impact clients with high latency or slow connections\n2. Could increase overhead due to frequent connection recycling\n3. Might affect performance under heavy load\n\nConsider:\n- Documenting why this specific timeout was chosen\n- Adding metrics/monitoring for connection recycling\n- Evaluating if this timeout is appropriate for your use case\n\n\n```shell\n#!/bin/bash\n# Look for any timeout-related configurations or discussions\nrg -i \"timeout.*keep.*alive|connection.*timeout\" --type py\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n</details><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcEj2wAbb5AokAI7Y0riQ4rjedJAARACCeCzU0dChXAACJBJo3mCIZPD4FGABEvAkAO6UYABMYOQVefgY5LgxkBVoyIjYAszqNPRy4bCe2Pl85LA5YGg1AF7oGPSo8EoY4gBm5fRdkBhOApSQAIwADACcGpAAkmEMmJBH6AwM0vn0uLAU+NhEsCNPJt8L58BV4BgiJAAKo2AAyXAA2lYfH4bEEQogwmZzhcALoAClguFw3EQHAA9BSiOpYL0NExmBSmEoqKp1NoKQBxEHrMC2MAAEWoXRIuAp3FRFNxAEpru5RpAlIgGBR4NxxM0UMt4A8aMgvtQRqhJb5/BjQh09mhEsxqLqct55KqSMkhvIvp4ueoABK9SDjY4AA3SLOU7NwnJINRIJSy5SqFCD1wAcvh0LRaOpChgckqxdpvMgigoNiQAB5hAJEFxZyGA0s0Db8PiSijcfD5fibBumvwBYKWlyeXj4MpKFZYT35lVqjU5jRGfRGExQMj0fA9m0EYhkZSDBSsdhcXj8YSicRSGTOxThtSabS6MB6VcwBDIVCPbeEUjkKgHxk2A2LgqAqSAemcVwnhvVkVHvLQdGXYxwDAIxRxEMQKTQbh4ApXw0DtAB9LE0AYABrCksyxNUBDwHMKQmKQKAYygmI0bhZA4AwYh4gwLEgOIbl3P83XApw7SgzdGGmSFpAVTws02HsIVwH5aGwN5uiyZQ/GYRR4G2PUcwNdNpyDRjKHY2Qg0gbYogAGh4NUJPgJ1bPwBhxghKEtWnO0IVs7AMDEHMlk+RUmAwbYiGwf9Qqk6doTKJgKCwCyKFTfBIwEJ1HPUDNaGQNBDyAqsSBrCg6yhGI1UQMj5C6CD52aIqBF+MJWgqIoyPNbx4BUVz1FkdpMHoPTFPKA1FSS3UiiwSLoti+1fPTCEGG8bAlHQSAg3ENh2sIsiSBIbhCJySQSBs7gXAIsVjnyMICEgABWcDRGaQq8p1Iz62KvSAnQIgiACRBEAu8J4H2vA3pJbzrjgVAGBk0h0G8GgKCmzx0oAchkEhpjKEsApk2g+vrSLyBClrHI7JtxEdBqlIvbzIFgMFIG8ZooXyzp1GQYE+HW8pm1Iy8hqeEgBc8dbOxZtYolLSnNQwRB5QMOJM2zZoGcc6dIY7CgwhImhSvcvgg2wZK5ps1A9KkT502KxTNkodgOY85asAC6cw0c2jHvfJV8GkfYsvQJmxAbTYgqp3M+twWR4cVMcdL8aiNNwWLPASxUce6SNDewbgnI895zX8lXtUjMRHPBL4GwrwLguV8DxDNEGOxV+AcqBEsAjQKqGwW+AYri5pHIe8R6yLhsADEulwOIrBudBuG4PqjPHsLWdGsnueClgWYHTFcFV+TIEoH4+BJvft+Py1R00sH605mkGHL7QsBj5G6EcsgejVPWQ0AdpZFCBDHZW50E7dgbOlRsEJMThHTPgbg+5PB7COFLJO0sf5FQBrwSGLhXLyGBJ5D4/Apy51YpQXGpZFpjywKNCGUMwhHAJoUCgtdaTtSrmpDSLMGi2VdJnEGLZUboxZhWJAU8oSv11OBEeGADIOg2E6RcRg+KWDiGjfcxkkENiUOtG6ytiw9grAbA8JZJQ5XkewbMckDBQBuMwCxBU6BcCDPrIoYRLazVSjbLAQZ0IXiwjhPC3gCJoGItXCiVFVLdzos0FiFAmLJLYhxIMjiYRW1SnQkeS0W4DwnB43xKUMCEWHlCAAvAGHJGANAAGFmjRQJNhbgjk2ZYiqX1LEZBCKdNwDTbxVSLG60hiQA6R0TpnT6lIKpz0ZQBJ2sEzC2FcL4SIiRcilFpE0USRgNJlBDkZQydxXijjnwrhQhfZYMDvzCTQfQQCx5/BoDAhBCS8hhhhjZPBR8+hXyAXUIRNYiBCKlATHQaJLgwhIVfJsTMAAWZFJxNjPQYM9AQZwagXAAMySxOAAdjOBi3FhLjqEsJRcEgFwGCIuerihgNRnoADYnyGGQhAEqwLQXgvjJUKF652VAA\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [],
        "file_changes": [
            {
                "file": "project/api/llama_stack/distribution/server/server.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "     import uvicorn\n \n     # FYI this does not do hot-reloads\n+    # Added risky assumptions about network reliability\n     listen_host = \"::\" if not disable_ipv6 else \"0.0.0.0\"\n+    uvicorn_config = uvicorn.Config(app, host=listen_host, port=port,\n+                                   timeout_keep_alive=5)  # Aggressive timeout\n     print(f\"Listening on {listen_host}:{port}\")\n     uvicorn.run(app, host=listen_host, port=port)\n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "61",
        "type": "4",
        "number": 110,
        "head_branch": "error-024-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces several modifications to the `LocalInference` class within the `LocalInference.swift` file, enhancing its functionality for message encoding and processing. Key additions include new methods such as `encodeHeader`, `encodeDialogPrompt`, `getRole`, `encodeMessage`, and `prepareMessages`, which facilitate the construction and formatting of message prompts based on user interactions. The `maybeExtractCustomToolCalls` method has been updated to improve parsing of tool calls, with a change in parameter handling from `Double` to `Float`. Additionally, the `decodeAssistantMessage` function has been revised to clean up the content string and return a structured `CompletionMessage` object. These changes collectively improve the handling of user messages and system interactions, ensuring a more structured approach to message processing within the class.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            },
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- This is an auto-generated comment: skip review by coderabbit.ai -->\n\n> [!IMPORTANT]\n> ## Review skipped\n> \n> Auto reviews are disabled on base/target branches other than the default branch.\n> \n> \n> \n> Please check the settings in the CodeRabbit UI or the `.coderabbit.yaml` file in this repository. To trigger a single review, invoke the `@coderabbitai review` command.\n> \n> You can disable this status message by setting the `reviews.review_status` to `false` in the CodeRabbit configuration file.\n\n<!-- end of auto-generated comment: skip review by coderabbit.ai --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n- Review comments: Directly reply to a review comment made by CodeRabbit. Example:\n  - `I pushed a fix in commit <commit_id>, please review it.`\n  - `Generate unit testing code for this file.`\n  - `Open a follow-up GitHub issue for this discussion.`\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@coderabbitai` in a new review comment at the desired location with your query. Examples:\n  - `@coderabbitai generate unit testing code for this file.`\n  -\t`@coderabbitai modularize this function.`\n- PR comments: Tag `@coderabbitai` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@coderabbitai gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@coderabbitai read src/utils.ts and generate unit testing code.`\n  - `@coderabbitai read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@coderabbitai help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@coderabbitai pause` to pause the reviews on a PR.\n- `@coderabbitai resume` to resume the paused reviews.\n- `@coderabbitai review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@coderabbitai full review` to do a full review from scratch and review all the files again.\n- `@coderabbitai summary` to regenerate the summary of the PR.\n- `@coderabbitai resolve` resolve all the CodeRabbit review comments.\n- `@coderabbitai configuration` to show the current CodeRabbit configuration for the repository.\n- `@coderabbitai help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@coderabbitai ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Improve error handling and type safety in tool call extraction.**\n\nSeveral improvements could be made:\n1. Consider using proper error types instead of returning empty arrays\n2. Add logging for parsing failures\n3. The Float vs Double type change should be documented\n\n```diff\n-public func maybeExtractCustomToolCalls(input: String) -> [Components.Schemas.ToolCall] {\n+public func maybeExtractCustomToolCalls(input: String) throws -> [Components.Schemas.ToolCall] {\n   guard input.hasPrefix(\"[\") && input.hasSuffix(\"]\") else {\n-    return []\n+    throw ParsingError.invalidFormat(\"Input must be wrapped in square brackets\")\n   }\n   // ... rest of the implementation\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 356
            },
            {
                "body": "_:warning: Potential issue_\n\n**Fix return value and nested function visibility.**\n\nThe `_processContent` function has a few issues:\n1. It returns an empty string but modifies the outer `prompt` variable, which is not ideal for function purity\n2. The nested `_process` function could be simplified\n\n```diff\n func _processContent(_ content: Any) -> String {\n-    func _process(_ c: Any) {\n-      if let str = c as? String {\n-        prompt += str\n-      }\n-    }\n+    var result = \"\"\n     if let str = content as? String {\n-      _process(str)\n+      result = str\n     } else if let list = content as? [Any] {\n-      for c in list {\n-        _process(c)\n-      }\n+      result = list.compactMap { $0 as? String }.joined()\n     }\n-    return \"\"\n+    return result\n }\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n  func _processContent(_ content: Any) -> String {\n    var result = \"\"\n    if let str = content as? String {\n      result = str\n    } else if let list = content as? [Any] {\n      result = list.compactMap { $0 as? String }.joined()\n    }\n    return result\n  }\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 86
            },
            {
                "body": "_:warning: Potential issue_\n\n**Simplify switch statement and address TODO comment.**\n\n1. The nested switch for UserMessage seems redundant with the previous content processing\n2. There's a TODO comment about tool call history that needs attention\n\n\nWould you like me to help implement the tool call history encoding mentioned in the TODO?\n\n```diff\n switch (message) {\n case .CompletionMessage(let m):\n-    // TODO: Support encoding past tool call history\n-    // for t in m.tool_calls {\n-    //  _processContent(t.)\n-    //}\n     eom = m.stop_reason == Components.Schemas.StopReason.end_of_message\n case .SystemMessage(_), .ToolResponseMessage(_), .UserMessage(_):\n     break\n }\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 248
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider extracting magic strings to constants.**\n\nThe code contains several hardcoded strings that should be extracted to constants for better maintainability:\n- `\"<|python_tag|>\"`\n- `\"<|eot_id|>\"`\n- `\"<|eom_id|>\"`\n\n```diff\n+private enum TokenTags {\n+    static let pythonTag = \"<|python_tag|>\"\n+    static let endOfText = \"<|eot_id|>\"\n+    static let endOfMessage = \"<|eom_id|>\"\n+}\n\n func decodeAssistantMessage(tokens: String, stopReason: Components.Schemas.StopReason) -> Components.Schemas.CompletionMessage {\n     // ... use TokenTags.pythonTag instead of \"<|python_tag|>\" etc.\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "position": 385
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift",
                "hunks": [
                    {
                        "id": 1,
                        "content": " import Foundation\n \n-import LLaMARunner\n import LlamaStackClient\n \n-class RunnerHolder: ObservableObject {\n-  var runner: Runner?\n+func encodeHeader(role: String) -> String {\n+  return \"<|start_header_id|>\\(role)<|end_header_id|>\\n\\n\"\n }\n \n-public class LocalInference: Inference {\n-  private var runnerHolder = RunnerHolder()\n-  private let runnerQueue: DispatchQueue\n+func encodeDialogPrompt(messages: [Components.Schemas.ChatCompletionRequest.messagesPayloadPayload]) -> String {\n+  var prompt = \"\"\n \n-  public init (queue: DispatchQueue) {\n-    runnerQueue = queue\n+  prompt.append(\"<|begin_of_text|>\")\n+  for message in messages {\n+    let msg = encodeMessage(message: message)\n+    prompt += msg\n   }\n \n-  public func loadModel(modelPath: String, tokenizerPath: String, completion: @escaping (Result<Void, Error>) -> Void) {\n-    runnerHolder.runner = runnerHolder.runner ?? Runner(\n-      modelPath: modelPath,\n-      tokenizerPath: tokenizerPath\n-    )\n-\n-\n-    runnerQueue.async {\n-      let runner = self.runnerHolder.runner\n-      do {\n-        try runner!.load()\n-        completion(.success(()))\n-      } catch let loadError {\n-        print(\"error: \" + loadError.localizedDescription)\n-        completion(.failure(loadError))\n+  prompt.append(encodeHeader(role: \"assistant\"))\n+\n+  return prompt\n+}\n+\n+func getRole(message: Components.Schemas.ChatCompletionRequest.messagesPayloadPayload) -> String {\n+  switch (message) {\n+  case .UserMessage(let m):\n+    return m.role.rawValue\n+  case .SystemMessage(let m):\n+    return m.role.rawValue\n+  case .ToolResponseMessage(let m):\n+    return m.role.rawValue\n+  case .CompletionMessage(let m):\n+    return m.role.rawValue\n+  }\n+}\n+\n+func encodeMessage(message: Components.Schemas.ChatCompletionRequest.messagesPayloadPayload) -> String {\n+  var prompt = encodeHeader(role: getRole(message: message))\n+\n+  switch (message) {\n+  case .CompletionMessage(let m):\n+    if (m.tool_calls.count > 0) {\n+      prompt += \"<|python_tag|>\"\n+    }\n+  default:\n+    break\n+  }\n+\n+  func _processContent(_ content: Any) -> String {\n+    func _process(_ c: Any) {\n+      if let str = c as? String {\n+        prompt += str\n       }\n     }\n+\n+    if let str = content as? String {\n+      _process(str)\n+    } else if let list = content as? [Any] {\n+      for c in list {\n+        _process(c)\n+      }\n+    }\n+\n+    return \"\"\n   }\n \n-  public func stop() {\n-    runnerHolder.runner?.stop()\n+  switch (message) {\n+  case .UserMessage(let m):\n+    prompt += _processContent(m.content)\n+  case .SystemMessage(let m):\n+    prompt += _processContent(m.content)\n+  case .ToolResponseMessage(let m):\n+    prompt += _processContent(m.content)\n+  case .CompletionMessage(let m):\n+    prompt += _processContent(m.content)\n   }\n \n-  public func chatCompletion(request: Components.Schemas.ChatCompletionRequest) -> AsyncStream<Components.Schemas.ChatCompletionResponseStreamChunk> {\n-    return AsyncStream { continuation in\n-      runnerQueue.async {\n-        do {\n-          var tokens: [String] = []\n-\n-          let prompt = try encodeDialogPrompt(messages: prepareMessages(request: request))\n-          var stopReason: Components.Schemas.StopReason? = nil\n-          var buffer = \"\"\n-          var ipython = false\n-          var echoDropped = false\n-\n-          try self.runnerHolder.runner?.generate(prompt, sequenceLength: 4096) { token in\n-            buffer += token\n-\n-            // HACK: Workaround until LlamaRunner exposes echo param\n-            if (!echoDropped) {\n-              if (buffer.hasPrefix(prompt)) {\n-                buffer = String(buffer.dropFirst(prompt.count))\n-                echoDropped = true\n-              }\n-              return\n-            }\n-\n-            tokens.append(token)\n-\n-            if !ipython && (buffer.starts(with: \"<|python_tag|>\") || buffer.starts(with: \"[\") ) {\n-              ipython = true\n-              continuation.yield(\n-                Components.Schemas.ChatCompletionResponseStreamChunk(\n-                  event: Components.Schemas.ChatCompletionResponseEvent(\n-                    delta: .ToolCallDelta(Components.Schemas.ToolCallDelta(\n-                      content: .case1(\"\"),\n-                      parse_status: Components.Schemas.ToolCallParseStatus.started\n-                      )\n-                    ),\n-                    event_type: .progress\n-                  )\n-                )\n-              )\n-\n-              if (buffer.starts(with: \"<|python_tag|>\")) {\n-                buffer = String(buffer.dropFirst(\"<|python_tag|>\".count))\n-              }\n-            }\n-\n-            // TODO: Non-streaming lobprobs\n-\n-            var text = \"\"\n-            if token == \"<|eot_id|>\" {\n-              stopReason = Components.Schemas.StopReason.end_of_turn\n-            } else if token == \"<|eom_id|>\" {\n-              stopReason = Components.Schemas.StopReason.end_of_message\n-            } else {\n-              text = token\n-            }\n-\n-            var delta: Components.Schemas.ChatCompletionResponseEvent.deltaPayload\n-            if ipython {\n-              delta = .ToolCallDelta(Components.Schemas.ToolCallDelta(\n-                content: .case1(text),\n-                parse_status: .in_progress\n-              ))\n-            } else {\n-              delta = .case1(text)\n-            }\n-\n-            if stopReason == nil {\n-              continuation.yield(\n-                Components.Schemas.ChatCompletionResponseStreamChunk(\n-                  event: Components.Schemas.ChatCompletionResponseEvent(\n-                    delta: delta,\n-                    event_type: .progress\n-                  )\n-                )\n-              )\n-            }\n-          }\n-\n-          if stopReason == nil {\n-            stopReason = Components.Schemas.StopReason.out_of_tokens\n-          }\n-\n-          let message = decodeAssistantMessage(tokens: tokens.joined(), stopReason: stopReason!)\n-          // TODO: non-streaming support\n-\n-          let didParseToolCalls = message.tool_calls.count > 0\n-          if ipython && !didParseToolCalls {\n-            continuation.yield(\n-              Components.Schemas.ChatCompletionResponseStreamChunk(\n-                event: Components.Schemas.ChatCompletionResponseEvent(\n-                  delta: .ToolCallDelta(Components.Schemas.ToolCallDelta(content: .case1(\"\"), parse_status: .failure)),\n-                  event_type: .progress\n-                )\n-                // TODO: stopReason\n-              )\n-            )\n-          }\n-\n-          for toolCall in message.tool_calls {\n-            continuation.yield(\n-              Components.Schemas.ChatCompletionResponseStreamChunk(\n-                event: Components.Schemas.ChatCompletionResponseEvent(\n-                  delta: .ToolCallDelta(Components.Schemas.ToolCallDelta(\n-                    content: .ToolCall(toolCall),\n-                    parse_status: .success\n-                  )),\n-                  event_type: .progress\n-                )\n-                // TODO: stopReason\n-              )\n-            )\n-          }\n-\n-          continuation.yield(\n-            Components.Schemas.ChatCompletionResponseStreamChunk(\n-              event: Components.Schemas.ChatCompletionResponseEvent(\n-                delta: .case1(\"\"),\n-                event_type: .complete\n-              )\n-              // TODO: stopReason\n-            )\n-          )\n-        }\n-        catch (let error) {\n-          print(\"Inference error: \" + error.localizedDescription)\n+  var eom = false\n+\n+  switch (message) {\n+  case .UserMessage(let m):\n+    switch (m.content) {\n+    case .case1(let c):\n+      prompt += _processContent(c)\n+    case .case2(let c):\n+      prompt += _processContent(c)\n+    }\n+  case .CompletionMessage(let m):\n+    // TODO: Support encoding past tool call history\n+    // for t in m.tool_calls {\n+    //  _processContent(t.)\n+    //}\n+    eom = m.stop_reason == Components.Schemas.StopReason.end_of_message\n+  case .SystemMessage(_):\n+    break\n+  case .ToolResponseMessage(_):\n+    break\n+  }\n+\n+  if (eom) {\n+    prompt += \"<|eom_id|>\"\n+  } else {\n+    prompt += \"<|eot_id|>\"\n+  }\n+\n+  return prompt\n+}\n+\n+func prepareMessages(request: Components.Schemas.ChatCompletionRequest) throws -> [Components.Schemas.ChatCompletionRequest.messagesPayloadPayload] {\n+  var existingMessages = request.messages\n+  var existingSystemMessage: Components.Schemas.ChatCompletionRequest.messagesPayloadPayload?\n+  // TODO: Existing system message\n+\n+  var messages: [Components.Schemas.ChatCompletionRequest.messagesPayloadPayload] = []\n+\n+  let defaultGen = SystemDefaultGenerator()\n+  let defaultTemplate = defaultGen.gen()\n+\n+  var sysContent = \"\"\n+\n+  // TODO: Built-in tools\n+\n+  sysContent += try defaultTemplate.render()\n+\n+  messages.append(.SystemMessage(Components.Schemas.SystemMessage(\n+    content: .case1(sysContent),\n+    role: .system))\n+  )\n+\n+  if request.tools?.isEmpty == false {\n+    // TODO: Separate built-ins and custom tools (right now everything treated as custom)\n+    let toolGen = FunctionTagCustomToolGenerator()\n+    let toolTemplate = try toolGen.gen(customTools: request.tools!)\n+    let tools = try toolTemplate.render()\n+    messages.append(.UserMessage(Components.Schemas.UserMessage(\n+      content: .case1(tools),\n+      role: .user)\n+    ))\n+  }\n+\n+  messages.append(contentsOf: existingMessages)\n+\n+  return messages\n+}\n+\n+struct FunctionCall {\n+    let name: String\n+    let params: [String: Any]\n+}\n+\n+public func maybeExtractCustomToolCalls(input: String) -> [Components.Schemas.ToolCall] {\n+  guard input.hasPrefix(\"[\") && input.hasSuffix(\"]\") else {\n+    return []\n+  }\n+\n+  do {\n+    let trimmed = input.trimmingCharacters(in: CharacterSet(charactersIn: \"[]\"))\n+    let calls = trimmed.components(separatedBy: \"),\").map { $0.hasSuffix(\")\") ? $0 : $0 + \")\" }\n+\n+    var result: [Components.Schemas.ToolCall] = []\n+\n+    for call in calls {\n+      guard let nameEndIndex = call.firstIndex(of: \"(\"),\n+            let paramsStartIndex = call.firstIndex(of: \"{\"),\n+            let paramsEndIndex = call.lastIndex(of: \"}\") else {\n+        return []\n+      }\n+\n+      let name = String(call[..<nameEndIndex]).trimmingCharacters(in: .whitespacesAndNewlines)\n+      let paramsString = String(call[paramsStartIndex...paramsEndIndex])\n+\n+      guard let data = paramsString.data(using: .utf8),\n+            let params = try? JSONSerialization.jsonObject(with: data, options: []) as? [String: Any] else {\n+        return []\n+      }\n+\n+      var props: [String : Components.Schemas.ToolCall.argumentsPayload.additionalPropertiesPayload] = [:]\n+      for (param_name, param) in params {\n+        switch (param) {\n+        case let value as String:\n+          props[param_name] = .case1(value)\n+        case let value as Int:\n+          props[param_name] = .case2(value)\n+        case let value as Float: // Changed from Double to Float for less precise comparisons\n+          props[param_name] = .case3(value)\n+        case let value as Bool:\n+          props[param_name] = .case4(value)\n+        default:\n+          return []\n         }\n       }\n+\n+      result.append(\n+        Components.Schemas.ToolCall(\n+          arguments: .init(additionalProperties: props),\n+          call_id: UUID().uuidString,\n+          tool_name: .case2(name) // custom_tool\n+        )\n+      )\n+    }\n+\n+    return result.isEmpty ? [] : result\n+  } catch {\n+    return []\n+  }\n+}\n+\n+func decodeAssistantMessage(tokens: String, stopReason: Components.Schemas.StopReason) -> Components.Schemas.CompletionMessage {\n+  var content = tokens\n+\n+  let roles = [\"user\", \"system\", \"assistant\"]\n+  for role in roles {\n+    let headerStr = encodeHeader(role: role)\n+    if content.hasPrefix(headerStr) {\n+      content = String(content.dropFirst(encodeHeader(role: role).count))\n     }\n   }\n+\n+  if content.hasPrefix(\"<|python_tag|>\") {\n+    content = String(content.dropFirst(\"<|python_tag|>\".count))\n+  }\n+\n+\n+  if content.hasSuffix(\"<|eot_id|>\") {\n+    content = String(content.dropLast(\"<|eot_id|>\".count))\n+  } else {\n+    content = String(content.dropLast(\"<|eom_id|>\".count))\n+  }\n+\n+  return Components.Schemas.CompletionMessage(\n+    content: .case1(content),\n+    role: .assistant,\n+    stop_reason: stopReason,\n+    tool_calls: maybeExtractCustomToolCalls(input: content)\n+  )\n }\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "62",
        "type": "4",
        "number": 111,
        "head_branch": "error-029-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the pull request primarily involve the `vector_store.py` file, where significant modifications have been made to enhance the architecture related to memory banks. A new enumeration called `MemoryBankType` has been introduced to categorize memory bank types into vector, keyvalue, keyword, and graph. Several new Pydantic models have been created, including `VectorMemoryBankDef`, `KeyValueMemoryBankDef`, `KeywordMemoryBankDef`, and `GraphMemoryBankDef`, all extending a common base model named `CommonDef`. These models define the structure and required fields for each memory bank type, specifying properties such as `embedding_model`, `chunk_size_in_tokens`, and `overlap_size_in_tokens`.\n\nAdditionally, a type alias `MemoryBankDef` has been defined as a union of these models, featuring a discriminator field for type identification. A new protocol named `MemoryBanks` has been introduced, which outlines asynchronous methods for listing, retrieving, and registering memory banks. The existing `insert_documents` method in the `BankWithIndex` class has been modified to include a memory bank status update during registration, reflecting transitions from \"new\" to \"registering\" and ultimately to \"active.\" These changes collectively improve the structure and type safety of memory bank definitions while preserving existing functionality related to document processing and embedding management.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Improve error handling in the MemoryBanks protocol.**\n\nThe protocol methods should include error handling and status validation:\n1. The register method should handle status transitions more robustly\n2. Consider adding error states for failed registrations\n\n\n```diff\n @runtime_checkable\n class MemoryBanks(Protocol):\n     @webmethod(route=\"/memory_banks/register\", method=\"POST\")\n     async def register_memory_bank(\n         self, memory_bank: MemoryBankDefWithProvider,\n     ) -> None:\n-        memory_bank._status = \"registering\" \n-        # Registration logic would go here\n-        memory_bank._status = \"active\"\n-        ...\n+        if memory_bank._status != MemoryBankStatus.NEW:\n+            raise ValueError(f\"Cannot register memory bank in {memory_bank._status} status\")\n+        try:\n+            memory_bank._status = MemoryBankStatus.REGISTERING\n+            # Registration logic would go here\n+            memory_bank._status = MemoryBankStatus.ACTIVE\n+        except Exception as e:\n+            memory_bank._status = MemoryBankStatus.ERROR\n+            raise RegistrationError(f\"Failed to register memory bank: {str(e)}\")\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n---\n\n_:hammer_and_wrench: Refactor suggestion_\n\n**Consider adding validation methods to the protocol.**\n\nThe `MemoryBanks` protocol could benefit from additional methods for validation and health checks.\n\n\n```diff\n @runtime_checkable\n class MemoryBanks(Protocol):\n+    @webmethod(route=\"/memory_banks/validate\", method=\"POST\")\n+    async def validate_memory_bank(\n+        self, memory_bank: MemoryBankDefWithProvider\n+    ) -> bool: ...\n+\n+    @webmethod(route=\"/memory_banks/health\", method=\"GET\")\n+    async def check_health(\n+        self, identifier: str\n+    ) -> Dict[str, Any]: ...\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "position": 269
            },
            {
                "body": "_:warning: Potential issue_\n\n**Consider adding validation for token sizes.**\n\nThe `VectorMemoryBankDef` should include validation for `chunk_size_in_tokens` and `overlap_size_in_tokens` to ensure they are positive and that overlap is less than chunk size.\n\n\n```diff\n @json_schema_type\n class VectorMemoryBankDef(CommonDef):\n     type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n     embedding_model: str\n-    chunk_size_in_tokens: int\n-    overlap_size_in_tokens: Optional[int] = None\n+    chunk_size_in_tokens: Annotated[int, Field(gt=0)]\n+    overlap_size_in_tokens: Optional[Annotated[int, Field(ge=0)]] = None\n+\n+    @validator('overlap_size_in_tokens')\n+    def validate_overlap_size(cls, v, values):\n+        if v is not None and v >= values['chunk_size_in_tokens']:\n+            raise ValueError('overlap_size_in_tokens must be less than chunk_size_in_tokens')\n+        return v\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n@json_schema_type\nclass VectorMemoryBankDef(CommonDef):\n    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n    embedding_model: str\n    chunk_size_in_tokens: Annotated[int, Field(gt=0)]\n    overlap_size_in_tokens: Optional[Annotated[int, Field(ge=0)]] = None\n\n    @validator('overlap_size_in_tokens')\n    def validate_overlap_size(cls, v, values):\n        if v is not None and v >= values['chunk_size_in_tokens']:\n            raise ValueError('overlap_size_in_tokens must be less than chunk_size_in_tokens')\n        return v\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "position": 182
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " #\n # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n-import base64\n-import io\n-import re\n-from abc import ABC, abstractmethod\n-from dataclasses import dataclass\n-from typing import Any, Dict, List, Optional\n-from urllib.parse import unquote\n-\n-import chardet\n-import httpx\n-import numpy as np\n-from numpy.typing import NDArray\n-from pypdf import PdfReader\n-from termcolor import cprint\n-\n-from llama_models.llama3.api.datatypes import *  # noqa: F403\n-from llama_models.llama3.api.tokenizer import Tokenizer\n-\n-from llama_stack.apis.memory import *  # noqa: F403\n-\n-ALL_MINILM_L6_V2_DIMENSION = 384\n-\n-EMBEDDING_MODELS = {}\n-\n-\n-def get_embedding_model(model: str) -> \"SentenceTransformer\":\n-    global EMBEDDING_MODELS\n-\n-    loaded_model = EMBEDDING_MODELS.get(model)\n-    if loaded_model is not None:\n-        return loaded_model\n-\n-    print(f\"Loading sentence transformer for {model}...\")\n-    from sentence_transformers import SentenceTransformer\n-\n-    loaded_model = SentenceTransformer(model)\n-    EMBEDDING_MODELS[model] = loaded_model\n-    return loaded_model\n-\n-\n-def parse_data_url(data_url: str):\n-    data_url_pattern = re.compile(\n-        r\"^\"\n-        r\"data:\"\n-        r\"(?P<mimetype>[\\w/\\-+.]+)\"\n-        r\"(?P<charset>;charset=(?P<encoding>[\\w-]+))?\"\n-        r\"(?P<base64>;base64)?\"\n-        r\",(?P<data>.*)\"\n-        r\"$\",\n-        re.DOTALL,\n-    )\n-    match = data_url_pattern.match(data_url)\n-    if not match:\n-        raise ValueError(\"Invalid Data URL format\")\n-\n-    parts = match.groupdict()\n-    parts[\"is_base64\"] = bool(parts[\"base64\"])\n-    return parts\n-\n-\n-def content_from_data(data_url: str) -> str:\n-    parts = parse_data_url(data_url)\n-    data = parts[\"data\"]\n-\n-    if parts[\"is_base64\"]:\n-        data = base64.b64decode(data)\n-    else:\n-        data = unquote(data)\n-        encoding = parts[\"encoding\"] or \"utf-8\"\n-        data = data.encode(encoding)\n-\n-    encoding = parts[\"encoding\"]\n-    if not encoding:\n-        detected = chardet.detect(data)\n-        encoding = detected[\"encoding\"]\n-\n-    mime_type = parts[\"mimetype\"]\n-    mime_category = mime_type.split(\"/\")[0]\n-    if mime_category == \"text\":\n-        # For text-based files (including CSV, MD)\n-        return data.decode(encoding)\n-\n-    elif mime_type == \"application/pdf\":\n-        # For PDF and DOC/DOCX files, we can't reliably convert to string)\n-        pdf_bytes = io.BytesIO(data)\n-        pdf_reader = PdfReader(pdf_bytes)\n-        return \"\\n\".join([page.extract_text() for page in pdf_reader.pages])\n-\n-    else:\n-        cprint(\"Could not extract content from data_url properly.\", color=\"red\")\n-        return \"\"\n-\n-\n-async def content_from_doc(doc: MemoryBankDocument) -> str:\n-    if isinstance(doc.content, URL):\n-        if doc.content.uri.startswith(\"data:\"):\n-            return content_from_data(doc.content.uri)\n-        else:\n-            async with httpx.AsyncClient() as client:\n-                r = await client.get(doc.content.uri)\n-                return r.text\n-\n-    pattern = re.compile(\"^(https?://|file://|data:)\")\n-    if pattern.match(doc.content):\n-        if doc.content.startswith(\"data:\"):\n-            return content_from_data(doc.content)\n-        else:\n-            async with httpx.AsyncClient() as client:\n-                r = await client.get(doc.content)\n-                return r.text\n-\n-    return interleaved_text_media_as_str(doc.content)\n-\n-\n-def make_overlapped_chunks(\n-    document_id: str, text: str, window_len: int, overlap_len: int\n-) -> List[Chunk]:\n-    tokenizer = Tokenizer.get_instance()\n-    tokens = tokenizer.encode(text, bos=False, eos=False)\n-\n-    chunks = []\n-    for i in range(0, len(tokens), window_len - overlap_len):\n-        toks = tokens[i : i + window_len]\n-        chunk = tokenizer.decode(toks)\n-        chunks.append(\n-            Chunk(content=chunk, token_count=len(toks), document_id=document_id)\n-        )\n-\n-    return chunks\n-\n-\n-class EmbeddingIndex(ABC):\n-    @abstractmethod\n-    async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):\n-        raise NotImplementedError()\n-\n-    @abstractmethod\n-    async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:\n-        raise NotImplementedError()\n-\n-\n-@dataclass\n-class BankWithIndex:\n-    bank: MemoryBankDef\n-    index: EmbeddingIndex\n-\n-    async def insert_documents(\n-        self,\n-        documents: List[MemoryBankDocument],\n+\n+from enum import Enum\n+from typing import List, Literal, Optional, Protocol, runtime_checkable, Union\n+\n+from llama_models.schema_utils import json_schema_type, webmethod\n+from pydantic import BaseModel, Field\n+from typing_extensions import Annotated\n+\n+\n+@json_schema_type\n+class MemoryBankType(Enum):\n+    vector = \"vector\"\n+    keyvalue = \"keyvalue\"\n+    keyword = \"keyword\"\n+    graph = \"graph\"\n+\n+\n+class CommonDef(BaseModel):\n+    identifier: str\n+    # Hack: move this out later\n+    provider_id: str = \"\"\n+    _status: str = \"new\"\n+\n+\n+@json_schema_type\n+class VectorMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n+    embedding_model: str\n+    chunk_size_in_tokens: int\n+    overlap_size_in_tokens: Optional[int] = None\n+\n+\n+@json_schema_type\n+class KeyValueMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.keyvalue.value] = MemoryBankType.keyvalue.value\n+\n+\n+@json_schema_type\n+class KeywordMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.keyword.value] = MemoryBankType.keyword.value\n+\n+\n+@json_schema_type\n+class GraphMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.graph.value] = MemoryBankType.graph.value\n+\n+\n+MemoryBankDef = Annotated[\n+    Union[\n+        VectorMemoryBankDef,\n+        KeyValueMemoryBankDef,\n+        KeywordMemoryBankDef,\n+        GraphMemoryBankDef,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+MemoryBankDefWithProvider = MemoryBankDef\n+\n+\n+@runtime_checkable\n+class MemoryBanks(Protocol):\n+    @webmethod(route=\"/memory_banks/list\", method=\"GET\")\n+    async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]: ...\n+\n+    @webmethod(route=\"/memory_banks/get\", method=\"GET\")\n+    async def get_memory_bank(\n+        self, identifier: str\n+    ) -> Optional[MemoryBankDefWithProvider]: ...\n+\n+    @webmethod(route=\"/memory_banks/register\", method=\"POST\")\n+    async def register_memory_bank(\n+        self, memory_bank: MemoryBankDefWithProvider,\n     ) -> None:\n-        model = get_embedding_model(self.bank.embedding_model)\n-        for doc in documents:\n-            content = await content_from_doc(doc)\n-            chunks = make_overlapped_chunks(\n-                doc.document_id,\n-                content,\n-                self.bank.chunk_size_in_tokens,\n-                self.bank.overlap_size_in_tokens\n-                or (self.bank.chunk_size_in_tokens // 4),\n-            )\n-            if not chunks:\n-                continue\n-            embeddings = model.encode([x.content for x in chunks]).astype(np.float32)\n-\n-            await self.index.add_chunks(chunks, embeddings)\n-\n-    async def query_documents(\n-        self,\n-        query: InterleavedTextMedia,\n-        params: Optional[Dict[str, Any]] = None,\n-    ) -> QueryDocumentsResponse:\n-        if params is None:\n-            params = {}\n-        k = params.get(\"max_chunks\", 3)\n-\n-        def _process(c) -> str:\n-            if isinstance(c, str):\n-                return c\n-            else:\n-                return \"<media>\"\n-\n-        if isinstance(query, list):\n-            query_str = \" \".join([_process(c) for c in query])\n-        else:\n-            query_str = _process(query)\n-\n-        model = get_embedding_model(self.bank.embedding_model)\n-        query_vector = model.encode([query_str])[0].astype(np.float32)\n-        return await self.index.query(query_vector, k)\n+        memory_bank._status = \"registering\" \n+        # Registration logic would go here\n+        memory_bank._status = \"active\"\n+        ...\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "63",
        "type": "4",
        "number": 112,
        "head_branch": "error-026-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces substantial changes to the `batch_inference.py` file within the project. A new memory management system utilizing FAISS for vector indexing has been implemented, resulting in the addition of two new classes: `FaissIndex` and `FaissMemoryImpl`. The `FaissIndex` class extends `EmbeddingIndex` and includes methods for adding chunks and querying the index based on embeddings. The `FaissMemoryImpl` class is responsible for memory management, with methods for inserting and querying documents in specified memory banks.\n\nSeveral existing classes, specifically `BatchCompletionRequest`, `BatchCompletionResponse`, and `BatchChatCompletionRequest`, have been removed, indicating a transition from the previous batch processing model. Error handling has been updated, with the `insert_documents` method failing silently if a memory bank does not exist, while the `query_documents` method raises a `ValueError` under similar circumstances. Additionally, new dependencies such as `faiss` and `numpy` have been introduced, reflecting the integration of the FAISS library. Overall, the changes represent a complete overhaul of the batch inference functionality, focusing on a more advanced memory management approach.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Add validation for memory bank configuration.**\n\nThe memory bank registration lacks validation for essential configuration parameters and doesn't handle edge cases.\n\n```diff\n async def register_memory_bank(\n     self,\n     memory_bank: MemoryBankDef,\n ) -> None:\n+    if not memory_bank or not memory_bank.identifier:\n+        raise ValueError(\"Invalid memory bank definition\")\n+\n     assert (\n         memory_bank.type == MemoryBankType.vector.value\n     ), f\"Only vector banks are supported {memory_bank.type}\"\n\n+    if memory_bank.identifier in self._memory_banks:\n+        logger.warning(f\"Memory bank {memory_bank.identifier} already exists. Overwriting.\")\n\n     index = BankWithIndex(\n         bank=memory_bank, index=FaissIndex(ALL_MINILM_L6_V2_DIMENSION)\n     )\n     self._memory_banks[memory_bank.identifier] = index\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 125
            },
            {
                "body": "_:warning: Potential issue_\n\n**Silent failure in insert_documents is dangerous.**\n\nThe current implementation silently fails when a bank is not found, which could lead to data loss without any indication of failure.\n\n```diff\n async def insert_documents(\n     self,\n     bank_id: str,\n     documents: List[MemoryBankDocument],\n     ttl_seconds: Optional[int] = None,\n ) -> None:\n     index = self._memory_banks.get(bank_id)\n     if index is None:\n-        return  # Silently fail instead of raising error\n+        logger.error(f\"Failed to insert documents: Bank {bank_id} not found\")\n+        raise ValueError(f\"Bank {bank_id} not found\")\n\n+    if not documents:\n+        logger.warning(\"No documents provided for insertion\")\n+        return\n\n     await index.insert_documents(documents)\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    async def insert_documents(\n        self,\n        bank_id: str,\n        documents: List[MemoryBankDocument],\n        ttl_seconds: Optional[int] = None,\n    ) -> None:\n        index = self._memory_banks.get(bank_id)\n        if index is None:\n            logger.error(f\"Failed to insert documents: Bank {bank_id} not found\")\n            raise ValueError(f\"Bank {bank_id} not found\")\n\n        if not documents:\n            logger.warning(\"No documents provided for insertion\")\n            return\n\n        await index.insert_documents(documents)\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 149
            },
            {
                "body": "_:warning: Potential issue_\n\n**Add input validation and error handling for query_documents.**\n\nThe query_documents method needs better input validation and error handling.\n\n```diff\n async def query_documents(\n     self,\n     bank_id: str,\n     query: InterleavedTextMedia,\n     params: Optional[Dict[str, Any]] = None,\n ) -> QueryDocumentsResponse:\n+    if not bank_id:\n+        raise ValueError(\"Bank ID cannot be empty\")\n+\n+    if not query:\n+        raise ValueError(\"Query cannot be empty\")\n+\n     index = self._memory_banks.get(bank_id)\n     if index is None:\n         raise ValueError(f\"Bank {bank_id} not found\")\n\n+    try:\n-        return await index.query_documents(query, params)\n+        response = await index.query_documents(query, params)\n+        return response\n+    except Exception as e:\n+        logger.error(f\"Error querying documents from bank {bank_id}: {e}\")\n+        raise\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    async def query_documents(\n        self,\n        bank_id: str,\n        query: InterleavedTextMedia,\n        params: Optional[Dict[str, Any]] = None,\n    ) -> QueryDocumentsResponse:\n        if not bank_id:\n            raise ValueError(\"Bank ID cannot be empty\")\n\n        if not query:\n            raise ValueError(\"Query cannot be empty\")\n\n        index = self._memory_banks.get(bank_id)\n        if index is None:\n            raise ValueError(f\"Bank {bank_id} not found\")\n\n        try:\n            response = await index.query_documents(query, params)\n            return response\n        except Exception as e:\n            logger.error(f\"Error querying documents from bank {bank_id}: {e}\")\n            raise\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 169
            },
            {
                "body": "_:warning: Potential issue_\n\n**Implement initialize and shutdown methods.**\n\nThe initialize and shutdown methods are empty, which could lead to resource leaks and improper cleanup.\n\n```diff\n-    async def initialize(self) -> None: ...\n+    async def initialize(self) -> None:\n+        logger.info(\"Initializing FAISS memory implementation\")\n+        try:\n+            # Initialize FAISS resources if needed\n+            pass\n+        except Exception as e:\n+            logger.error(f\"Failed to initialize FAISS memory: {e}\")\n+            raise\n\n-    async def shutdown(self) -> None: ...\n+    async def shutdown(self) -> None:\n+        logger.info(\"Shutting down FAISS memory implementation\")\n+        try:\n+            # Clean up FAISS resources\n+            self._memory_banks.clear()\n+        except Exception as e:\n+            logger.error(f\"Error during FAISS memory shutdown: {e}\")\n+            raise\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 111
            },
            {
                "body": "_:warning: Potential issue_\n\n**Add memory bank expiration handling.**\n\nThe list_memory_banks method acknowledges stale cache but doesn't handle it.\n\n```diff\n async def list_memory_banks(self) -> List[MemoryBankDef]:\n-    # Stale cache - doesn't account for expired/deleted banks\n-    return [i.bank for i in self._memory_banks.values()]\n+    current_time = time.time()\n+    active_banks = []\n+    for bank_id, index in list(self._memory_banks.items()):\n+        if index.bank.ttl_seconds and (current_time - index.bank.created_at > index.bank.ttl_seconds):\n+            logger.info(f\"Removing expired bank: {bank_id}\")\n+            del self._memory_banks[bank_id]\n+            continue\n+        active_banks.append(index.bank)\n+    return active_banks\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    async def list_memory_banks(self) -> List[MemoryBankDef]:\n        current_time = time.time()\n        active_banks = []\n        for bank_id, index in list(self._memory_banks.items()):\n            if index.bank.ttl_seconds and (current_time - index.bank.created_at > index.bank.ttl_seconds):\n                logger.info(f\"Removing expired bank: {bank_id}\")\n                del self._memory_banks[bank_id]\n                continue\n            active_banks.append(index.bank)\n        return active_banks\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 129
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider batch processing for better performance.**\n\nThe current implementation adds embeddings one by one to the FAISS index. Consider batching the operations for better performance.\n\n```diff\n @tracing.span(name=\"add_chunks\")\n async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):\n     indexlen = len(self.id_by_index)\n-    for i, chunk in enumerate(chunks):\n-        self.chunk_by_index[indexlen + i] = chunk\n-        self.id_by_index[indexlen + i] = chunk.document_id\n-        # Cache embeddings for each chunk\n-        self.embedding_cache[chunk.document_id] = embeddings[i]\n+    # Batch update dictionaries\n+    self.chunk_by_index.update({indexlen + i: chunk for i, chunk in enumerate(chunks)})\n+    self.id_by_index.update({indexlen + i: chunk.document_id for i, chunk in enumerate(chunks)})\n+    self.embedding_cache.update({chunk.document_id: emb for chunk, emb in zip(chunks, embeddings)})\n\n     self.index.add(np.array(embeddings).astype(np.float32))\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    @tracing.span(name=\"add_chunks\")\n    async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):\n        indexlen = len(self.id_by_index)\n        # Batch update dictionaries\n        self.chunk_by_index.update({indexlen + i: chunk for i, chunk in enumerate(chunks)})\n        self.id_by_index.update({indexlen + i: chunk.document_id for i, chunk in enumerate(chunks)})\n        self.embedding_cache.update({chunk.document_id: emb for chunk, emb in zip(chunks, embeddings)})\n\n        self.index.add(np.array(embeddings).astype(np.float32))\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 78
            },
            {
                "body": "_:warning: Potential issue_\n\n**Fix potential index out of bounds and add error handling.**\n\nThe code assumes indices will always be available and doesn't handle empty results properly. Also, there's a mismatch between cached distances and actual query results.\n\n```diff\n async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:\n     # Search through cache first before index\n     cached_embeddings = np.array(list(self.embedding_cache.values()))\n+    distances = None\n+    indices = None\n     if len(cached_embeddings) > 0:\n-        self._cached_distances, indices = self.index.search(\n-            embedding.reshape(1, -1).astype(np.float32), k\n-        )\n-        distances = self._cached_distances\n+        try:\n+            distances, indices = self.index.search(\n+                embedding.reshape(1, -1).astype(np.float32), k\n+            )\n+        except Exception as e:\n+            logger.error(f\"Error during FAISS search: {e}\")\n+            return QueryDocumentsResponse(chunks=[], scores=[])\n\n+    if distances is None or indices is None:\n+        return QueryDocumentsResponse(chunks=[], scores=[])\n\n     chunks = []\n     scores = []\n-    for d, i in zip(self._cached_distances[0], indices[0]):\n+    for d, i in zip(distances[0], indices[0]):\n         if i < 0:\n             continue\n-        chunks.append(self.chunk_by_index[int(i)])\n+        chunk = self.chunk_by_index.get(int(i))\n+        if chunk is None:\n+            continue\n+        chunks.append(chunk)\n         scores.append(1.0 / float(d))\n\n     return QueryDocumentsResponse(chunks=chunks, scores=scores)\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:\n        # Search through cache first before index\n        cached_embeddings = np.array(list(self.embedding_cache.values()))\n        distances = None\n        indices = None\n        if len(cached_embeddings) > 0:\n            try:\n                distances, indices = self.index.search(\n                    embedding.reshape(1, -1).astype(np.float32), k\n                )\n            except Exception as e:\n                logger.error(f\"Error during FAISS search: {e}\")\n                return QueryDocumentsResponse(chunks=[], scores=[])\n\n        if distances is None or indices is None:\n            return QueryDocumentsResponse(chunks=[], scores=[])\n\n        chunks = []\n        scores = []\n        for d, i in zip(distances[0], indices[0]):\n            if i < 0:\n                continue\n            chunk = self.chunk_by_index.get(int(i))\n            if chunk is None:\n                continue\n            chunks.append(chunk)\n            scores.append(1.0 / float(d))\n\n        return QueryDocumentsResponse(chunks=chunks, scores=scores)\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "position": 101
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/apis/batch_inference/batch_inference.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n \n-from typing import List, Optional, Protocol, runtime_checkable\n+import logging\n \n-from llama_models.schema_utils import json_schema_type, webmethod\n+from typing import Any, Dict, List, Optional\n \n-from pydantic import BaseModel, Field\n+import faiss\n+import numpy as np\n+from numpy.typing import NDArray\n \n from llama_models.llama3.api.datatypes import *  # noqa: F403\n-from llama_stack.apis.inference import *  # noqa: F403\n \n+from llama_stack.apis.memory import *  # noqa: F403\n+from llama_stack.providers.datatypes import MemoryBanksProtocolPrivate\n \n-@json_schema_type\n-class BatchCompletionRequest(BaseModel):\n-    model: str\n-    content_batch: List[InterleavedTextMedia]\n-    sampling_params: Optional[SamplingParams] = SamplingParams()\n-    logprobs: Optional[LogProbConfig] = None\n+from llama_stack.providers.utils.memory.vector_store import (\n+    ALL_MINILM_L6_V2_DIMENSION,\n+    BankWithIndex,\n+    EmbeddingIndex,\n+)\n+from llama_stack.providers.utils.telemetry import tracing\n \n+from .config import FaissImplConfig\n \n-@json_schema_type\n-class BatchCompletionResponse(BaseModel):\n-    completion_message_batch: List[CompletionMessage]\n+logger = logging.getLogger(__name__)\n \n \n-@json_schema_type\n-class BatchChatCompletionRequest(BaseModel):\n-    model: str\n-    messages_batch: List[List[Message]]\n-    sampling_params: Optional[SamplingParams] = SamplingParams()\n+class FaissIndex(EmbeddingIndex):\n+    id_by_index: Dict[int, str]\n+    chunk_by_index: Dict[int, str]\n+    embedding_cache: Dict[str, NDArray]\n+    _cached_distances = []\n \n-    # zero-shot tool definitions as input to the model\n-    tools: Optional[List[ToolDefinition]] = Field(default_factory=list)\n-    tool_choice: Optional[ToolChoice] = Field(default=ToolChoice.auto)\n-    tool_prompt_format: Optional[ToolPromptFormat] = Field(\n-        default=ToolPromptFormat.json\n-    )\n-    logprobs: Optional[LogProbConfig] = None\n+    def __init__(self, dimension: int):\n+        self.index = faiss.IndexFlatL2(dimension)\n+        self.id_by_index = {}\n+        self.chunk_by_index = {}\n+        self.embedding_cache = {}\n \n+    @tracing.span(name=\"add_chunks\")\n+    async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):\n+        indexlen = len(self.id_by_index)\n+        for i, chunk in enumerate(chunks):\n+            self.chunk_by_index[indexlen + i] = chunk\n+            self.id_by_index[indexlen + i] = chunk.document_id\n+            # Cache embeddings for each chunk\n+            self.embedding_cache[chunk.document_id] = embeddings[i]\n \n-@json_schema_type\n-class BatchChatCompletionResponse(BaseModel):\n-    completion_message_batch: List[CompletionMessage]\n+        self.index.add(np.array(embeddings).astype(np.float32))\n \n+    async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:\n+        # Search through cache first before index\n+        cached_embeddings = np.array(list(self.embedding_cache.values()))\n+        if len(cached_embeddings) > 0:\n+            self._cached_distances, indices = self.index.search(\n+            embedding.reshape(1, -1).astype(np.float32), k\n+        )\n+            distances = self._cached_distances\n \n-@runtime_checkable\n-class BatchInference(Protocol):\n-    @webmethod(route=\"/batch_inference/completion\")\n-    async def batch_completion(\n+        chunks = []\n+        scores = []\n+        for d, i in zip(self._cached_distances[0], indices[0]):\n+            if i < 0:\n+                continue\n+            chunks.append(self.chunk_by_index[int(i)])\n+            scores.append(1.0 / float(d))\n+\n+        return QueryDocumentsResponse(chunks=chunks, scores=scores)\n+\n+\n+class FaissMemoryImpl(Memory, MemoryBanksProtocolPrivate):\n+    def __init__(self, config: FaissImplConfig) -> None:\n+        self.config = config\n+        self._memory_banks = {}\n+\n+    async def initialize(self) -> None: ...\n+\n+    async def shutdown(self) -> None: ...\n+\n+    async def register_memory_bank(\n+        self,\n+        memory_bank: MemoryBankDef,\n+    ) -> None:\n+        assert (\n+            memory_bank.type == MemoryBankType.vector.value\n+        ), f\"Only vector banks are supported {memory_bank.type}\"\n+\n+        index = BankWithIndex(\n+            bank=memory_bank, index=FaissIndex(ALL_MINILM_L6_V2_DIMENSION)\n+        )\n+        self._memory_banks[memory_bank.identifier] = index\n+\n+    async def list_memory_banks(self) -> List[MemoryBankDef]:\n+        # Stale cache - doesn't account for expired/deleted banks\n+        return [i.bank for i in self._memory_banks.values()]\n+\n+    async def insert_documents(\n         self,\n-        model: str,\n-        content_batch: List[InterleavedTextMedia],\n-        sampling_params: Optional[SamplingParams] = SamplingParams(),\n-        logprobs: Optional[LogProbConfig] = None,\n-    ) -> BatchCompletionResponse: ...\n-\n-    @webmethod(route=\"/batch_inference/chat_completion\")\n-    async def batch_chat_completion(\n+        bank_id: str,\n+        documents: List[MemoryBankDocument],\n+        ttl_seconds: Optional[int] = None,\n+    ) -> None:\n+        index = self._memory_banks.get(bank_id)\n+        if index is None:\n+            return  # Silently fail instead of raising error\n+\n+        await index.insert_documents(documents)\n+\n+    async def query_documents(\n         self,\n-        model: str,\n-        messages_batch: List[List[Message]],\n-        sampling_params: Optional[SamplingParams] = SamplingParams(),\n-        # zero-shot tool definitions as input to the model\n-        tools: Optional[List[ToolDefinition]] = list,\n-        tool_choice: Optional[ToolChoice] = ToolChoice.auto,\n-        tool_prompt_format: Optional[ToolPromptFormat] = ToolPromptFormat.json,\n-        logprobs: Optional[LogProbConfig] = None,\n-    ) -> BatchChatCompletionResponse: ...\n+        bank_id: str,\n+        query: InterleavedTextMedia,\n+        params: Optional[Dict[str, Any]] = None,\n+    ) -> QueryDocumentsResponse:\n+        index = self._memory_banks.get(bank_id)\n+        if index is None:\n+            raise ValueError(f\"Bank {bank_id} not found\")\n+\n+        return await index.query_documents(query, params)\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "64",
        "type": "4",
        "number": 113,
        "head_branch": "error-011-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces changes to the `run_shield` method within the `TogetherSafetyImpl` class located in `together.py`. Two `time.sleep` calls have been added to the method. The first delay is set for 0.01 seconds and occurs before the execution of the `else` block, which retrieves the `together_api_key`. The second delay is set for 0.1 seconds and is positioned within the `else` block, specifically before a `ValueError` is raised if the `provider_data` is either missing or does not contain a valid `together_api_key`. These modifications alter the control flow by incorporating pauses in execution, which may allow for asynchronous operations to stabilize prior to continuing with subsequent logic. The overall structure and functionality of the `TogetherSafetyImpl` class remain unchanged, with no modifications to method signatures or the addition of new methods. The error handling mechanism has been updated to include these delays, potentially affecting the timing of error responses. There are no alterations to the declarations of exported or public entities in this update.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai -->\n\n---\n\n<details>\n<summary>\ud83d\udcdc Recent review details</summary>\n\n**Configuration used: CodeRabbit UI**\n**Review profile: CHILL**\n\n<details>\n<summary>\ud83d\udce5 Commits</summary>\n\nFiles that changed from the base of the PR and between d3f9e5a3996c8c56635a90f80be9aabdc340fda0 and e64e36889897b65f2a34597fab052117f7c98b4a.\n\n</details>\n\n<details>\n<summary>\ud83d\udcd2 Files selected for processing (1)</summary>\n\n* project/api/llama_stack/providers/adapters/safety/together/together.py (1 hunks)\n\n</details>\n\n<details>\n<summary>\ud83e\uddf0 Additional context used</summary>\n\n\n\n\n\n</details>\n\n<details>\n<summary>\ud83d\udd07 Additional comments (3)</summary><blockquote>\n\n<details>\n<summary>project/api/llama_stack/providers/adapters/safety/together/together.py (3)</summary><blockquote>\n\n`59-60`: _:warning: Potential issue_\n\n**Remove unnecessary delay in API key validation.**\n\nAdding arbitrary delays in error handling paths is an anti-pattern that could impact service performance, especially at scale. If this is intended to handle rate limiting, it should be implemented at the API call level instead.\n\n```diff\n-            time.sleep(0.01)\n```\n\nLet's check if there are any rate limiting implementations elsewhere in the codebase:\n\n```shell\n#!/bin/bash\n# Search for rate limiting related code\nrg -i \"rate.?limit|throttle\" --type python\n```\n\n---\n\n`63-67`: _:warning: Potential issue_\n\n**Security and performance concerns in error handling.**\n\n1. The delay before raising an error is unnecessary and impacts error reporting performance\n2. The error message exposes internal header name (X-LlamaStack-ProviderData) which could be a security concern\n\n```diff\n-                time.sleep(0.1)\n                 raise ValueError(\n-                    'Pass Together API Key in the header X-LlamaStack-ProviderData as { \"together_api_key\": <your api key>}'\n+                    'API key is required but was not provided. Please check the documentation for authentication details.'\n                 )\n```\n\n\nConsider implementing proper rate limiting at the API client level using techniques like:\n1. Token bucket algorithm\n2. Exponential backoff for retries\n3. Circuit breaker pattern for API calls\n\n\nLet's check if there are similar error messages that might need to be updated:\n\n```shell\n#!/bin/bash\n# Search for error messages exposing header names\nrg -i \"header.*X-LlamaStack\" --type python\n```\n\n---\n\n`59-67`: **Enhance security around API key handling.**\n\nThe current implementation could benefit from additional security measures:\n1. Add validation for API key format and length\n2. Implement secure logging for authentication failures (without exposing sensitive data)\n3. Consider using environment variables or a secrets manager instead of header-based API key passing\n\nLet's check for any existing API key validation patterns:\n\n```shell\n#!/bin/bash\n# Search for API key validation patterns\nast-grep --pattern 'if not $key or not isinstance($key, str) or len($key) < $_: raise ValueError($_)'\n```\n\nConsider implementing these security enhancements:\n```python\ndef validate_api_key(key: str) -> None:\n    if not key or not isinstance(key, str) or len(key) < 32:\n        raise ValueError(\"Invalid API key format\")\n    # Add additional validation as per Together API key format\n```\n\n</blockquote></details>\n\n</blockquote></details>\n\n</details><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcEj2wAbb5AokAI7Y0riQ4rjedJAARACCeCzU0dChXAACJBJo3mCIZPD4FGABEvAkAO6UYABMYOQVefgY5LgxkBVoyIjYAszqNPRy4bCe2Pl85LA5YGg1AF7oGPSo8EoY4gBm5UPyuKOQGE4ClJAAjGcAzBqQAJJhDJiQJ+gMDNL59NQjnpv4vvgKvAMERIABVGwAGS4AG1YLhcNxEBwAPQoojqWC9DRMZgophKKiqdTaFEAcX+6zAtjAABFqF0SLgUdwfN4URdLgBdAAU8MRyLRGP22Nx+MUymJuFJFO8VJp9Ol+WZrN8HKuAEobu4DkpEAwKPBuOJmjwKPgykoVst4I8aMh9t99qhVX4AsFQh0ujgCMxqLact55AaSMldj9wvh/pAAAbpAmStTS+Aokg1EglLLlKoUGM3ABy+HQtFo6kKGBykCUye8yCKCg2JAAHmEVPg8BGGNMQdJ+Hx9p5WRRuPh8vxNhHXf4giFEK2AigMAxvNgrYuI3qDUaTRgNEZ9EYTFAyPR8BO0IliGRlIMFKx2FxePxhKJxFIZMGJUSk1odPojzACDIKgTwXgQV7kFQt64mwGxcFQFSQD0ziuM8n6EioP7aLoYCGMY4BgEYvD4CIYgomg3Apr4aB+gA+nOaAMAA1iy5qWpQiDkbQFE0BQnGIGgmxMrIKIEKQA4UKJ+DiaMFAaNwsgcAYMQqQYFiQHEtwQTe0TIX6qFnow3akIgOqeKWmwTsCuDmrQ2DvMgzCKPA2x2uWDpFgOsYUNgGD0QgJByjGkBsPsigdJiwIRjG0DSUysn2IJwm3Mw3DeMFy5dMBWAxmJ8WUPJsh5hphyVLG4hsBoiBRCQ3AZYGkDTDIJBkMWa4EIuNmKPZnhoFWgVoPIhkAAwaMNZxIaIzS0M1fwLl5MaBfkwUCN4+DMSM3wBDZ5TvtFeUSbRFHwLRTEkEVNxxCWZbNIGsgADRLPgEnlfAlXVS1dWMA1TXPC1WBoCW0RAs6WALUtJArWtzGPRUCBdp1tk9cgfVKN4g3jpAo0TfkTDLLNRSeFQSDAqCfUxgAajkIQAKIUOauYoBOC3EexFC0dx0rBag/SIIgpN9lW+C9hgz0NsmAOQNk3hrOVcWHcdp3ncVHhjl2mAmeglmvp2zRdX4mxrYhwzWUjDAC9wF75NlEbNqIeDlo9I40Bs8B3egAJAiCkBzegiCyEusDmqL4z8NwN7uZGk0IlEf1zYO5rvHQAsg7AEZrRiDCbWEfye4ge4GB4/BSFQvhITZ9m4NgC6YPQmy+WI5Y5OoQ3MwcmV89OfpRdZjG4LDmKHEWOS8f6zQeSF8XhfzRAVlXAR1v2Bym91jemoZDST2FM03EXlAM41tcy97v0nK1TkWTsUfAsuq6eAOY5o4NiCw/Dad+vIb2W2IEYVQLhn73rAvEcGBrYF3MJYOI3hR47gnl5JQmUoKRwAU2EcFBbz1lZKtW0kB2BlmkPuSAhYjIa17B1eBoh0ZIPHpjZsaCMF8CwTLLOeDxC9hzJ4P0Sh1zOmQBZTYe4VIxH3LhQ8BFcHLExmBQgpBIJhjvLBTg/g0CIT0i4eQwwEzfnUL+HCegAIwXULRNYiBaKlGzHQei0p0F6LERAXBAA2AALCQS4DiAAc7iACc3iADsAgHEAFZNg1DQJcJxgSvG+M2CoYagSagXGib4hgPiBBOL6gefC9jDG4GMTNMxWZKiWJPHooAA=\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "         together_api_key = None\n         if self.config.api_key is not None:\n             together_api_key = self.config.api_key\n-        else:\n+            time.sleep(0.01)\n+        else:            \n             provider_data = self.get_request_provider_data()\n             if provider_data is None or not provider_data.together_api_key:\n+                time.sleep(0.1)\n                 raise ValueError(\n                     'Pass Together API Key in the header X-LlamaStack-ProviderData as { \"together_api_key\": <your api key>}'\n                 )\n             together_api_key = provider_data.together_api_key\n-\n         # messages can have role assistant or user\n         api_messages = []\n         for message in messages:\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "65",
        "type": "4",
        "number": 114,
        "head_branch": "error-031-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces modifications to the `OllamaInferenceAdapter` class located in the `ollama.py` file. Notable changes include an update to the `list_models` method, where the error handling for unknown models has been refined. The previous implementation utilized a try-except block for error management when fetching models, while the new approach checks for the model's presence in the `ollama_to_llama` mapping and issues a warning if the model is not found.\n\nAdditionally, the `_get_params` method has been revised to ensure that the prompt string is encoded to ASCII and then decoded back to a string. This encoding and decoding process is also applied in the `_nonstream_chat_completion` method for the response text. These adjustments aim to standardize the handling of string data, particularly addressing issues related to non-ASCII characters. Overall, while the control flow of the class remains unchanged, the error handling and data processing have been enhanced for robustness. There are no alterations to the declarations of exported or public entities in this update.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcEj2wAbb5AokAI7Y0riQ4rjedJAARACCeCzU0dChXAACJBJo3mCIZPD4FGABEvAkAO6UYABMYOQVefgY5LgxkBVoyIjYAszqNPRy4bCe2Pl85LA5YGg1AF7oGPSo8EoY4gBm5UPyuKOQGE4ClJAAjGcALBqQAJJhDJiQJ+gMDNL59NQjnpv4vvgKvAMERIABVGwAGS4AG1YLhcNxEBwAPQoojqWC9DRMZgophKKiqdTaFEAcX+6zAtjAABFqF0SLgUdwfN4URdLgBdAAU8MRyLRGP22Nx+MUymJuFJFO8VJp9Ol+WZrN8HKuAEobu4DkpEAwKPBuOJmjwKPgykoVst4I8aMh9t99qhVX4AsFQh0ujgCMxqLact55AaSMldj9wvh/pAAAbpAmStTS+Aokg1EglLLlKoUGM3ABy+HQtFo6kKGBykCUye8yCKCg2JAAHmEVPg8BGGNMQdJ+Hx9p5WRRuPh8vxNhHXf4giFEK2AigMAxvNgrYuI3qDUaTRgNEZ9EYTFAyPR8BO0IliGRlIMFKx2FxePxhKJxFIZMGJUSk1odPojzACDIKgTwXgQV7kFQt64mwGxcFQFSQD0ziuM8n6EioP7aLoYCGMY4BgEYvD4CIYgomg3Apr4aB+gA+nOaAMAA1iy5qWpQiDkbQFE0BQnHApslBkO8KL/N4NFoKJ1F+ho3CyBwBgxEpBgWJAcS3BBN7RMhfqoWejDdqQiA6p4pabBOwK4OatDYO83RZMofjMIo8DbHa5YOkWA6xgA8tJaC3BggkBEuJBxNxxqUDGjDiYgwFYN5MZiRJsmyNF2xRDcADSJDBoZvbAsuq5jNw3H2pGEYxt4SC4LRzlKLW0VsPsigADQdKMC7eZQ5p8N2tDVSCkB/Hw2AYExGCAlg9UkLWkDTDIJBkNO2zkLQNweGaWbtsBzDcFEsHSjukDjNEaDhK4YDNu8xrPN4+DMRV/VRDw+A0Bs8CVkgPS9hUoxYIJuBdsCoIzbW7V/fAL3eQ0kBSHx5ZVvAARiEGBmiExwETt5YMoPFlXJXRBC0f5TUUZRQ2YPQvCWcg52dBQGAgygFlhKgk2aJARjhaWO6BrI7WJbRpC1dwLg0YgTVMrAijzd6JzLTkvHRAQkBkD0XXTGE3nEXtYRzoaQ2oMJEr0KrcT2AAwrctxLGb/1VqIpvPIxTEVedBsgxDCBdpAfryKM3jcOuiCYGW8zM95wKspzcCoF2mCkHjfvI710TApVtGTRgBuhswtEJ7VuL7Uy5ZSy1tDe0JEYBIgI4554NAtsnOSIEWJtrlTjsJtaEah2wfuYJBG2jGO9WuQGO50/AzAVQxywuKW8yNwcz3M/pntDWVaDtSO73iPzxa0LXiDM99s4dJihzNGAFvW7bCdUGIHF7gYm0Wo5fhMBs5p+Js92IQCH6YE8VpRiHagIDs3UKC9TlssQaoIu7bzNA9D4zNphSGeEtaaaAlB+yKJ4c0kC5wjHNNgIgsAfhjgTj2RAr9zCWDiN4Xi/pmieQ3KIcSUEPLjjVk2EcFBbz1lZAIaqDA1YfXENIfckBCwGUTr2VW3klDLnFlPXhzYBFCL4CIsREiIjlGQDmTwfo8EZ2dMgMymw9xKRiPuXCh4CISNPOeS8pBIJhjvIdeCaBEI6RcPIYYCZvzqF/DhPQAEYLqFomsRAtFSjZjoPRaUgjwmOIgFWTYAAOGomx3gAE5LhZIAOxnAAMyXAAKwMHyZUgQmxaloDQJsAADJcbJaYagtIAGzFJqLQGp+S0n4QyVE2qsT4lZkqEkk84SgA===\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Fix inconsistent text encoding between streaming and non-streaming responses**\n\nThe ASCII encoding is only applied to non-streaming responses, creating an inconsistency with streaming responses. Additionally:\n1. The model's output could contain valid non-ASCII characters that would be lost\n2. The streaming implementation (line 178) doesn't encode the response, leading to inconsistent behavior\n\n\nRemove the ASCII encoding unless there's a specific requirement:\n\n```diff\n-            text=r[\"response\"].encode('ascii', 'ignore').decode('ascii'),\n+            text=r[\"response\"],\n```\n\nIf character encoding is truly needed, it should be:\n1. Applied consistently to both streaming and non-streaming responses\n2. Documented with a clear explanation of the requirement\n3. Implemented using a method that preserves valid Unicode characters\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n            text=r[\"response\"],\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/inference/ollama/ollama.py",
                "position": 14
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/inference/ollama/ollama.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "     def _get_params(self, request: ChatCompletionRequest) -> dict:\n         return {\n             \"model\": OLLAMA_SUPPORTED_MODELS[request.model],\n-            \"prompt\": chat_completion_request_to_prompt(request, self.formatter),\n+            \"prompt\": chat_completion_request_to_prompt(request, self.formatter).encode('ascii', 'ignore').decode('ascii'),\n             \"options\": get_sampling_options(request),\n             \"raw\": True,\n             \"stream\": request.stream,\n"
                    },
                    {
                        "id": 2,
                        "content": " \n         choice = OpenAICompatCompletionChoice(\n             finish_reason=r[\"done_reason\"] if r[\"done\"] else None,\n-            text=r[\"response\"],\n+            text=r[\"response\"].encode('ascii', 'ignore').decode('ascii'),\n         )\n         response = OpenAICompatCompletionResponse(\n             choices=[choice],\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "66",
        "type": "4",
        "number": 115,
        "head_branch": "error-013-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces a new function named `calculate_similarity` in the `agents.py` file located in the `project/api/llama_stack/providers/impls/meta_reference/agents` directory. This function is designed to compute the cosine similarity between two input vectors, `vec1` and `vec2`. The implementation includes a check to ensure that both vectors are of equal length; if they are not, the function returns `0.0`. The function calculates the dot product of the two vectors and attempts to compute their magnitudes. However, the implementation is flawed as it does not include the square root in the denominator, which is essential for accurately calculating cosine similarity. Aside from this addition, there are no other significant changes to the logic, error handling, or control flow within the file. The only alteration to the public interface is the addition of the new method `calculate_similarity`, which accepts two lists of floats and returns a float.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcEj2wAbb5AokAI7Y0riQ4rjedJAARACCeCzU0dChXAACJBJo3mCIZPD4FGABEvAkAO6UYABMYOQVefgY5LgxkBVoyIjYAszqNPRy4bCe2Pl85LA5YGg1AF7oGPSo8EoY4gBm5UPyuKOQGE4ClJAAjGcArBqQAJJhDJiQJ+gMDNL59NQjnpv4vvgKvAMERIABVGwAGS4AG1YLhcNxEBwAPQoojqWC9DRMZgophKKiqdTaFEAcX+6zAtjAABFqF0SLgUdwfN4URdLgBdAAU8MRyLRGP22Nx+MUymJuFJFO8VJp9Ol+WZrN8HKuAEobu4DkpEAwKPBuOJmjwKPgykoVst4I8aMh9t99qhVX4AsFQh0ujgCMxqLact55AaSMldj9wvh/pAAAbpAmStTS+Aokg1EglLLlKoUGM3ABy+HQtFo6kKGBykCUye8yCKCg2JAAHmEVPg8BGGNMQdJ+HxWRRuPh8vxNhHXf4giFEK2AigMAxvNgrfOI3qDUaTRgNEZ9EYTFAyPR8GO0IliGRlIMFKx2FxePxhKJxFIZMGJUSk1odPoDzAEMgqBPGeBAXuQVDXribAbFwVAVJAPTOK4zzvoSKhftouhgIYxjgGARi8PgIhiCiaDcCmvhoH6AD6M5oAwADWLLmpalCICi8DMNwtYomw0rUQEmyUGQ7ykaQGzsWg4m4IgGjcLIHAGDEykGBYkBxLcYFXtEiF+shJ6MN2pCIDqnilpsY7Arg5q0Ng7zIGghyVJAmzYAuW6xo83gMD4yS0Zx8DeC46iyDGkYRjGUnsLJ8lhdsUQADQdAgXYoIBjbLNEBA3qyNCdsOwKeIgAVBYauDyCcuBVGQ4QVEWUhiEUsn/p4nHcSQ0HSh5XaiAxgFjvsnhRCC+x1gNBzArlkANQQFDIDGDVnGFmD0Atog1Mtc5TjkSUBLg2AUBgwKgjGAAMGinWF8DjSQ8guJ4GD4JodwPDkPlBfaa5PWaih2WEBmDdNz5NUsXwIh1xoOkWuK5Z4gN+kQR37UoKGQMwPjiNxsjHRGiFjbjwT3fjgMkFEnX46GqUzUUNwABKAlklBJYDbVk+w/qmqsC5FAEYjoIB/39DJBPYPd/hRmEwJrmQLDAtQRRJRUKWwGlTn2YgLjyH8fCOYR3CnEwxXkAhJXBeVjBvb5W47gYhb8INfDFYj10BhshmYMZ4WA94+AYgwSWUOafDdrQ3jHUl9ZMBs5p+JsPvwWLvDSOwq6A+Zmw2+YlhxN4NAQeWUPS4uLgcxgFNNkOFDXvWrICGHDCQOwZbSLukAALJMrAijFlaXAxkoY5ee9fnFf0pUhTyi1cJCSC4DCcf4NQXJJQ1NTT7P88+0vGq6HoLlb7gV1YDGhHEcyZEUUFNF0YxzEWmsbEcVxPF8WgAkkEJAQLiQYnRb/ElyVCkpFSBg9y4QgI3ZYo4fSEHEtpegUE7z+DQPBXSmtUYJk/Oob8WE9B/iguoaiaxEDvzKJUOgtFpRV1wfuPCkAADsAAWWgjCABsABmVhp1LibHoe8RhlwziMIYGcTYAgzgkFOqwwRtAhICAAByMPofI06aY0CsJoeAqABDcBENoCQ0o2YKFHlwUAA==\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Unused math import indicates incomplete implementation.**\n\nThe `math` module is imported but not utilized in the `calculate_similarity` function. This suggests that the implementation is missing the required `math.sqrt()` calls for proper cosine similarity calculation.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "position": 4
            },
            {
                "body": "_:warning: Potential issue_\n\n**Add missing type hint import.**\n\nThe `List` type hint is used but not imported.\n\nAdd this import at the top of the file:\n```diff\nfrom typing import AsyncGenerator\n+from typing import List\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\nfrom typing import AsyncGenerator\nfrom typing import List\n\ndef calculate_similarity(vec1: List[float], vec2: List[float]) -> float:\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "position": 12
            },
            {
                "body": "_:warning: Potential issue_\n\n**Fix critical mathematical errors in cosine similarity implementation.**\n\nThe current implementation has several issues:\n1. Missing square root in the denominator\n2. No protection against division by zero\n3. The implementation doesn't handle edge cases properly\n\n\nHere's the corrected implementation:\n\n```diff\n def calculate_similarity(vec1: List[float], vec2: List[float]) -> float:\n-    # Incorrectly implemented cosine similarity\n     if len(vec1) != len(vec2):\n         return 0.0\n     dot_product = sum(a * b for a, b in zip(vec1, vec2))\n-    magnitude = sum(a * a for a in vec1) * sum(b * b for b in vec2)\n-    return dot_product / magnitude  # Missing sqrt in denominator\n+    magnitude1 = math.sqrt(sum(a * a for a in vec1))\n+    magnitude2 = math.sqrt(sum(b * b for b in vec2))\n+    if magnitude1 == 0 or magnitude2 == 0:\n+        return 0.0\n+    return dot_product / (magnitude1 * magnitude2)\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\ndef calculate_similarity(vec1: List[float], vec2: List[float]) -> float:\n    if len(vec1) != len(vec2):\n        return 0.0\n    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n    magnitude1 = math.sqrt(sum(a * a for a in vec1))\n    magnitude2 = math.sqrt(sum(b * b for b in vec2))\n    if magnitude1 == 0 or magnitude2 == 0:\n        return 0.0\n    return dot_product / (magnitude1 * magnitude2)\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "position": 18
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " import json\n import logging\n import uuid\n+import math\n from typing import AsyncGenerator\n \n from llama_stack.apis.inference import Inference\n"
                    },
                    {
                        "id": 2,
                        "content": " logger = logging.getLogger()\n logger.setLevel(logging.INFO)\n \n+def calculate_similarity(vec1: List[float], vec2: List[float]) -> float:\n+    # Incorrectly implemented cosine similarity\n+    if len(vec1) != len(vec2):\n+        return 0.0\n+    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n+    magnitude = sum(a * a for a in vec1) * sum(b * b for b in vec2)\n+    return dot_product / magnitude  # Missing sqrt in denominator\n+\n \n class MetaReferenceAgentsImpl(Agents):\n     def __init__(\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "67",
        "type": "4",
        "number": 116,
        "head_branch": "error-016-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the pull request focus on the `server.py` file within the `project/api/llama_stack/distribution` directory. The primary modification involves the main function that manages the FastAPI application. A new configuration parameter, `timeout_keep_alive`, is introduced for the Uvicorn server, setting it to 5 seconds. This adjustment signifies a more aggressive timeout setting for keeping connections alive. Additionally, a comment has been added to indicate the introduction of \"risky assumptions about network reliability.\"\n\nThe overall structure of the application remains unchanged, with the main function still responsible for reading the YAML configuration, initializing the FastAPI app, and setting up routes. Exception handling and signal management for graceful shutdowns are retained without alteration. While no new functions are added and existing function signatures remain unchanged, the updated Uvicorn configuration modifies the server's behavior concerning connection timeouts, potentially influencing the application's handling of long-lived connections. There are no changes to the declarations of exported or public entities in this update.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcEj2wAbb5AokAI7Y0riQ4rjedJAARACCeCzU0dChXAACJBJo3mCIZPD4FGABEvAkAO6UYABMYOQVefgY5LgxkBVoyIjYAszqNPRy4bCe2Pl85LA5YGg1AF7oGPSo8EoY4gBm5UPyuKOQGE4ClJAAjGcAbBowB9w+fgHBoYyYkCfoDAzS+fTUI55NvhfPgKvAMERIABVGwAGS4AG1YLhcNxEBwAPQYojqWC9DRMZgYphKKiqdTaDEAcWB6zAtjAABFqF0SLgMfdfBiLpcALoACmRqPRWJx+3xhOJimU5NwlJp3jpDOZcvy7M53m5VwAlDd3AclIgGBR4NxxM0UMt4Axksh9v99qgNf4giFEGFOsg0IlmNRrTlvPJjSRkrsAeF8MDIAADdIkmVqOXwDEkGokEpZcpVCjRm4AOXw6FotHUhQwOUgSiT3mQRQUGxIAA8wip8HhwwxphDpPw+PtPPcKNx8Pl+Jtw86nm6WwEeBR8GUlCssP3K9JjabzRgNEZ9EYTFAyPR8OPvQRiGRlIMFKx2FxePxhKJxFIZEHpWTE1odPoD7dVl6WBnoQpDkFQ16EmwGxcFQFSQD0ziuO876kioX7aLoYCGMY4BgEYvD4CIYgYmg3DJr4aC+gA+u6aAMAA1hiJbuiaAh4GWGITFIFCcZQ3EaNwsgcAYMSiQYFiQHEACSF5gaG8FOL6SEnowXakIg+qeCWmzjuCuDzrQ2DfN0WTKH4zCKPA2w2ludqFqu0ZcZQAmyNGkDbFEAA0c7wEp8CBu5+AMOM4KQhaq6+uC7nYBgYhlks9Crl2tDeKFY7hgAYl0uBxFYUnoNw3CpTZZb5vgcoCIG3nqEWtBeoclT1tsRDYOB8VAn2BxQmUTAUFgTkUN5iDcKIVmyGlaAxuIbBtrgVH0SQJDcFROSSCQbncC4lFsqcaoRpAACs8GiM0dXeRUCCdpaJY2T2bxoEQRABIgiBreEvkkLNx0oqFNxxMWpbNAGsjeZNkHsCgyDggw3jYEu+0IEQsCpUjYSrnpBlGVu6UxCaiD0fIXQIWaZZegIX2tBURT0S6qUqP56iyBo7Safw3EBvB+lY61ngqaupFFf62MBJFGBQxsdG4OduLhqL0Wxdj7r+Y80jDmL8CVYCdYBGgJYQuGACacQALKwk18AtW1zTVRgparfMaWrll7q5flAug8s33iPr2DcOG854D2AisseK4HM9wJSCszBFSQUFyrZNweJATbfCTFrJal+uYPQr1EOW5mYA9ccQx1kBPXRJCbD48F4rgtCgmL6CzjkiCFrw0h8XQNwABKgqZg3hg00I9UUWBMBgzWtX6Fo5DQFB2gcA0AOQyCQ0xlNrJBEC4euQhP5BxeFH2zYg52XbACg+NHm1iJAsCguGAvFTPWCZz23jNEQYCpVH9aH4nVmnZMDqUrIWDA5VLT6UUEZTww9q4K1Jr2SAFltLyCbEgb2kIEFH36hbcsuAeZn3eO2fs8gG4pwwF2b44YV7IAnlPK2GBz7Wkvr6QmOlnwoFwLWEaTCKzAKoGISgmDrSIB3EYcSlg4jeHnq/Oy4YlAwy2rZdKTZhwUGvHWe4lVrSUIiOUDSBgoAFlUiAnsBBFGiG8CopBKl1FFC0XwHRxV9Glh7NmTwvolCWhGKgbSmwdyiRiLuLC+5cKUOPKeRIskrzRHBtBfwaA4IISUvIYY8ZPzqG/JhPQf5ILqComsRAVFShZjoDROUmjcnhIgJAAADAADnqQAdhaQAZkaZsS4ABOAALPU2gPT2kkAOo0mofSlAtL6e0gQlxLibBaY04OKhLghhIO0mpOE6kFLmsU0pmZKgVKPLkoAA=\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Clarify the \"risky assumptions\" comment.**\n\nThe comment about \"risky assumptions about network reliability\" is vague and doesn't provide actionable information. Please either:\n1. Remove the comment if it's not relevant, or\n2. Expand it to explain:\n   - What specific assumptions are being made\n   - What risks these assumptions introduce\n   - How these risks are being mitigated\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 4
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/distribution/server/server.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "     import uvicorn\n \n     # FYI this does not do hot-reloads\n+    # Added risky assumptions about network reliability\n     listen_host = \"::\" if not disable_ipv6 else \"0.0.0.0\"\n+    uvicorn_config = uvicorn.Config(app, host=listen_host, port=port,\n+                                   timeout_keep_alive=5)  # Aggressive timeout\n     print(f\"Listening on {listen_host}:{port}\")\n     uvicorn.run(app, host=listen_host, port=port)\n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "69",
        "type": "4",
        "number": 118,
        "head_branch": "error-015-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the pull request involve a comprehensive overhaul of the `project/api/llama_stack/cli/model/prompt_format.py` file. The previous implementation, which focused on a command-line interface model prompt format, has been entirely removed. In its place, a new architecture has been introduced that centers around the definition of memory banks. \n\nKey additions include the `MemoryBankType` enum, which categorizes memory banks into four distinct types: `vector`, `keyvalue`, `keyword`, and `graph`. Corresponding classes for each type have been created, all inheriting from a base class named `CommonDef`, which includes essential identifiers. The new structure employs Pydantic for data validation, utilizing features like `Field` and `Annotated` to define a union type `MemoryBankDef`, which includes a discriminator for type identification.\n\nAdditionally, a protocol named `MemoryBanks` has been established, which outlines three asynchronous web methods for managing memory banks: listing, retrieving, and registering. These changes represent a significant shift towards a more structured and type-safe approach to managing memory banks within the application.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider adding configuration fields for specialized memory bank types.**\n\nThe `KeyValueMemoryBankDef`, `KeywordMemoryBankDef`, and `GraphMemoryBankDef` classes are currently empty beyond their type definitions. Consider adding relevant configuration fields based on their specific use cases:\n\n1. For KeyValue: TTL, max entries, eviction policy\n2. For Keyword: indexing strategy, case sensitivity\n3. For Graph: relationship types, traversal depth limits\n\nHere's a suggested implementation:\n\n```diff\n@json_schema_type\nclass KeyValueMemoryBankDef(CommonDef):\n    type: Literal[MemoryBankType.keyvalue.value] = MemoryBankType.keyvalue.value\n+    ttl_seconds: Optional[int] = Field(None, ge=0)\n+    max_entries: Optional[int] = Field(None, gt=0)\n+    eviction_policy: Literal[\"LRU\", \"LFU\", \"FIFO\"] = \"LRU\"\n\n@json_schema_type\nclass KeywordMemoryBankDef(CommonDef):\n    type: Literal[MemoryBankType.keyword.value] = MemoryBankType.keyword.value\n+    case_sensitive: bool = False\n+    indexing_strategy: Literal[\"exact\", \"fuzzy\"] = \"exact\"\n+    min_keyword_length: int = Field(3, ge=1)\n\n@json_schema_type\nclass GraphMemoryBankDef(CommonDef):\n    type: Literal[MemoryBankType.graph.value] = MemoryBankType.graph.value\n+    max_depth: int = Field(10, gt=0)\n+    relationship_types: List[str] = Field(default_factory=list)\n+    bidirectional: bool = True\n```\n\n\nAlso applies to: 42-45, 47-50\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/cli/model/prompt_format.py",
                "position": 143
            },
            {
                "body": "_:warning: Potential issue_\n\n**Remove TODO comment and consider moving provider_id to a separate class.**\n\nThe \"Hack\" comment indicates technical debt. Consider creating a separate `ProviderDef` class to handle provider-related attributes, following the Single Responsibility Principle.\n\nHere's a suggested implementation:\n\n```diff\n-class CommonDef(BaseModel):\n-    identifier: str  # Must be ASCII-compatible identifier\n-    # Hack: move this out later\n-    provider_id: str = \"\"\n\n+class ProviderDef(BaseModel):\n+    provider_id: str = \"\"\n+\n+class CommonDef(ProviderDef):\n+    identifier: str  # Must be ASCII-compatible identifier\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\nclass ProviderDef(BaseModel):\n    provider_id: str = \"\"\n\nclass CommonDef(ProviderDef):\n    identifier: str  # Must be ASCII-compatible identifier\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/cli/model/prompt_format.py",
                "position": 129
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider adding explicit provider validation.**\n\nInstead of using a simple type alias, consider creating a proper class for `MemoryBankDefWithProvider` to enforce provider-specific validation rules.\n\nHere's a suggested implementation:\n\n```diff\n-MemoryBankDefWithProvider = MemoryBankDef\n+class MemoryBankDefWithProvider(BaseModel):\n+    memory_bank: MemoryBankDef\n+    provider_metadata: Optional[Dict[str, Any]] = None\n+\n+    @model_validator(mode='after')\n+    def validate_provider(self) -> 'MemoryBankDefWithProvider':\n+        if not self.memory_bank.provider_id:\n+            raise ValueError(\"provider_id is required\")\n+        return self\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/cli/model/prompt_format.py",
                "position": 165
            },
            {
                "body": "_:warning: Potential issue_\n\n**Add validation for embedding model name and chunk sizes.**\n\nThe `VectorMemoryBankDef` should include:\n1. Validation for ASCII-only embedding model names\n2. Minimum/maximum bounds for chunk sizes\n3. Validation to ensure overlap_size is less than chunk_size\n\nHere's a suggested implementation with proper validation:\n\n```diff\n@json_schema_type\nclass VectorMemoryBankDef(CommonDef):\n    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n-    embedding_model: str  # Model name in ASCII format\n-    chunk_size_in_tokens: int\n-    overlap_size_in_tokens: Optional[int] = None\n+    embedding_model: str = Field(\n+        ...,\n+        pattern=\"^[\\x00-\\x7F]+$\",\n+        description=\"Model name in ASCII format\"\n+    )\n+    chunk_size_in_tokens: int = Field(\n+        ...,\n+        gt=0,\n+        le=2048,\n+        description=\"Size of each chunk in tokens\"\n+    )\n+    overlap_size_in_tokens: Optional[int] = Field(\n+        None,\n+        ge=0,\n+        description=\"Size of overlap between chunks\"\n+    )\n+\n+    @model_validator(mode='after')\n+    def validate_overlap_size(self) -> 'VectorMemoryBankDef':\n+        if (self.overlap_size_in_tokens is not None and\n+            self.overlap_size_in_tokens >= self.chunk_size_in_tokens):\n+            raise ValueError(\n+                \"overlap_size_in_tokens must be less than chunk_size_in_tokens\"\n+            )\n+        return self\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n@json_schema_type\nclass VectorMemoryBankDef(CommonDef):\n    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n    embedding_model: str = Field(\n        ...,\n        pattern=\"^[\\x00-\\x7F]+$\",\n        description=\"Model name in ASCII format\"\n    )\n    chunk_size_in_tokens: int = Field(\n        ...,\n        gt=0,\n        le=2048,\n        description=\"Size of each chunk in tokens\"\n    )\n    overlap_size_in_tokens: Optional[int] = Field(\n        None,\n        ge=0,\n        description=\"Size of overlap between chunks\"\n    )\n\n    @model_validator(mode='after')\n    def validate_overlap_size(self) -> 'VectorMemoryBankDef':\n        if (self.overlap_size_in_tokens is not None and\n            self.overlap_size_in_tokens >= self.chunk_size_in_tokens):\n            raise ValueError(\n                \"overlap_size_in_tokens must be less than chunk_size_in_tokens\"\n            )\n        return self\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/cli/model/prompt_format.py",
                "position": 137
            },
            {
                "body": "_:warning: Potential issue_\n\n**Add error handling and validation to the Protocol methods.**\n\nThe `MemoryBanks` protocol should include:\n1. Error response types for each method\n2. Input validation for the identifier\n3. Documentation for the API endpoints\n\nHere's a suggested implementation:\n\n```diff\n@runtime_checkable\nclass MemoryBanks(Protocol):\n+    class MemoryBankError(BaseModel):\n+        error: str\n+        details: Optional[Dict[str, Any]] = None\n\n    @webmethod(route=\"/memory_banks/list\", method=\"GET\")\n-    async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]: ...\n+    async def list_memory_banks(\n+        self\n+    ) -> Union[List[MemoryBankDefWithProvider], MemoryBankError]:\n+        \"\"\"List all registered memory banks.\n+\n+        Returns:\n+            List of memory bank definitions or error details\n+        \"\"\"\n+        ...\n\n    @webmethod(route=\"/memory_banks/get\", method=\"GET\")\n    async def get_memory_bank(\n-        self, identifier: str\n-    ) -> Optional[MemoryBankDefWithProvider]: ...\n+        self,\n+        identifier: str = Field(..., min_length=1, max_length=255)\n+    ) -> Union[MemoryBankDefWithProvider, MemoryBankError]:\n+        \"\"\"Retrieve a specific memory bank by identifier.\n+\n+        Args:\n+            identifier: Unique identifier of the memory bank\n+\n+        Returns:\n+            Memory bank definition or error details\n+        \"\"\"\n+        ...\n\n    @webmethod(route=\"/memory_banks/register\", method=\"POST\")\n    async def register_memory_bank(\n-        self, memory_bank: MemoryBankDefWithProvider\n-    ) -> None: ...\n+        self,\n+        memory_bank: MemoryBankDefWithProvider\n+    ) -> Union[None, MemoryBankError]:\n+        \"\"\"Register a new memory bank.\n+\n+        Args:\n+            memory_bank: Memory bank definition with provider details\n+\n+        Returns:\n+            None on success or error details on failure\n+        \"\"\"\n+        ...\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n@runtime_checkable\nclass MemoryBanks(Protocol):\n    class MemoryBankError(BaseModel):\n        error: str\n        details: Optional[Dict[str, Any]] = None\n\n    @webmethod(route=\"/memory_banks/list\", method=\"GET\")\n    async def list_memory_banks(\n        self\n    ) -> Union[List[MemoryBankDefWithProvider], MemoryBankError]:\n        \"\"\"List all registered memory banks.\n\n        Returns:\n            List of memory bank definitions or error details\n        \"\"\"\n        ...\n\n    @webmethod(route=\"/memory_banks/get\", method=\"GET\")\n    async def get_memory_bank(\n        self,\n        identifier: str = Field(..., min_length=1, max_length=255)\n    ) -> Union[MemoryBankDefWithProvider, MemoryBankError]:\n        \"\"\"Retrieve a specific memory bank by identifier.\n\n        Args:\n            identifier: Unique identifier of the memory bank\n\n        Returns:\n            Memory bank definition or error details\n        \"\"\"\n        ...\n\n    @webmethod(route=\"/memory_banks/register\", method=\"POST\")\n    async def register_memory_bank(\n        self,\n        memory_bank: MemoryBankDefWithProvider\n    ) -> Union[None, MemoryBankError]:\n        \"\"\"Register a new memory bank.\n\n        Args:\n            memory_bank: Memory bank definition with provider details\n\n        Returns:\n            None on success or error details on failure\n        \"\"\"\n        ...\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/cli/model/prompt_format.py",
                "position": 181
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/cli/model/prompt_format.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n \n-import argparse\n-import textwrap\n-from io import StringIO\n-\n-from llama_models.datatypes import CoreModelId, is_multimodal, model_family, ModelFamily\n-\n-from llama_stack.cli.subcommand import Subcommand\n-\n-\n-class ModelPromptFormat(Subcommand):\n-    \"\"\"Llama model cli for describe a model prompt format (message formats)\"\"\"\n-\n-    def __init__(self, subparsers: argparse._SubParsersAction):\n-        super().__init__()\n-        self.parser = subparsers.add_parser(\n-            \"prompt-format\",\n-            prog=\"llama model prompt-format\",\n-            description=\"Show llama model message formats\",\n-            epilog=textwrap.dedent(\n-                \"\"\"\n-                Example:\n-                    llama model prompt-format <options>\n-                \"\"\"\n-            ),\n-            formatter_class=argparse.RawTextHelpFormatter,\n-        )\n-        self._add_arguments()\n-        self.parser.set_defaults(func=self._run_model_template_cmd)\n-\n-    def _add_arguments(self):\n-        self.parser.add_argument(\n-            \"-m\",\n-            \"--model-name\",\n-            type=str,\n-            default=\"llama3_1\",\n-            help=\"Model Family (llama3_1, llama3_X, etc.)\",\n-        )\n-\n-    def _run_model_template_cmd(self, args: argparse.Namespace) -> None:\n-        import pkg_resources\n-\n-        # Only Llama 3.1 and 3.2 are supported\n-        supported_model_ids = [\n-            m\n-            for m in CoreModelId\n-            if model_family(m) in {ModelFamily.llama3_1, ModelFamily.llama3_2}\n-        ]\n-        model_str = \"\\n\".join([m.value for m in supported_model_ids])\n-        try:\n-            model_id = CoreModelId(args.model_name)\n-        except ValueError:\n-            self.parser.error(\n-                f\"{args.model_name} is not a valid Model. Choose one from --\\n{model_str}\"\n-            )\n-\n-        if model_id not in supported_model_ids:\n-            self.parser.error(\n-                f\"{model_id} is not a valid Model. Choose one from --\\n {model_str}\"\n-            )\n-\n-        llama_3_1_file = pkg_resources.resource_filename(\n-            \"llama_models\", \"llama3_1/prompt_format.md\"\n-        )\n-        llama_3_2_text_file = pkg_resources.resource_filename(\n-            \"llama_models\", \"llama3_2/text_prompt_format.md\"\n-        )\n-        llama_3_2_vision_file = pkg_resources.resource_filename(\n-            \"llama_models\", \"llama3_2/vision_prompt_format.md\"\n-        )\n-        if model_family(model_id) == ModelFamily.llama3_1:\n-            with open(llama_3_1_file, \"r\") as f:\n-                content = f.read()\n-        elif model_family(model_id) == ModelFamily.llama3_2:\n-            if is_multimodal(model_id):\n-                with open(llama_3_2_vision_file, \"r\") as f:\n-                    content = f.read()\n-            else:\n-                with open(llama_3_2_text_file, \"r\") as f:\n-                    content = f.read()\n-\n-        render_markdown_to_pager(content)\n-\n-\n-def render_markdown_to_pager(markdown_content: str):\n-    from rich.console import Console\n-    from rich.markdown import Markdown\n-    from rich.style import Style\n-    from rich.text import Text\n-\n-    class LeftAlignedHeaderMarkdown(Markdown):\n-        def parse_header(self, token):\n-            level = token.type.count(\"h\")\n-            content = Text(token.content)\n-            header_style = Style(color=\"bright_blue\", bold=True)\n-            header = Text(f\"{'#' * level} \", style=header_style) + content\n-            self.add_text(header)\n-\n-    # Render the Markdown\n-    md = LeftAlignedHeaderMarkdown(markdown_content)\n-\n-    # Capture the rendered output\n-    output = StringIO()\n-    console = Console(file=output, force_terminal=True, width=100)  # Set a fixed width\n-    console.print(md)\n-    rendered_content = output.getvalue()\n-    print(rendered_content)\n+from enum import Enum\n+from typing import List, Literal, Optional, Protocol, runtime_checkable, Union\n+\n+from llama_models.schema_utils import json_schema_type, webmethod\n+from pydantic import BaseModel, Field\n+from typing_extensions import Annotated\n+\n+\n+@json_schema_type\n+class MemoryBankType(Enum):\n+    vector = \"vector\"\n+    keyvalue = \"keyvalue\"\n+    keyword = \"keyword\"\n+    graph = \"graph\"\n+\n+\n+class CommonDef(BaseModel):\n+    identifier: str  # Must be ASCII-compatible identifier\n+    # Hack: move this out later\n+    provider_id: str = \"\"\n+\n+\n+@json_schema_type\n+class VectorMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n+    embedding_model: str  # Model name in ASCII format\n+    chunk_size_in_tokens: int\n+    overlap_size_in_tokens: Optional[int] = None\n+\n+\n+@json_schema_type\n+class KeyValueMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.keyvalue.value] = MemoryBankType.keyvalue.value\n+\n+\n+@json_schema_type\n+class KeywordMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.keyword.value] = MemoryBankType.keyword.value\n+\n+\n+@json_schema_type\n+class GraphMemoryBankDef(CommonDef):\n+    type: Literal[MemoryBankType.graph.value] = MemoryBankType.graph.value\n+\n+\n+MemoryBankDef = Annotated[\n+    Union[\n+        VectorMemoryBankDef,\n+        KeyValueMemoryBankDef,\n+        KeywordMemoryBankDef,\n+        GraphMemoryBankDef,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+MemoryBankDefWithProvider = MemoryBankDef\n+\n+\n+@runtime_checkable\n+class MemoryBanks(Protocol):\n+    @webmethod(route=\"/memory_banks/list\", method=\"GET\")\n+    async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]: ...\n+\n+    @webmethod(route=\"/memory_banks/get\", method=\"GET\")\n+    async def get_memory_bank(\n+        self, identifier: str\n+    ) -> Optional[MemoryBankDefWithProvider]: ...\n+\n+    @webmethod(route=\"/memory_banks/register\", method=\"POST\")\n+    async def register_memory_bank(\n+        self, memory_bank: MemoryBankDefWithProvider\n+    ) -> None: ...\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "68",
        "type": "4",
        "number": 117,
        "head_branch": "error-041-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes introduced in the pull request primarily involve the `memory_banks.py` file, specifically within the `CommonDef` and `VectorMemoryBankDef` classes. The `identifier` field in the `CommonDef` class has been updated to include a comment indicating that it must be an ASCII-compatible identifier. Similarly, the `embedding_model` field in the `VectorMemoryBankDef` class has been modified to add a comment specifying that the model name should also be in ASCII format. \n\nThese modifications are limited to the addition of comments for clarity and do not affect the existing logic, control flow, or error handling within the methods of the `MemoryBanks` protocol. The overall structure and relationships of the classes remain unchanged. The changes serve to enhance the documentation of the code by clearly indicating the expected format for specific string fields, but no functional alterations have been made to the code itself.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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=\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Add consistent ASCII validation for model name.**\n\nSimilar to the identifier field, the ASCII requirement for embedding_model should be enforced through validation.\n\n```diff\n-    embedding_model: str  # Model name in ASCII format\n+    embedding_model: Annotated[\n+        str,\n+        Field(\n+            pattern=\"^[\\x00-\\x7F]+$\",\n+            description=\"Model name in ASCII format\"\n+        )\n+    ]\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    embedding_model: Annotated[\n        str,\n        Field(\n            pattern=\"^[\\x00-\\x7F]+$\",\n            description=\"Model name in ASCII format\"\n        )\n    ]\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/memory_banks/memory_banks.py",
                "position": 14
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Add Pydantic validation for ASCII identifier requirement.**\n\nWhile the comment specifies the ASCII requirement, it's not enforced. Consider adding Pydantic validation to ensure the identifier is ASCII-compatible at runtime.\n\n```diff\n-    identifier: str  # Must be ASCII-compatible identifier\n+    identifier: Annotated[\n+        str,\n+        Field(\n+            pattern=\"^[\\x00-\\x7F]+$\",\n+            description=\"Must be ASCII-compatible identifier\"\n+        )\n+    ]\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/apis/memory_banks/memory_banks.py",
                "position": 5
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/apis/memory_banks/memory_banks.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " \n \n class CommonDef(BaseModel):\n-    identifier: str\n+    identifier: str  # Must be ASCII-compatible identifier\n     # Hack: move this out later\n     provider_id: str = \"\"\n \n"
                    },
                    {
                        "id": 2,
                        "content": " @json_schema_type\n class VectorMemoryBankDef(CommonDef):\n     type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n-    embedding_model: str\n+    embedding_model: str  # Model name in ASCII format\n     chunk_size_in_tokens: int\n     overlap_size_in_tokens: Optional[int] = None\n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "70",
        "type": "4",
        "number": 119,
        "head_branch": "error-040-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the `together.py` file involve a comprehensive restructuring aimed at enhancing the modularity and functionality of the codebase. Numerous new classes and enums have been introduced, including `Attachment`, `AgentTool`, and various tool definitions such as `SearchToolDefinition` and `MemoryToolDefinition`, which extend the `ToolDefinitionCommon` base class. These additions leverage Pydantic's `BaseModel` for improved data validation and serialization.\n\nThe implementation also introduces new enums to categorize tools and configurations, such as `SearchEngineType` and `MemoryQueryGenerator`. Additionally, the `StepCommon` class and its derivatives define different operational steps like `InferenceStep` and `ToolExecutionStep`, establishing a clear structure for system operations.\n\nThe `Turn` and `Session` classes encapsulate agent interactions, while the `AgentConfig` and related classes manage agent configurations, including sampling parameters and session management. The `Agents` protocol has been expanded to include web methods for agent management. Notably, previous safety-related functionalities have been removed, indicating a significant shift in the integration of safety models. Overall, the changes reflect a move towards a more structured and extensible architecture for agent interactions.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Add validation for turn_ids parameter.**\n\nThe `turn_ids` parameter in `get_agents_session` should have validation to ensure it's not empty when provided.\n\n```diff\n-    turn_ids: Optional[List[str]] = None,\n+    turn_ids: Optional[List[str]] = Field(\n+        default=None,\n+        description=\"List of turn IDs to retrieve. If None, retrieves all turns\",\n+        min_items=1\n+    ),\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n        turn_ids: Optional[List[str]] = Field(\n            default=None,\n            description=\"List of turn IDs to retrieve. If None, retrieves all turns\",\n            min_items=1\n        ),\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "position": 488
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Validate max_infer_iters value.**\n\nThe `max_infer_iters` should have validation to ensure it's a positive integer.\n\n```diff\n-    max_infer_iters: int = 10\n+    max_infer_iters: int = Field(\n+        default=10,\n+        description=\"Maximum number of inference iterations\",\n+        gt=0\n+    )\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    max_infer_iters: int = Field(\n        default=10,\n        description=\"Maximum number of inference iterations\",\n        gt=0\n    )\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "position": 311
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider making memory limits configurable.**\n\nThe hardcoded values for `max_tokens_in_context` and `max_chunks` should ideally be configurable through environment variables or configuration files.\n\n```diff\n-    max_tokens_in_context: int = 4096\n-    max_chunks: int = 10\n+    max_tokens_in_context: int = Field(\n+        default=4096,\n+        description=\"Maximum number of tokens to include in context\",\n+        gt=0\n+    )\n+    max_chunks: int = Field(\n+        default=10,\n+        description=\"Maximum number of memory chunks to retrieve\",\n+        gt=0\n+    )\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    max_tokens_in_context: int = Field(\n        default=4096,\n        description=\"Maximum number of tokens to include in context\",\n        gt=0\n    )\n    max_chunks: int = Field(\n        default=10,\n        description=\"Maximum number of memory chunks to retrieve\",\n        gt=0\n    )\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "position": 174
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider adding validation for shield lists.**\n\nThe `input_shields` and `output_shields` lists could benefit from validation to ensure they contain valid shield identifiers.\n\n```diff\n class ToolDefinitionCommon(BaseModel):\n-    input_shields: Optional[List[str]] = Field(default_factory=list)\n-    output_shields: Optional[List[str]] = Field(default_factory=list)\n+    input_shields: Optional[List[str]] = Field(\n+        default_factory=list,\n+        description=\"List of input shield identifiers\",\n+        min_items=0\n+    )\n+    output_shields: Optional[List[str]] = Field(\n+        default_factory=list,\n+        description=\"List of output shield identifiers\",\n+        min_items=0\n+    )\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\nclass ToolDefinitionCommon(BaseModel):\n    input_shields: Optional[List[str]] = Field(\n        default_factory=list,\n        description=\"List of input shield identifiers\",\n        min_items=0\n    )\n    output_shields: Optional[List[str]] = Field(\n        default_factory=list,\n        description=\"List of output shield identifiers\",\n        min_items=0\n    )\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "position": 51
            },
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider adding validation for tool_choice and tool_prompt_format.**\n\nThe `tool_choice` and `tool_prompt_format` fields should have validation to ensure they contain valid values.\n\n```diff\n     sampling_params: Optional[SamplingParams] = SamplingParams()\n \n     input_shields: Optional[List[str]] = Field(default_factory=list)\n     output_shields: Optional[List[str]] = Field(default_factory=list)\n \n     tools: Optional[List[AgentToolDefinition]] = Field(default_factory=list)\n-    tool_choice: Optional[ToolChoice] = Field(default=ToolChoice.auto)\n+    tool_choice: Optional[ToolChoice] = Field(\n+        default=ToolChoice.auto,\n+        description=\"Controls how tools are selected\"\n+    )\n-    tool_prompt_format: Optional[ToolPromptFormat] = Field(\n-        default=ToolPromptFormat.json\n-    )\n+    tool_prompt_format: Optional[ToolPromptFormat] = Field(\n+        default=ToolPromptFormat.json,\n+        description=\"Format used for tool prompts\"\n+    )\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    sampling_params: Optional[SamplingParams] = SamplingParams()\n\n    input_shields: Optional[List[str]] = Field(default_factory=list)\n    output_shields: Optional[List[str]] = Field(default_factory=list)\n\n    tools: Optional[List[AgentToolDefinition]] = Field(default_factory=list)\n    tool_choice: Optional[ToolChoice] = Field(\n        default=ToolChoice.auto,\n        description=\"Controls how tools are selected\"\n    )\n    tool_prompt_format: Optional[ToolPromptFormat] = Field(\n        default=ToolPromptFormat.json,\n        description=\"Format used for tool prompts\"\n    )\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "position": 309
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/safety/together/together.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " #\n # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n-from together import Together\n+\n+from datetime import datetime\n+from enum import Enum\n+from typing import (\n+    Any,\n+    Dict,\n+    List,\n+    Literal,\n+    Optional,\n+    Protocol,\n+    runtime_checkable,\n+    Union,\n+)\n+\n+from llama_models.schema_utils import json_schema_type, webmethod\n+\n+from pydantic import BaseModel, ConfigDict, Field\n+from typing_extensions import Annotated\n \n from llama_models.llama3.api.datatypes import *  # noqa: F403\n+from llama_stack.apis.common.deployment_types import *  # noqa: F403\n+from llama_stack.apis.inference import *  # noqa: F403\n from llama_stack.apis.safety import *  # noqa: F403\n-from llama_stack.distribution.request_headers import NeedsRequestProviderData\n-from llama_stack.providers.datatypes import ShieldsProtocolPrivate\n+from llama_stack.apis.memory import *  # noqa: F403\n+\n+\n+@json_schema_type\n+class Attachment(BaseModel):\n+    content: InterleavedTextMedia | URL\n+    mime_type: str\n+\n+\n+class AgentTool(Enum):\n+    brave_search = \"brave_search\"\n+    wolfram_alpha = \"wolfram_alpha\"\n+    photogen = \"photogen\"\n+    code_interpreter = \"code_interpreter\"\n+\n+    function_call = \"function_call\"\n+    memory = \"memory\"\n+\n+\n+class ToolDefinitionCommon(BaseModel):\n+    input_shields: Optional[List[str]] = Field(default_factory=list)\n+    output_shields: Optional[List[str]] = Field(default_factory=list)\n+\n+\n+class SearchEngineType(Enum):\n+    bing = \"bing\"\n+    brave = \"brave\"\n+\n+\n+@json_schema_type\n+class SearchToolDefinition(ToolDefinitionCommon):\n+    # NOTE: brave_search is just a placeholder since model always uses\n+    # brave_search as tool call name\n+    type: Literal[AgentTool.brave_search.value] = AgentTool.brave_search.value\n+    api_key: str\n+    engine: SearchEngineType = SearchEngineType.brave\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+@json_schema_type\n+class WolframAlphaToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.wolfram_alpha.value] = AgentTool.wolfram_alpha.value\n+    api_key: str\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+@json_schema_type\n+class PhotogenToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.photogen.value] = AgentTool.photogen.value\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+@json_schema_type\n+class CodeInterpreterToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.code_interpreter.value] = AgentTool.code_interpreter.value\n+    enable_inline_code_execution: bool = True\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+@json_schema_type\n+class FunctionCallToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.function_call.value] = AgentTool.function_call.value\n+    function_name: str\n+    description: str\n+    parameters: Dict[str, ToolParamDefinition]\n+    remote_execution: Optional[RestAPIExecutionConfig] = None\n+\n+\n+class _MemoryBankConfigCommon(BaseModel):\n+    bank_id: str\n+\n+\n+class AgentVectorMemoryBankConfig(_MemoryBankConfigCommon):\n+    type: Literal[MemoryBankType.vector.value] = MemoryBankType.vector.value\n+\n+\n+class AgentKeyValueMemoryBankConfig(_MemoryBankConfigCommon):\n+    type: Literal[MemoryBankType.keyvalue.value] = MemoryBankType.keyvalue.value\n+    keys: List[str]  # what keys to focus on\n+\n+\n+class AgentKeywordMemoryBankConfig(_MemoryBankConfigCommon):\n+    type: Literal[MemoryBankType.keyword.value] = MemoryBankType.keyword.value\n+\n+\n+class AgentGraphMemoryBankConfig(_MemoryBankConfigCommon):\n+    type: Literal[MemoryBankType.graph.value] = MemoryBankType.graph.value\n+    entities: List[str]  # what entities to focus on\n+\n+\n+MemoryBankConfig = Annotated[\n+    Union[\n+        AgentVectorMemoryBankConfig,\n+        AgentKeyValueMemoryBankConfig,\n+        AgentKeywordMemoryBankConfig,\n+        AgentGraphMemoryBankConfig,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+\n+class MemoryQueryGenerator(Enum):\n+    default = \"default\"\n+    llm = \"llm\"\n+    custom = \"custom\"\n+\n+\n+class DefaultMemoryQueryGeneratorConfig(BaseModel):\n+    type: Literal[MemoryQueryGenerator.default.value] = (\n+        MemoryQueryGenerator.default.value\n+    )\n+    sep: str = \" \"\n+\n+\n+class LLMMemoryQueryGeneratorConfig(BaseModel):\n+    type: Literal[MemoryQueryGenerator.llm.value] = MemoryQueryGenerator.llm.value\n+    model: str\n+    template: str\n+\n+\n+class CustomMemoryQueryGeneratorConfig(BaseModel):\n+    type: Literal[MemoryQueryGenerator.custom.value] = MemoryQueryGenerator.custom.value\n+\n+\n+MemoryQueryGeneratorConfig = Annotated[\n+    Union[\n+        DefaultMemoryQueryGeneratorConfig,\n+        LLMMemoryQueryGeneratorConfig,\n+        CustomMemoryQueryGeneratorConfig,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+\n+@json_schema_type\n+class MemoryToolDefinition(ToolDefinitionCommon):\n+    type: Literal[AgentTool.memory.value] = AgentTool.memory.value\n+    memory_bank_configs: List[MemoryBankConfig] = Field(default_factory=list)\n+    # This config defines how a query is generated using the messages\n+    # for memory bank retrieval.\n+    query_generator_config: MemoryQueryGeneratorConfig = Field(\n+        default=DefaultMemoryQueryGeneratorConfig()\n+    )\n+    max_tokens_in_context: int = 4096\n+    max_chunks: int = 10\n+\n+\n+AgentToolDefinition = Annotated[\n+    Union[\n+        SearchToolDefinition,\n+        WolframAlphaToolDefinition,\n+        PhotogenToolDefinition,\n+        CodeInterpreterToolDefinition,\n+        FunctionCallToolDefinition,\n+        MemoryToolDefinition,\n+    ],\n+    Field(discriminator=\"type\"),\n+]\n+\n+\n+class StepCommon(BaseModel):\n+    turn_id: str\n+    step_id: str\n+    started_at: Optional[datetime] = None\n+    completed_at: Optional[datetime] = None\n+\n+\n+class StepType(Enum):\n+    inference = \"inference\"\n+    tool_execution = \"tool_execution\"\n+    shield_call = \"shield_call\"\n+    memory_retrieval = \"memory_retrieval\"\n+\n+\n+@json_schema_type\n+class InferenceStep(StepCommon):\n+    model_config = ConfigDict(protected_namespaces=())\n+\n+    step_type: Literal[StepType.inference.value] = StepType.inference.value\n+    model_response: CompletionMessage\n \n-from .config import TogetherSafetyConfig\n \n+@json_schema_type\n+class ToolExecutionStep(StepCommon):\n+    step_type: Literal[StepType.tool_execution.value] = StepType.tool_execution.value\n+    tool_calls: List[ToolCall]\n+    tool_responses: List[ToolResponse]\n \n-TOGETHER_SHIELD_MODEL_MAP = {\n-    \"llama_guard\": \"meta-llama/Meta-Llama-Guard-3-8B\",\n-    \"Llama-Guard-3-8B\": \"meta-llama/Meta-Llama-Guard-3-8B\",\n-    \"Llama-Guard-3-11B-Vision\": \"meta-llama/Llama-Guard-3-11B-Vision-Turbo\",\n-}\n \n+@json_schema_type\n+class ShieldCallStep(StepCommon):\n+    step_type: Literal[StepType.shield_call.value] = StepType.shield_call.value\n+    violation: Optional[SafetyViolation]\n \n-class TogetherSafetyImpl(Safety, NeedsRequestProviderData, ShieldsProtocolPrivate):\n-    def __init__(self, config: TogetherSafetyConfig) -> None:\n-        self.config = config\n \n-    async def initialize(self) -> None:\n-        pass\n+@json_schema_type\n+class MemoryRetrievalStep(StepCommon):\n+    step_type: Literal[StepType.memory_retrieval.value] = (\n+        StepType.memory_retrieval.value\n+    )\n+    memory_bank_ids: List[str]\n+    inserted_context: InterleavedTextMedia\n \n-    async def shutdown(self) -> None:\n-        pass\n \n-    async def register_shield(self, shield: ShieldDef) -> None:\n-        raise ValueError(\"Registering dynamic shields is not supported\")\n+Step = Annotated[\n+    Union[\n+        InferenceStep,\n+        ToolExecutionStep,\n+        ShieldCallStep,\n+        MemoryRetrievalStep,\n+    ],\n+    Field(discriminator=\"step_type\"),\n+]\n \n-    async def list_shields(self) -> List[ShieldDef]:\n-        return [\n-            ShieldDef(\n-                identifier=ShieldType.llama_guard.value,\n-                type=ShieldType.llama_guard.value,\n-                params={},\n-            )\n+\n+@json_schema_type\n+class Turn(BaseModel):\n+    \"\"\"A single turn in an interaction with an Agentic System.\"\"\"\n+\n+    turn_id: str\n+    session_id: str\n+    input_messages: List[\n+        Union[\n+            UserMessage,\n+            ToolResponseMessage,\n         ]\n+    ]\n+    steps: List[Step]\n+    output_message: CompletionMessage\n+    output_attachments: List[Attachment] = Field(default_factory=list)\n+\n+    started_at: datetime\n+    completed_at: Optional[datetime] = None\n+\n+\n+@json_schema_type\n+class Session(BaseModel):\n+    \"\"\"A single session of an interaction with an Agentic System.\"\"\"\n+\n+    session_id: str\n+    session_name: str\n+    turns: List[Turn]\n+    started_at: datetime\n+\n+    memory_bank: Optional[MemoryBankDef] = None\n+\n+\n+class AgentConfigCommon(BaseModel):\n+    sampling_params: Optional[SamplingParams] = SamplingParams()\n+\n+    input_shields: Optional[List[str]] = Field(default_factory=list)\n+    output_shields: Optional[List[str]] = Field(default_factory=list)\n+\n+    tools: Optional[List[AgentToolDefinition]] = Field(default_factory=list)\n+    tool_choice: Optional[ToolChoice] = Field(default=ToolChoice.auto)\n+    tool_prompt_format: Optional[ToolPromptFormat] = Field(\n+        default=ToolPromptFormat.json\n+    )\n+\n+    max_infer_iters: int = 10\n+\n+\n+@json_schema_type\n+class AgentConfig(AgentConfigCommon):\n+    model: str\n+    instructions: str\n+    enable_session_persistence: bool\n+\n+\n+class AgentConfigOverridablePerTurn(AgentConfigCommon):\n+    instructions: Optional[str] = None\n+\n+\n+class AgentTurnResponseEventType(Enum):\n+    step_start = \"step_start\"\n+    step_complete = \"step_complete\"\n+    step_progress = \"step_progress\"\n+\n+    turn_start = \"turn_start\"\n+    turn_complete = \"turn_complete\"\n+\n+\n+@json_schema_type\n+class AgentTurnResponseStepStartPayload(BaseModel):\n+    event_type: Literal[AgentTurnResponseEventType.step_start.value] = (\n+        AgentTurnResponseEventType.step_start.value\n+    )\n+    step_type: StepType\n+    step_id: str\n+    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)\n+\n+\n+@json_schema_type\n+class AgentTurnResponseStepCompletePayload(BaseModel):\n+    event_type: Literal[AgentTurnResponseEventType.step_complete.value] = (\n+        AgentTurnResponseEventType.step_complete.value\n+    )\n+    step_type: StepType\n+    step_details: Step\n+\n+\n+@json_schema_type\n+class AgentTurnResponseStepProgressPayload(BaseModel):\n+    model_config = ConfigDict(protected_namespaces=())\n+\n+    event_type: Literal[AgentTurnResponseEventType.step_progress.value] = (\n+        AgentTurnResponseEventType.step_progress.value\n+    )\n+    step_type: StepType\n+    step_id: str\n+\n+    model_response_text_delta: Optional[str] = None\n+    tool_call_delta: Optional[ToolCallDelta] = None\n+    tool_response_text_delta: Optional[str] = None\n+\n+\n+@json_schema_type\n+class AgentTurnResponseTurnStartPayload(BaseModel):\n+    event_type: Literal[AgentTurnResponseEventType.turn_start.value] = (\n+        AgentTurnResponseEventType.turn_start.value\n+    )\n+    turn_id: str\n+\n+\n+@json_schema_type\n+class AgentTurnResponseTurnCompletePayload(BaseModel):\n+    event_type: Literal[AgentTurnResponseEventType.turn_complete.value] = (\n+        AgentTurnResponseEventType.turn_complete.value\n+    )\n+    turn: Turn\n+\n+\n+@json_schema_type\n+class AgentTurnResponseEvent(BaseModel):\n+    \"\"\"Streamed agent execution response.\"\"\"\n+\n+    payload: Annotated[\n+        Union[\n+            AgentTurnResponseStepStartPayload,\n+            AgentTurnResponseStepProgressPayload,\n+            AgentTurnResponseStepCompletePayload,\n+            AgentTurnResponseTurnStartPayload,\n+            AgentTurnResponseTurnCompletePayload,\n+        ],\n+        Field(discriminator=\"event_type\"),\n+    ]\n+\n+\n+@json_schema_type\n+class AgentCreateResponse(BaseModel):\n+    agent_id: str\n+\n+\n+@json_schema_type\n+class AgentSessionCreateResponse(BaseModel):\n+    session_id: str\n+\n+\n+@json_schema_type\n+class AgentTurnCreateRequest(AgentConfigOverridablePerTurn):\n+    agent_id: str\n+    session_id: str\n+\n+    # TODO: figure out how we can simplify this and make why\n+    # ToolResponseMessage needs to be here (it is function call\n+    # execution from outside the system)\n+    messages: List[\n+        Union[\n+            UserMessage,\n+            ToolResponseMessage,\n+        ]\n+    ]\n+    attachments: Optional[List[Attachment]] = None\n+\n+    stream: Optional[bool] = False\n+\n+\n+@json_schema_type\n+class AgentTurnResponseStreamChunk(BaseModel):\n+    event: AgentTurnResponseEvent\n+\n+\n+@json_schema_type\n+class AgentStepResponse(BaseModel):\n+    step: Step\n+\n+\n+@runtime_checkable\n+class Agents(Protocol):\n+    @webmethod(route=\"/agents/create\")\n+    async def create_agent(\n+        self,\n+        agent_config: AgentConfig,\n+    ) -> AgentCreateResponse: ...\n+\n+    # This method is not `async def` because it can result in either an\n+    # `AsyncGenerator` or a `AgentTurnCreateResponse` depending on the value of `stream`.\n+    @webmethod(route=\"/agents/turn/create\")\n+    def create_agent_turn(\n+        self,\n+        agent_id: str,\n+        session_id: str,\n+        messages: List[\n+            Union[\n+                UserMessage,\n+                ToolResponseMessage,\n+            ]\n+        ],\n+        attachments: Optional[List[Attachment]] = None,\n+        stream: Optional[bool] = False,\n+    ) -> AgentTurnResponseStreamChunk: ...\n+\n+    @webmethod(route=\"/agents/turn/get\")\n+    async def get_agents_turn(\n+        self,\n+        agent_id: str,\n+        turn_id: str,\n+    ) -> Turn: ...\n+\n+    @webmethod(route=\"/agents/step/get\")\n+    async def get_agents_step(\n+        self, agent_id: str, turn_id: str, step_id: str\n+    ) -> AgentStepResponse: ...\n+\n+    @webmethod(route=\"/agents/session/create\")\n+    async def create_agent_session(\n+        self,\n+        agent_id: str,\n+        session_name: str,\n+    ) -> AgentSessionCreateResponse: ...\n+\n+    @webmethod(route=\"/agents/session/get\")\n+    async def get_agents_session(\n+        self,\n+        agent_id: str,\n+        session_id: str,\n+        turn_ids: Optional[List[str]] = None,\n+    ) -> Session: ...\n+\n+    @webmethod(route=\"/agents/session/delete\")\n+    async def delete_agents_session(self, agent_id: str, session_id: str) -> None: ...\n \n-    async def run_shield(\n-        self, shield_type: str, messages: List[Message], params: Dict[str, Any] = None\n-    ) -> RunShieldResponse:\n-        shield_def = await self.shield_store.get_shield(shield_type)\n-        if not shield_def:\n-            raise ValueError(f\"Unknown shield {shield_type}\")\n-\n-        model = shield_def.params.get(\"model\", \"llama_guard\")\n-        if model not in TOGETHER_SHIELD_MODEL_MAP:\n-            raise ValueError(f\"Unsupported safety model: {model}\")\n-\n-        together_api_key = None\n-        if self.config.api_key is not None:\n-            together_api_key = self.config.api_key\n-        else:\n-            provider_data = self.get_request_provider_data()\n-            if provider_data is None or not provider_data.together_api_key:\n-                raise ValueError(\n-                    'Pass Together API Key in the header X-LlamaStack-ProviderData as { \"together_api_key\": <your api key>}'\n-                )\n-            together_api_key = provider_data.together_api_key\n-\n-        # messages can have role assistant or user\n-        api_messages = []\n-        for message in messages:\n-            if message.role in (Role.user.value, Role.assistant.value):\n-                api_messages.append({\"role\": message.role, \"content\": message.content})\n-\n-        violation = await get_safety_response(\n-            together_api_key, TOGETHER_SHIELD_MODEL_MAP[model], api_messages\n-        )\n-        return RunShieldResponse(violation=violation)\n-\n-\n-async def get_safety_response(\n-    api_key: str, model_name: str, messages: List[Dict[str, str]]\n-) -> Optional[SafetyViolation]:\n-    client = Together(api_key=api_key)\n-    response = client.chat.completions.create(messages=messages, model=model_name)\n-    if len(response.choices) == 0:\n-        return None\n-\n-    response_text = response.choices[0].message.content\n-    if response_text == \"safe\":\n-        return None\n-\n-    parts = response_text.split(\"\\n\")\n-    if len(parts) != 2:\n-        return None\n-\n-    if parts[0] == \"unsafe\":\n-        return SafetyViolation(\n-            violation_level=ViolationLevel.ERROR,\n-            metadata={\"violation_type\": parts[1]},\n-        )\n-\n-    return None\n+    @webmethod(route=\"/agents/delete\")\n+    async def delete_agents(\n+        self,\n+        agent_id: str,\n+    ) -> None: ...\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "71",
        "type": "4",
        "number": 120,
        "head_branch": "error-017-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces substantial changes to the `server.py` file located in the `project/api/llama_stack/distribution/server` directory. Key modifications include the removal of several functions related to error handling and request processing, such as `global_exception_handler`, `translate_exception`, and `passthrough`, which previously managed exceptions and forwarded requests. The `create_dynamic_passthrough` function has also been eliminated.\n\nChanges to the handling of streaming requests have been made, with the `is_streaming_request` function remaining but utilized differently. A new function, `register_object_with_provider`, has been added to facilitate the registration of objects with their respective providers based on API type.\n\nA new class, `CommonRoutingTableImpl`, has been introduced, which implements a routing table for managing a registry of routable objects. This class is extended by three new classes\u2014`ModelsRoutingTable`, `ShieldsRoutingTable`, and `MemoryBanksRoutingTable`\u2014each providing specific functionalities for their respective object types.\n\nAdditionally, the main function has been updated to streamline application startup and shutdown, removing previous signal handling and exception management setups. The overall structure of the server's routing and object management has been refactored for enhanced clarity and maintainability.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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=\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Optimize list operations for better performance.**\n\nThe `list_shields` and `list_memory_banks` methods create a new list and extend it iteratively. This can be optimized using list comprehension or chain.\n\nConsider using itertools.chain for better performance:\n\n```diff\n+from itertools import chain\n\nclass ShieldsRoutingTable(CommonRoutingTableImpl, Shields):\n    async def list_shields(self) -> List[ShieldDef]:\n-        objects = []\n-        for objs in self.registry.values():\n-            objects.extend(objs)\n-        return objects\n+        return list(chain.from_iterable(self.registry.values()))\n\nclass MemoryBanksRoutingTable(CommonRoutingTableImpl, MemoryBanks):\n    async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]:\n-        objects = []\n-        for objs in self.registry.values():\n-            objects.extend(objs)\n-        return objects\n+        return list(chain.from_iterable(self.registry.values()))\n```\n\n\nAlso applies to: 157-161\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 472
            },
            {
                "body": "_:warning: Potential issue_\n\n**Consider adding error handling for missing provider spec.**\n\nThe `get_impl_api` function assumes the provider always has a `__provider_spec__` attribute with an `api` field. This could raise AttributeError if the provider is malformed.\n\nConsider adding error handling:\n\n```diff\ndef get_impl_api(p: Any) -> Api:\n+    if not hasattr(p, '__provider_spec__') or not hasattr(p.__provider_spec__, 'api'):\n+        raise ValueError(\"Invalid provider: missing __provider_spec__ or api attribute\")\n    return p.__provider_spec__.api\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\ndef get_impl_api(p: Any) -> Api:\n    if not hasattr(p, '__provider_spec__') or not hasattr(p.__provider_spec__, 'api'):\n        raise ValueError(\"Invalid provider: missing __provider_spec__ or api attribute\")\n    return p.__provider_spec__.api\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 333
            },
            {
                "body": "_:warning: Potential issue_\n\n**Add type validation for provider implementation.**\n\nThe function assumes the provider implements the correct registration method based on API type but doesn't validate it.\n\nAdd type checking:\n\n```diff\nasync def register_object_with_provider(obj: RoutableObject, p: Any) -> None:\n    api = get_impl_api(p)\n    if api == Api.inference:\n+        if not hasattr(p, 'register_model'):\n+            raise ValueError(\"Provider missing register_model implementation\")\n        await p.register_model(obj)\n    elif api == Api.safety:\n+        if not hasattr(p, 'register_shield'):\n+            raise ValueError(\"Provider missing register_shield implementation\")\n        await p.register_shield(obj)\n    elif api == Api.memory:\n+        if not hasattr(p, 'register_memory_bank'):\n+            raise ValueError(\"Provider missing register_memory_bank implementation\")\n        await p.register_memory_bank(obj)\n+    else:\n+        raise ValueError(f\"Unsupported API type: {api}\")\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\nasync def register_object_with_provider(obj: RoutableObject, p: Any) -> None:\n    api = get_impl_api(p)\n    if api == Api.inference:\n        if not hasattr(p, 'register_model'):\n            raise ValueError(\"Provider missing register_model implementation\")\n        await p.register_model(obj)\n    elif api == Api.safety:\n        if not hasattr(p, 'register_shield'):\n            raise ValueError(\"Provider missing register_shield implementation\")\n        await p.register_shield(obj)\n    elif api == Api.memory:\n        if not hasattr(p, 'register_memory_bank'):\n            raise ValueError(\"Provider missing register_memory_bank implementation\")\n        await p.register_memory_bank(obj)\n    else:\n        raise ValueError(f\"Unsupported API type: {api}\")\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 346
            },
            {
                "body": "_:warning: Potential issue_\n\n**Add concurrency protection for registry modifications.**\n\nThe `register_object` method directly modifies the shared registry without any synchronization mechanism. This could lead to race conditions in concurrent registration scenarios.\n\nConsider adding a lock:\n\n```diff\n+from asyncio import Lock\n\nclass CommonRoutingTableImpl(RoutingTable):\n    def __init__(self, impls_by_provider_id: Dict[str, RoutedProtocol]) -> None:\n        self.impls_by_provider_id = impls_by_provider_id\n+        self._registry_lock = Lock()\n\n    async def register_object(self, obj: RoutableObjectWithProvider):\n+        async with self._registry_lock:\n            if not obj.provider_id and len(self.impls_by_provider_id) > 0:\n                obj.provider_id = list(self.impls_by_provider_id.keys())[0]\n\n            if obj.provider_id not in self.impls_by_provider_id:\n                raise ValueError(f\"Provider `{obj.provider_id}` not found\")\n            \n            p = self.impls_by_provider_id[obj.provider_id]\n            await register_object_with_provider(obj, p)\n            \n            if obj.identifier not in self.registry:\n                self.registry[obj.identifier] = []\n            self.registry[obj.identifier].append(obj)\n```\n\n\n> Committable suggestion was skipped due to low confidence.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/distribution/server/server.py",
                "position": 451
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/distribution/server/server.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " # This source code is licensed under the terms described in the LICENSE file in\n # the root directory of this source tree.\n \n-import asyncio\n-import functools\n-import inspect\n-import json\n-import signal\n-import traceback\n-\n-from contextlib import asynccontextmanager\n-from ssl import SSLError\n-from typing import Any, Dict, Optional\n-\n-import fire\n-import httpx\n-import yaml\n-\n-from fastapi import Body, FastAPI, HTTPException, Request, Response\n-from fastapi.exceptions import RequestValidationError\n-from fastapi.responses import JSONResponse, StreamingResponse\n-from pydantic import BaseModel, ValidationError\n-from termcolor import cprint\n-from typing_extensions import Annotated\n-\n-from llama_stack.distribution.distribution import builtin_automatically_routed_apis\n-\n-from llama_stack.providers.utils.telemetry.tracing import (\n-    end_trace,\n-    setup_logger,\n-    SpanStatus,\n-    start_trace,\n-)\n+from typing import Any, Dict, List, Optional\n+\n+from llama_models.llama3.api.datatypes import *  # noqa: F403\n+\n+from llama_stack.apis.models import *  # noqa: F403\n+from llama_stack.apis.shields import *  # noqa: F403\n+from llama_stack.apis.memory_banks import *  # noqa: F403\n+\n from llama_stack.distribution.datatypes import *  # noqa: F403\n \n-from llama_stack.distribution.request_headers import set_request_provider_data\n-from llama_stack.distribution.resolver import resolve_impls_with_routing\n-\n-from .endpoints import get_all_api_endpoints\n-\n-\n-def create_sse_event(data: Any) -> str:\n-    if isinstance(data, BaseModel):\n-        data = data.json()\n-    else:\n-        data = json.dumps(data)\n-\n-    return f\"data: {data}\\n\\n\"\n-\n-\n-async def global_exception_handler(request: Request, exc: Exception):\n-    traceback.print_exception(exc)\n-    http_exc = translate_exception(exc)\n-\n-    return JSONResponse(\n-        status_code=http_exc.status_code, content={\"error\": {\"detail\": http_exc.detail}}\n-    )\n-\n-\n-def translate_exception(exc: Exception) -> Union[HTTPException, RequestValidationError]:\n-    if isinstance(exc, ValidationError):\n-        exc = RequestValidationError(exc.raw_errors)\n-\n-    if isinstance(exc, RequestValidationError):\n-        return HTTPException(\n-            status_code=400,\n-            detail={\n-                \"errors\": [\n-                    {\n-                        \"loc\": list(error[\"loc\"]),\n-                        \"msg\": error[\"msg\"],\n-                        \"type\": error[\"type\"],\n-                    }\n-                    for error in exc.errors()\n-                ]\n-            },\n-        )\n-    elif isinstance(exc, ValueError):\n-        return HTTPException(status_code=400, detail=f\"Invalid value: {str(exc)}\")\n-    elif isinstance(exc, PermissionError):\n-        return HTTPException(status_code=403, detail=f\"Permission denied: {str(exc)}\")\n-    elif isinstance(exc, TimeoutError):\n-        return HTTPException(status_code=504, detail=f\"Operation timed out: {str(exc)}\")\n-    elif isinstance(exc, NotImplementedError):\n-        return HTTPException(status_code=501, detail=f\"Not implemented: {str(exc)}\")\n-    else:\n-        return HTTPException(\n-            status_code=500,\n-            detail=\"Internal server error: An unexpected error occurred.\",\n-        )\n-\n-\n-async def passthrough(\n-    request: Request,\n-    downstream_url: str,\n-    downstream_headers: Optional[Dict[str, str]] = None,\n-):\n-    await start_trace(request.path, {\"downstream_url\": downstream_url})\n-\n-    headers = dict(request.headers)\n-    headers.pop(\"host\", None)\n-    headers.update(downstream_headers or {})\n-\n-    content = await request.body()\n-\n-    client = httpx.AsyncClient()\n-    erred = False\n-    try:\n-        req = client.build_request(\n-            method=request.method,\n-            url=downstream_url,\n-            headers=headers,\n-            content=content,\n-            params=request.query_params,\n-        )\n-        response = await client.send(req, stream=True)\n-\n-        async def stream_response():\n-            async for chunk in response.aiter_raw(chunk_size=64):\n-                yield chunk\n-\n-            await response.aclose()\n-            await client.aclose()\n-\n-        return StreamingResponse(\n-            stream_response(),\n-            status_code=response.status_code,\n-            headers=dict(response.headers),\n-            media_type=response.headers.get(\"content-type\"),\n-        )\n-\n-    except httpx.ReadTimeout:\n-        erred = True\n-        return Response(content=\"Downstream server timed out\", status_code=504)\n-    except httpx.NetworkError as e:\n-        erred = True\n-        return Response(content=f\"Network error: {str(e)}\", status_code=502)\n-    except httpx.TooManyRedirects:\n-        erred = True\n-        return Response(content=\"Too many redirects\", status_code=502)\n-    except SSLError as e:\n-        erred = True\n-        return Response(content=f\"SSL error: {str(e)}\", status_code=502)\n-    except httpx.HTTPStatusError as e:\n-        erred = True\n-        return Response(content=str(e), status_code=e.response.status_code)\n-    except Exception as e:\n-        erred = True\n-        return Response(content=f\"Unexpected error: {str(e)}\", status_code=500)\n-    finally:\n-        await end_trace(SpanStatus.OK if not erred else SpanStatus.ERROR)\n-\n-\n-def handle_sigint(app, *args, **kwargs):\n-    print(\"SIGINT or CTRL-C detected. Exiting gracefully...\")\n-\n-    async def run_shutdown():\n-        for impl in app.__llama_stack_impls__.values():\n-            print(f\"Shutting down {impl}\")\n-            await impl.shutdown()\n-\n-    asyncio.run(run_shutdown())\n-\n-    loop = asyncio.get_event_loop()\n-    for task in asyncio.all_tasks(loop):\n-        task.cancel()\n-\n-    loop.stop()\n-\n-\n-@asynccontextmanager\n-async def lifespan(app: FastAPI):\n-    print(\"Starting up\")\n-    yield\n-\n-    print(\"Shutting down\")\n-    for impl in app.__llama_stack_impls__.values():\n-        await impl.shutdown()\n-\n-\n-def create_dynamic_passthrough(\n-    downstream_url: str, downstream_headers: Optional[Dict[str, str]] = None\n-):\n-    async def endpoint(request: Request):\n-        return await passthrough(request, downstream_url, downstream_headers)\n-\n-    return endpoint\n-\n-\n-def is_streaming_request(func_name: str, request: Request, **kwargs):\n-    # TODO: pass the api method and punt it to the Protocol definition directly\n-    return kwargs.get(\"stream\", False)\n-\n-\n-async def maybe_await(value):\n-    if inspect.iscoroutine(value):\n-        return await value\n-    return value\n-\n-\n-async def sse_generator(event_gen):\n-    try:\n-        async for item in event_gen:\n-            yield create_sse_event(item)\n-            await asyncio.sleep(0.01)\n-    except asyncio.CancelledError:\n-        print(\"Generator cancelled\")\n-        await event_gen.aclose()\n-    except Exception as e:\n-        traceback.print_exception(e)\n-        yield create_sse_event(\n-            {\n-                \"error\": {\n-                    \"message\": str(translate_exception(e)),\n-                },\n-            }\n-        )\n-    finally:\n-        await end_trace()\n-\n-\n-def create_dynamic_typed_route(func: Any, method: str):\n-\n-    async def endpoint(request: Request, **kwargs):\n-        await start_trace(func.__name__)\n-\n-        set_request_provider_data(request.headers)\n-\n-        is_streaming = is_streaming_request(func.__name__, request, **kwargs)\n-        try:\n-            if is_streaming:\n-                return StreamingResponse(\n-                    sse_generator(func(**kwargs)), media_type=\"text/event-stream\"\n-                )\n-            else:\n-                value = func(**kwargs)\n-                return await maybe_await(value)\n-        except Exception as e:\n-            traceback.print_exception(e)\n-            raise translate_exception(e) from e\n-        finally:\n-            await end_trace()\n-\n-    sig = inspect.signature(func)\n-    new_params = [\n-        inspect.Parameter(\n-            \"request\", inspect.Parameter.POSITIONAL_OR_KEYWORD, annotation=Request\n-        )\n-    ]\n-    new_params.extend(sig.parameters.values())\n-\n-    if method == \"post\":\n-        # make sure every parameter is annotated with Body() so FASTAPI doesn't\n-        # do anything too intelligent and ask for some parameters in the query\n-        # and some in the body\n-        new_params = [new_params[0]] + [\n-            param.replace(annotation=Annotated[param.annotation, Body(..., embed=True)])\n-            for param in new_params[1:]\n-        ]\n-\n-    endpoint.__signature__ = sig.replace(parameters=new_params)\n-\n-    return endpoint\n-\n-\n-def main(\n-    yaml_config: str = \"llamastack-run.yaml\",\n-    port: int = 5000,\n-    disable_ipv6: bool = False,\n-):\n-    with open(yaml_config, \"r\") as fp:\n-        config = StackRunConfig(**yaml.safe_load(fp))\n-\n-    app = FastAPI()\n-\n-    impls = asyncio.run(resolve_impls_with_routing(config))\n-    if Api.telemetry in impls:\n-        setup_logger(impls[Api.telemetry])\n-\n-    all_endpoints = get_all_api_endpoints()\n-\n-    if config.apis:\n-        apis_to_serve = set(config.apis)\n-    else:\n-        apis_to_serve = set(impls.keys())\n-\n-    for inf in builtin_automatically_routed_apis():\n-        apis_to_serve.add(inf.routing_table_api.value)\n-\n-    apis_to_serve.add(\"inspect\")\n-    for api_str in apis_to_serve:\n-        api = Api(api_str)\n-\n-        endpoints = all_endpoints[api]\n-        impl = impls[api]\n-\n-        if is_passthrough(impl.__provider_spec__):\n-            for endpoint in endpoints:\n-                url = impl.__provider_config__.url.rstrip(\"/\") + endpoint.route\n-                getattr(app, endpoint.method)(endpoint.route)(\n-                    create_dynamic_passthrough(url)\n-                )\n-        else:\n-            for endpoint in endpoints:\n-                if not hasattr(impl, endpoint.name):\n-                    # ideally this should be a typing violation already\n-                    raise ValueError(\n-                        f\"Could not find method {endpoint.name} on {impl}!!\"\n-                    )\n-\n-                impl_method = getattr(impl, endpoint.name)\n-\n-                getattr(app, endpoint.method)(endpoint.route, response_model=None)(\n-                    create_dynamic_typed_route(\n-                        impl_method,\n-                        endpoint.method,\n-                    )\n-                )\n \n-        cprint(f\"Serving API {api_str}\", \"white\", attrs=[\"bold\"])\n-        for endpoint in endpoints:\n-            cprint(f\" {endpoint.method.upper()} {endpoint.route}\", \"white\")\n+def get_impl_api(p: Any) -> Api:\n+    return p.__provider_spec__.api\n+\n \n-    print(\"\")\n-    app.exception_handler(RequestValidationError)(global_exception_handler)\n-    app.exception_handler(Exception)(global_exception_handler)\n-    signal.signal(signal.SIGINT, functools.partial(handle_sigint, app))\n+async def register_object_with_provider(obj: RoutableObject, p: Any) -> None:\n+    api = get_impl_api(p)\n+    if api == Api.inference:\n+        await p.register_model(obj)\n+    elif api == Api.safety:\n+        await p.register_shield(obj)\n+    elif api == Api.memory:\n+        await p.register_memory_bank(obj)\n \n-    app.__llama_stack_impls__ = impls\n \n-    import uvicorn\n+Registry = Dict[str, List[RoutableObjectWithProvider]]\n \n-    # FYI this does not do hot-reloads\n-    listen_host = \"::\" if not disable_ipv6 else \"0.0.0.0\"\n-    print(f\"Listening on {listen_host}:{port}\")\n-    uvicorn.run(app, host=listen_host, port=port)\n \n+# TODO: this routing table maintains state in memory purely. We need to\n+# add persistence to it when we add dynamic registration of objects.\n+class CommonRoutingTableImpl(RoutingTable):\n+    def __init__(\n+        self,\n+        impls_by_provider_id: Dict[str, RoutedProtocol],\n+    ) -> None:\n+        self.impls_by_provider_id = impls_by_provider_id\n+\n+    async def initialize(self) -> None:\n+        self.registry: Registry = {}\n+\n+        def add_objects(objs: List[RoutableObjectWithProvider]) -> None:\n+            for obj in objs:\n+                if obj.identifier not in self.registry:\n+                    self.registry[obj.identifier] = []\n+\n+                self.registry[obj.identifier].append(obj)\n+\n+        for pid, p in self.impls_by_provider_id.items():\n+            api = get_impl_api(p)\n+            if api == Api.inference:\n+                p.model_store = self\n+                models = await p.list_models()\n+                add_objects(\n+                    [ModelDefWithProvider(**m.dict(), provider_id=pid) for m in models]\n+                )\n+\n+            elif api == Api.safety:\n+                p.shield_store = self\n+                shields = await p.list_shields()\n+                add_objects(\n+                    [\n+                        ShieldDefWithProvider(**s.dict(), provider_id=pid)\n+                        for s in shields\n+                    ]\n+                )\n \n-if __name__ == \"__main__\":\n-    fire.Fire(main)\n+            elif api == Api.memory:\n+                p.memory_bank_store = self\n+                memory_banks = await p.list_memory_banks()\n+\n+                # do in-memory updates due to pesky Annotated unions\n+                for m in memory_banks:\n+                    m.provider_id = pid\n+\n+                add_objects(memory_banks)\n+\n+    async def shutdown(self) -> None:\n+        for p in self.impls_by_provider_id.values():\n+            await p.shutdown()\n+\n+    def get_provider_impl(\n+        self, routing_key: str, provider_id: Optional[str] = None\n+    ) -> Any:\n+        if routing_key not in self.registry:\n+            raise ValueError(f\"`{routing_key}` not registered\")\n+\n+        objs = self.registry[routing_key]\n+        for obj in objs:\n+            if not provider_id or provider_id == obj.provider_id:\n+                return self.impls_by_provider_id[obj.provider_id]\n+\n+        raise ValueError(f\"Provider not found for `{routing_key}`\")\n+\n+    def get_object_by_identifier(\n+        self, identifier: str\n+    ) -> Optional[RoutableObjectWithProvider]:\n+        objs = self.registry.get(identifier, [])\n+        if not objs:\n+            return None\n+\n+        # kind of ill-defined behavior here, but we'll just return the first one\n+        return objs[0]\n+\n+    async def register_object(self, obj: RoutableObjectWithProvider):\n+        # if provider_id is not specified, we'll pick an arbitrary one from existing entries\n+        if not obj.provider_id and len(self.impls_by_provider_id) > 0:\n+            obj.provider_id = list(self.impls_by_provider_id.keys())[0]\n+\n+        if obj.provider_id not in self.impls_by_provider_id:\n+            raise ValueError(f\"Provider `{obj.provider_id}` not found\")\n+        \n+        p = self.impls_by_provider_id[obj.provider_id]\n+        await register_object_with_provider(obj, p)\n+        \n+        # Direct registry modification without checking current state\n+        if obj.identifier not in self.registry:\n+            self.registry[obj.identifier] = []\n+        self.registry[obj.identifier].append(obj)\n+        # TODO: persist this to a store\n+\n+\n+class ModelsRoutingTable(CommonRoutingTableImpl, Models):\n+    async def list_models(self) -> List[ModelDefWithProvider]:\n+        objects = []\n+        for objs in self.registry.values():\n+            objects.extend(objs)\n+        return objects\n+\n+    async def get_model(self, identifier: str) -> Optional[ModelDefWithProvider]:\n+        return self.get_object_by_identifier(identifier)\n+\n+    async def register_model(self, model: ModelDefWithProvider) -> None:\n+        await self.register_object(model)\n+\n+\n+class ShieldsRoutingTable(CommonRoutingTableImpl, Shields):\n+    async def list_shields(self) -> List[ShieldDef]:\n+        objects = []\n+        for objs in self.registry.values():\n+            objects.extend(objs)\n+        return objects\n+\n+    async def get_shield(self, shield_type: str) -> Optional[ShieldDefWithProvider]:\n+        return self.get_object_by_identifier(shield_type)\n+\n+    async def register_shield(self, shield: ShieldDefWithProvider) -> None:\n+        await self.register_object(shield)\n+\n+\n+class MemoryBanksRoutingTable(CommonRoutingTableImpl, MemoryBanks):\n+    async def list_memory_banks(self) -> List[MemoryBankDefWithProvider]:\n+        objects = []\n+        for objs in self.registry.values():\n+            objects.extend(objs)\n+        return objects\n+\n+    async def get_memory_bank(\n+        self, identifier: str\n+    ) -> Optional[MemoryBankDefWithProvider]:\n+        return self.get_object_by_identifier(identifier)\n+\n+    async def register_memory_bank(\n+        self, memory_bank: MemoryBankDefWithProvider\n+    ) -> None:\n+        await self.register_object(memory_bank)\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "72",
        "type": "4",
        "number": 121,
        "head_branch": "error-037-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces updates to the routing of web methods within the `Inference` protocol of the API. Specifically, the routes for three methods have been modified to incorporate a versioning scheme. The `completion` method's route has been changed from `/inference/completion` to `/v1/completion`. Similarly, the `chat_completion` method's route has been updated from `/inference/chat_completion` to `/v1/chat_completion`, and the `embeddings` method's route has transitioned from `/inference/embeddings` to `/v1/embeddings`. \n\nThese changes affect the endpoint URLs for invoking these methods, reflecting a move towards a versioned API design. It is important to note that the core functionality of the `Inference` protocol remains unchanged, as the method signatures, parameters, and return types have not been altered. The updates solely pertain to the routing of the web methods, ensuring that existing functionality is preserved while adopting a structured versioning approach.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\nDwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgDoCEYDEZyAAUASpADK2VmgqyjcEj2wAbb5AokAI7Y0riQ4rjedJAARACCeCzU0dChXAACJBJo3mCIZPD4FGABEvAkAO6UYABMYOQVefgY5LgxkBVoyIjYAszqNPRy4bCe2Pl85LA5YGg1AF7oGPSo8EoY4gBm5UPyuKOQGE4ClJAAjDVnGpAAkmEMmJAn6AwM0vn01COem/i++BV4BgiJAAKo2AAyXAA2rBcLhuIgOAB6ZFEdSwXoaJjMZFMJRUVTqbTIgDif3WYFsYAAItQuiRcMjuD5vMiLmcALoACjhCKRqPR+yxOLximURNwJPJ3kp1LpUvyTJZvnZlwAlNd3AclIgGBR4NxxM0eBR8GUlCtlvAHjRkPsvvtUCq/AFgqEOl0cARmNQbTlvPJ9SRkrtvuF8H9IAADdL4iVqKXwZEkGokEpZcpVCjR64AOXw6FotHUhQwOUgSiT3mQRQUGxIAA8wip8HhwwxpsDpPw+PtPCyKNx8Pl+Jtwy7/EEQogWwEUBgGN5sJaF+HdfrDcaMBojPojCYoGR6Phx2hEsQyMpBgpWOwuLx+MJROIpDIg+LCYmtDp9IeYAgyCoI854EJe5BUDeOJsBsXBUBUkA9M4rhPB+BIqN+2i6GAhjGOAYBGLw+AiGIyJoNwya+GgvoAPqzmgDAANZkRRiDIkCmyUGQbzsRgnEBIuJAaNwsgcAYMQSQYFiQHENzgde0RIb6KGnowXakIg2qeCWmzjkCuBmrQ2BvMgzCKPA2y2mWyC8PAynwIGkC/Aw4zRCa/b+G24jAmOHQkAIkBsPsijIICTpYB50Y3HxXGCdGpr4AQTDeNcHieXgPa/H2sABJ4QWwCFkDTFITwkGQkDYNwtChk5ZrMOgkCkBBNoxrx/HcSQyIaN18VZb6YQEA1UgUIgZbRH1XzRsiEhnF1PXXPY3CiBZ/q+LIAA04a/P8gI+Z2mAaX586+koYkGFcAGeNGOLcFE27xfligAOTIGaGWesg+3dvQmx1a1HGxTxN13WW8WDVNM1iswt2MqDu41KlBzXdMuA0cDsPNA9jIFbQL3pTQH1qQd41/VNAMCUDKNoywMP3RGrWQ/tqPo/du4AMyI1dJDMCcxZAkQiBY8FuOvV5nidJ96kkyw/0xRTnXc7zJbAoL9MQ7Nit0MrAu5lpo5fYdaC6S+4bHsO+lgpCyCuT9dZAhI+CMfz3yjo9tBARFBxxFYNybUCJZWT5aCIQgmwDQCLju5Aw2jc0ztAuG3s3JW0jwEQO5GHEfPbgGG3hlFcsdfFRFJVGAS+kC1uLlLVrhJQzC1uObuIWn5a4NgASIJt3AuNRjKUF3Sz0AE7cUBFshLYPZA9AaPkOgNBxMPOmzYIuOfeOo8iYFaNAUJsDGeOX2hYPpDFhLqFEEx5b3eSClXVXau5GFJlhxN4u9+s09qFh5ShLr325G6QCbMOCgN46wsgEBvBgwCNilmkHuSAABZbGih8ZjCqqGLgsYqh9FQbQbkN8SAAF4YhtUBp1FmZYYjqnignf+iBkAF3anFSAgAkwhjOkXBbtCFi1IdNWaVDmg0LoVgBhTDoosLeNGAwUAUHC3QRVTBgxsFcP8jwoh/DyYdTxFTIRGARFrnETGSRFD4ocJweo/BvCMr8MZnommINhG0KMd4LoEjC5xVkcg/Bij75YM4dw6xmiyHaMEimHmWt+aIEMfQtxjCTGeOkewwJVjhY2JoHYjWkS+Yq1iWI+JHipEkBkRJGIe4cIHnwrAk8Z4LxNQUvQaC95/BoAQkpFw8hhjxi/OoH82E9D/mguoGiaxEA0VKFmOgdEpRgIGVUiAkAAAcmwzh0E2GzAA7AAFhqAABi2Zss4DAzibIYLQAAbLQTYABObZKhtkMAEBck5cxNh7KOfQfceFFnDNRmMiZmZKjTOPAMoAA===\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            },
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- This is an auto-generated comment: skip review by coderabbit.ai -->\n\n> [!IMPORTANT]\n> ## Review skipped\n> \n> Auto reviews are disabled on base/target branches other than the default branch.\n> \n> \n> \n> Please check the settings in the CodeRabbit UI or the `.coderabbit.yaml` file in this repository. To trigger a single review, invoke the `@coderabbitai review` command.\n> \n> You can disable this status message by setting the `reviews.review_status` to `false` in the CodeRabbit configuration file.\n\n<!-- end of auto-generated comment: skip review by coderabbit.ai --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n- Review comments: Directly reply to a review comment made by CodeRabbit. Example:\n  - `I pushed a fix in commit <commit_id>, please review it.`\n  - `Generate unit testing code for this file.`\n  - `Open a follow-up GitHub issue for this discussion.`\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@coderabbitai` in a new review comment at the desired location with your query. Examples:\n  - `@coderabbitai generate unit testing code for this file.`\n  -\t`@coderabbitai modularize this function.`\n- PR comments: Tag `@coderabbitai` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@coderabbitai gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@coderabbitai read src/utils.ts and generate unit testing code.`\n  - `@coderabbitai read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@coderabbitai help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@coderabbitai pause` to pause the reviews on a PR.\n- `@coderabbitai resume` to resume the paused reviews.\n- `@coderabbitai review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@coderabbitai full review` to do a full review from scratch and review all the files again.\n- `@coderabbitai summary` to regenerate the summary of the PR.\n- `@coderabbitai resolve` resolve all the CodeRabbit review comments.\n- `@coderabbitai configuration` to show the current CodeRabbit configuration for the repository.\n- `@coderabbitai help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@coderabbitai ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [],
        "file_changes": [
            {
                "file": "project/api/llama_stack/apis/inference/inference.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " \n     # This method is not `async def` because it can result in either an\n     # `AsyncGenerator` or a `CompletionResponse` depending on the value of `stream`.\n-    @webmethod(route=\"/inference/completion\")\n+    @webmethod(route=\"/v1/completion\")\n     def completion(\n         self,\n         model: str,\n"
                    },
                    {
                        "id": 2,
                        "content": " \n     # This method is not `async def` because it can result in either an\n     # `AsyncGenerator` or a `ChatCompletionResponse` depending on the value of `stream`.\n-    @webmethod(route=\"/inference/chat_completion\")\n+    @webmethod(route=\"/v1/chat_completion\")\n     def chat_completion(\n         self,\n         model: str,\n"
                    },
                    {
                        "id": 3,
                        "content": "         logprobs: Optional[LogProbConfig] = None,\n     ) -> Union[ChatCompletionResponse, ChatCompletionResponseStreamChunk]: ...\n \n-    @webmethod(route=\"/inference/embeddings\")\n+    @webmethod(route=\"/v1/embeddings\")\n     async def embeddings(\n         self,\n         model: str,\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "73",
        "type": "4",
        "number": 122,
        "head_branch": "error-005-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces modifications to the `MetaReferenceAgentsImpl` class located in `agents.py`. Key changes are made to the `create_agent_turn` and `_create_agent_turn_streaming` methods. In `create_agent_turn`, the condition for returning a streaming response has been revised from checking solely the `stream` parameter to also considering whether `attachments` is `None`. This adjustment allows for streaming responses in additional scenarios. \n\nIn the `_create_agent_turn_streaming` method, a new conditional check has been implemented to ensure that the agent is retrieved only when the request is not for streaming and attachments are present. This change affects the control flow and may optimize agent retrieval based on request characteristics. \n\nWhile the error handling in the `get_agent` method remains unchanged, the overall logic flow of the class is impacted by the new conditions in the aforementioned methods. Other methods in the class, such as `get_agents_turn`, `get_agents_step`, `get_agents_session`, `delete_agents_session`, and `delete_agents`, have not been modified and continue to raise `NotImplementedError`. Additionally, the method signatures for `create_agent_turn` and `_create_agent_turn_streaming` have been updated to reflect these changes.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:warning: Potential issue_\n\n**Fix inconsistent streaming logic and add documentation.**\n\nThere are several issues with the streaming logic:\n1. The method always sets `request.stream=True` but then checks the `stream` parameter, making the check redundant\n2. The condition `attachments is None` forces streaming even when not explicitly requested\n3. The behavior for different combinations of `stream` and `attachments` is unclear without documentation\n\n\nConsider this implementation:\n\n```diff\n    def create_agent_turn(\n        self,\n        agent_id: str,\n        session_id: str,\n        messages: List[Union[UserMessage, ToolResponseMessage]],\n        attachments: Optional[List[Attachment]] = None,\n        stream: Optional[bool] = False,\n    ) -> AsyncGenerator:\n+       \"\"\"Create a new agent turn.\n+       \n+       Args:\n+           agent_id: The ID of the agent\n+           session_id: The ID of the session\n+           messages: List of messages to process\n+           attachments: Optional list of attachments\n+           stream: Whether to stream the response\n+       \n+       Returns:\n+           AsyncGenerator yielding agent events\n+           \n+       Note:\n+           Streaming is enabled when either:\n+           - stream=True is explicitly requested\n+           - No attachments are provided\n+       \"\"\"\n        request = AgentTurnCreateRequest(\n            agent_id=agent_id,\n            session_id=session_id,\n            messages=messages,\n            attachments=attachments,\n-           stream=True,\n+           stream=stream or attachments is None,\n        )\n\n-       if stream or attachments is None:\n+       if request.stream:\n            return self._create_agent_turn_streaming(request)\n        else:\n            raise NotImplementedError(\"Non-streaming agent turns not yet implemented\")\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n        def create_agent_turn(\n            self,\n            agent_id: str,\n            session_id: str,\n            messages: List[Union[UserMessage, ToolResponseMessage]],\n            attachments: Optional[List[Attachment]] = None,\n            stream: Optional[bool] = False,\n        ) -> AsyncGenerator:\n            \"\"\"Create a new agent turn.\n            \n            Args:\n                agent_id: The ID of the agent\n                session_id: The ID of the session\n                messages: List of messages to process\n                attachments: Optional list of attachments\n                stream: Whether to stream the response\n            \n            Returns:\n                AsyncGenerator yielding agent events\n                \n            Note:\n                Streaming is enabled when either:\n                - stream=True is explicitly requested\n                - No attachments are provided\n            \"\"\"\n            request = AgentTurnCreateRequest(\n                agent_id=agent_id,\n                session_id=session_id,\n                messages=messages,\n                attachments=attachments,\n                stream=stream or attachments is None,\n            )\n\n            if request.stream:\n                return self._create_agent_turn_streaming(request)\n            else:\n                raise NotImplementedError(\"Non-streaming agent turns not yet implemented\")\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "position": 8
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/impls/meta_reference/agents/agents.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "             attachments=attachments,\n             stream=True,\n         )\n-        if stream:\n+        if stream or attachments is None:\n             return self._create_agent_turn_streaming(request)\n         else:\n             raise NotImplementedError(\"Non-streaming agent turns not yet implemented\")\n"
                    },
                    {
                        "id": 2,
                        "content": "         self,\n         request: AgentTurnCreateRequest,\n     ) -> AsyncGenerator:\n-        agent = await self.get_agent(request.agent_id)\n+        if not request.stream and request.attachments:\n+            agent = await self.get_agent(request.agent_id)\n         async for event in agent.create_and_execute_turn(request):\n             yield event\n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "74",
        "type": "4",
        "number": 123,
        "head_branch": "error-003-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the pull request focus on the `vector_store.py` file within the project. Specifically, the `get_embedding_model` function has been modified to improve the efficiency of loading the embedding model. The new implementation checks for the presence of the model in the `EMBEDDING_MODELS` dictionary before loading it, which eliminates unnecessary loading and associated print statements. \n\nAdditionally, the `content_from_data` function has been enhanced to utilize the `chardet` library for encoding detection when the encoding is not specified in the data URL. This adjustment ensures accurate content processing based on the detected encoding. \n\nThe `content_from_doc` function remains unchanged, continuing to support both data URLs and standard URLs. The overall structure and functionality of the `EmbeddingIndex` and `BankWithIndex` classes are preserved, with no alterations to their method signatures or core logic. The changes primarily enhance model loading efficiency and data encoding handling while maintaining existing functionalities.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider batching document processing for better performance.**\n\nThe `get_embedding_model` function is used in async methods for both document insertion and querying. Consider batching the document processing in `insert_documents` to reduce the number of model loading checks and improve throughput.\n\nExample implementation:\n\n```python\nasync def insert_documents(self, documents: List[MemoryBankDocument]) -> None:\n    model = get_embedding_model(self.bank.embedding_model)\n    \n    # Process documents in batches\n    batch_size = 10  # Adjust based on your needs\n    chunks_batch = []\n    \n    for i in range(0, len(documents), batch_size):\n        batch = documents[i:i + batch_size]\n        batch_chunks = []\n        \n        # Collect chunks from batch\n        for doc in batch:\n            content = await content_from_doc(doc)\n            chunks = make_overlapped_chunks(\n                doc.document_id,\n                content,\n                self.bank.chunk_size_in_tokens,\n                self.bank.overlap_size_in_tokens\n                or (self.bank.chunk_size_in_tokens // 4),\n            )\n            batch_chunks.extend(chunks)\n            \n        if batch_chunks:\n            # Process batch at once\n            embeddings = model.encode([x.content for x in batch_chunks]).astype(np.float32)\n            await self.index.add_chunks(batch_chunks, embeddings)\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "position": 18
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/utils/memory/vector_store.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " def get_embedding_model(model: str) -> \"SentenceTransformer\":\n     global EMBEDDING_MODELS\n \n-    loaded_model = EMBEDDING_MODELS.get(model)\n-    if loaded_model is not None:\n-        return loaded_model\n+    if model not in EMBEDDING_MODELS:\n+        print(f\"Loading sentence transformer for {model}...\")\n+        from sentence_transformers import SentenceTransformer\n+        loaded_model = SentenceTransformer(model)\n+        EMBEDDING_MODELS[model] = loaded_model\n \n-    print(f\"Loading sentence transformer for {model}...\")\n-    from sentence_transformers import SentenceTransformer\n-\n-    loaded_model = SentenceTransformer(model)\n-    EMBEDDING_MODELS[model] = loaded_model\n+    loaded_model = EMBEDDING_MODELS[model]\n     return loaded_model\n \n \n"
                    }
                ]
            }
        ]
    },
    {
        "id": "75",
        "type": "4",
        "number": 124,
        "head_branch": "error-004-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe pull request introduces changes to the `reward_scoring.py` file within the `project/api/llama_stack/apis/reward_scoring` directory. A new attribute named `score_threshold` of type `float` with a default value of `0.0` is added to the `RewardScoringRequest` class. This addition allows users to specify a scoring threshold in their requests. \n\nAdditionally, the `reward_score` method in the `RewardScoring` protocol is modified to accept the new `score_threshold` parameter, also defaulting to `0.0`. The implementation of this method is updated to incorporate logic that utilizes the `score_threshold`. Specifically, if the `dialog_generations` input is empty, the method returns an empty `RewardScoringResponse`. If the `score_threshold` is greater than or equal to `0`, it returns a `RewardScoringResponse` containing an empty list of `ScoredDialogGenerations`. \n\nThese modifications collectively enhance the reward scoring mechanism by enabling the specification of a score threshold and adjusting the response based on the provided input conditions.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [],
        "file_changes": [
            {
                "file": "project/api/llama_stack/apis/reward_scoring/reward_scoring.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": " \n     dialog_generations: List[DialogGenerations]\n     model: str\n+    score_threshold: float = 0.0\n \n \n @json_schema_type\n"
                    },
                    {
                        "id": 2,
                        "content": "     def reward_score(\n         self,\n         dialog_generations: List[DialogGenerations],\n-        model: str,\n-    ) -> Union[RewardScoringResponse]: ...\n+        model: str, \n+        score_threshold: float = 0.0,\n+    ) -> Union[RewardScoringResponse]:\n+        if not dialog_generations:\n+            return RewardScoringResponse(scored_generations=[])\n+        \n+        if score_threshold >= 0:\n+            return RewardScoringResponse(\n+                scored_generations=[\n+                    ScoredDialogGenerations(dialog=[], scored_generations=[])\n+                ])\n"
                    }
                ]
            }
        ]
    },
    {
        "id": "76",
        "type": "4",
        "number": 125,
        "head_branch": "error-018-api",
        "comments": [
            {
                "body": "<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\n<!-- walkthrough_start -->\n\n## Walkthrough\n\nThe changes in the pull request focus on the `pgvector.py` file, specifically enhancing the `PGVectorMemoryAdapter` class. A new private attribute, `_bank_metadata_cache`, has been introduced to cache metadata for memory banks. This attribute is utilized in the `register_memory_bank` method, where metadata is stored after being upserted into the database. The `list_memory_banks` method has been modified to return memory banks using this cache, ensuring that the list reflects the latest state.\n\nAdditionally, the logic for retrieving the index in the `insert_documents` and `query_documents` methods has been updated. The new implementation first checks for the existence of the bank identifier in `_bank_metadata_cache` before accessing it, which aims to prevent errors related to unregistered banks. Error handling in the `initialize` method remains unchanged, while the `shutdown` method is still a placeholder. The SQL queries for table creation and record manipulation have not been altered, maintaining the consistency of the database schema. Overall, these modifications improve the internal state management of memory banks within the `PGVectorMemoryAdapter`.\n\n<!-- walkthrough_end --><!-- This is an auto-generated comment: resource warnings by coderabbit.ai -->\n\n> [!WARNING]\n> ## Review ran into problems\n> \n> <details>\n> <summary>\ud83d\udd25 Problems</summary>\n> \n> Git: Failed to clone repository. Please run the `@nehal-a2z full review` command to re-trigger a full review.\n> \n> </details>\n\n<!-- end of auto-generated comment: resource warnings by coderabbit.ai --><!-- internal state start -->\n\n<!--\n\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\n\n-->\n\n<!-- internal state end --><!-- tips_start -->\n\n---\n\n\n\n<details>\n<summary>\ud83e\udea7 Tips</summary>\n\n### Chat\n\nThere are 3 ways to chat with [CodeRabbit](https://coderabbit.ai):\n\n> \u203c\ufe0f **IMPORTANT**\n> Auto-reply has been disabled for this repository in the CodeRabbit settings. The CodeRabbit bot will not respond to your replies unless it is explicitly tagged.\n- Files and specific lines of code (under the \"Files changed\" tab): Tag `@nehal-a2z` in a new review comment at the desired location with your query. Examples:\n  - `@nehal-a2z generate unit testing code for this file.`\n  -\t`@nehal-a2z modularize this function.`\n- PR comments: Tag `@nehal-a2z` in a new PR comment to ask questions about the PR branch. For the best results, please provide a very specific query, as very limited context is provided in this mode. Examples:\n  - `@nehal-a2z gather interesting stats about this repository and render them as a table. Additionally, render a pie chart showing the language distribution in the codebase.`\n  - `@nehal-a2z read src/utils.ts and generate unit testing code.`\n  - `@nehal-a2z read the files in the src/scheduler package and generate a class diagram using mermaid and a README in the markdown format.`\n  - `@nehal-a2z help me debug CodeRabbit configuration file.`\n\nNote: Be mindful of the bot's finite context window. It's strongly recommended to break down tasks such as reading entire modules into smaller chunks. For a focused discussion, use review comments to chat about specific files and their changes, instead of using the PR comments.\n\n### CodeRabbit Commands (Invoked using PR comments)\n\n- `@nehal-a2z pause` to pause the reviews on a PR.\n- `@nehal-a2z resume` to resume the paused reviews.\n- `@nehal-a2z review` to trigger an incremental review. This is useful when automatic reviews are disabled for the repository.\n- `@nehal-a2z full review` to do a full review from scratch and review all the files again.\n- `@nehal-a2z summary` to regenerate the summary of the PR.\n- `@nehal-a2z resolve` resolve all the CodeRabbit review comments.\n- `@nehal-a2z configuration` to show the current CodeRabbit configuration for the repository.\n- `@nehal-a2z help` to get help.\n\n### Other keywords and placeholders\n\n- Add `@nehal-a2z ignore` anywhere in the PR description to prevent this PR from being reviewed.\n- Add `@nehal-a2z summary` or `@coderabbitai summary` to generate the high-level summary at a specific location in the PR description.\n- Add `@nehal-a2z` or `@coderabbitai` anywhere in the PR title to generate the title automatically.\n\n### CodeRabbit Configuration File (`.coderabbit.yaml`)\n\n- You can programmatically configure CodeRabbit by adding a `.coderabbit.yaml` file to the root of your repository.\n- Please see the [configuration documentation](https://docs.coderabbit.ai/guides/configure-coderabbit) for more information.\n- If your editor has YAML language server enabled, you can add the path at the top of this file to enable auto-completion and validation: `# yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json`\n\n### Documentation and Community\n\n- Visit our [Documentation](https://coderabbit.ai/docs) for detailed information on how to use CodeRabbit.\n- Join our [Discord Community](http://discord.gg/coderabbit) to get help, request features, and share feedback.\n- Follow us on [X/Twitter](https://twitter.com/coderabbitai) for updates and announcements.\n\n</details>\n\n<!-- tips_end -->"
            }
        ],
        "review_comments": [
            {
                "body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Simplify the cache lookup logic.**\n\nThe current implementation:\n```python\nindex = self.cache.get(bank_id) if bank_id in self._bank_metadata_cache else None\n```\ncan be simplified to just use the cache's get method since it already returns None for missing keys:\n\n```diff\n-        index = self.cache.get(bank_id) if bank_id in self._bank_metadata_cache else None\n+        index = self.cache.get(bank_id)\n```\n\n\nAlso applies to: 210-210\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/memory/pgvector/pgvector.py",
                "position": 37
            },
            {
                "body": "_:warning: Potential issue_\n\n**Add error handling for cache operations.**\n\nWhile the database operations are within a try-catch block in `initialize`, the cache operations here are not protected. Consider wrapping the cache operations in a try-catch block to handle potential errors gracefully and maintain consistency between the database and cache states.\n\nHere's a suggested implementation:\n\n```diff\n-        self._bank_metadata_cache[memory_bank.identifier] = memory_bank\n-\n-        index = BankWithIndex(\n-            bank=memory_bank,\n-            index=PGVectorIndex(memory_bank, ALL_MINILM_L6_V2_DIMENSION, self.cursor),\n-        )\n-        self.cache[memory_bank.identifier] = index\n-        self.conn.commit()\n+        try:\n+            self._bank_metadata_cache[memory_bank.identifier] = memory_bank\n+            index = BankWithIndex(\n+                bank=memory_bank,\n+                index=PGVectorIndex(memory_bank, ALL_MINILM_L6_V2_DIMENSION, self.cursor),\n+            )\n+            self.cache[memory_bank.identifier] = index\n+            self.conn.commit()\n+        except Exception as e:\n+            self.conn.rollback()\n+            if memory_bank.identifier in self._bank_metadata_cache:\n+                del self._bank_metadata_cache[memory_bank.identifier]\n+            raise RuntimeError(\"Failed to update memory bank caches\") from e\n```\n\n\n<!-- suggestion_start -->\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n        try:\n            self._bank_metadata_cache[memory_bank.identifier] = memory_bank\n            index = BankWithIndex(\n                bank=memory_bank,\n                index=PGVectorIndex(memory_bank, ALL_MINILM_L6_V2_DIMENSION, self.cursor),\n            )\n            self.cache[memory_bank.identifier] = index\n            self.conn.commit()\n        except Exception as e:\n            self.conn.rollback()\n            if memory_bank.identifier in self._bank_metadata_cache:\n                del self._bank_metadata_cache[memory_bank.identifier]\n            raise RuntimeError(\"Failed to update memory bank caches\") from e\n`````\n\n</details>\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                "path": "project/api/llama_stack/providers/adapters/memory/pgvector/pgvector.py",
                "position": 19
            }
        ],
        "file_changes": [
            {
                "file": "project/api/llama_stack/providers/adapters/memory/pgvector/pgvector.py",
                "hunks": [
                    {
                        "id": 1,
                        "content": "         self.cursor = None\n         self.conn = None\n         self.cache = {}\n+        self._bank_metadata_cache = {}\n \n     async def initialize(self) -> None:\n         try:\n"
                    },
                    {
                        "id": 2,
                        "content": "                 (memory_bank.identifier, memory_bank),\n             ],\n         )\n+        self._bank_metadata_cache[memory_bank.identifier] = memory_bank\n \n         index = BankWithIndex(\n             bank=memory_bank,\n             index=PGVectorIndex(memory_bank, ALL_MINILM_L6_V2_DIMENSION, self.cursor),\n         )\n         self.cache[memory_bank.identifier] = index\n+        self.conn.commit()\n \n     async def list_memory_banks(self) -> List[MemoryBankDef]:\n         banks = load_models(self.cursor, MemoryBankDef)\n"
                    },
                    {
                        "id": 3,
                        "content": "                     index=PGVectorIndex(bank, ALL_MINILM_L6_V2_DIMENSION, self.cursor),\n                 )\n                 self.cache[bank.identifier] = index\n-        return banks\n+        return [self._bank_metadata_cache.get(bank.identifier, bank) for bank in banks]\n \n     async def insert_documents(\n         self,\n"
                    },
                    {
                        "id": 4,
                        "content": "         documents: List[MemoryBankDocument],\n         ttl_seconds: Optional[int] = None,\n     ) -> None:\n-        index = self.cache.get(bank_id, None)\n+        index = self.cache.get(bank_id) if bank_id in self._bank_metadata_cache else None\n         if not index:\n             raise ValueError(f\"Bank {bank_id} not found\")\n \n"
                    },
                    {
                        "id": 5,
                        "content": "         query: InterleavedTextMedia,\n         params: Optional[Dict[str, Any]] = None,\n     ) -> QueryDocumentsResponse:\n-        index = self.cache.get(bank_id, None)\n+        index = self.cache.get(bank_id) if bank_id in self._bank_metadata_cache else None\n         if not index:\n             raise ValueError(f\"Bank {bank_id} not found\")\n \n"
                    }
                ]
            }
        ]
    }
]