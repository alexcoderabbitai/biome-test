{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of all XLSX files you want to merge\n",
    "file_paths = [\"cleaned_merged_data1.xlsx\", \"cleaned_merged_data2.xlsx\", \"cleaned_merged_data3.xlsx\", \"cleaned_merged_data4.xlsx\"]\n",
    "\n",
    "# Read and concatenate all XLSX files\n",
    "dataframes = [pd.read_excel(file) for file in file_paths]\n",
    "superset_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the combined data into a new XLSX file\n",
    "superset_df.to_excel(\"superset-super.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the superset file\n",
    "superset_df = pd.read_excel(\"superset.xlsx\")\n",
    "\n",
    "# List of the 4 additional CSV files to merge based on \"PR Number\"\n",
    "additional_files = [\"eval_json/pr_scores_1.csv\", \"eval_json/pr_scores_2.csv\", \"eval_json/pr_scores_3.csv\", \"eval_json/pr_scores_4.csv\"]\n",
    "\n",
    "# Loop through each additional file and merge it with the superset\n",
    "# for file in additional_files:\n",
    "#     additional_df = pd.read_csv(file)\n",
    "#     # Assuming \"PR Number\" is the common column name between files\n",
    "#     superset_df = pd.merge(superset_df, additional_df, left_on=\"number\", right_on=\"PR Number\", how=\"left\")\n",
    "\n",
    "for i, file in enumerate(additional_files, start=1):\n",
    "    additional_df = pd.read_csv(file)\n",
    "    # Merge on \"Call Number\" in superset_df and \"PR Number\" in additional_df, with unique suffix for each merge\n",
    "    superset_df = pd.merge(\n",
    "        superset_df, \n",
    "        additional_df, \n",
    "        left_on=\"number\", \n",
    "        right_on=\"PR Number\", \n",
    "        how=\"left\", \n",
    "        suffixes=(\"\", f\"_from_file_{i}\")\n",
    "    )\n",
    "\n",
    "\n",
    "# Save the updated superset back to a new file\n",
    "superset_df.to_excel(\"updated_superset.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data merged and cleaned successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel and CSV files\n",
    "xlsx_path = 'superset.xlsx'\n",
    "csv_paths = [\n",
    "    \"eval_json/pr_scores_1.csv\",\n",
    "    \"eval_json/pr_scores_2.csv\",\n",
    "    \"eval_json/pr_scores_3.csv\",\n",
    "    \"eval_json/pr_scores_4.csv\"\n",
    "]\n",
    "\n",
    "# Read the Excel data\n",
    "excel_data = pd.read_excel(xlsx_path)\n",
    "\n",
    "# Iterate over CSV files and merge data\n",
    "# Iterate over CSV files and merge data\n",
    "merged_data = excel_data\n",
    "for csv_path in csv_paths:\n",
    "    csv_data = pd.read_csv(csv_path)\n",
    "    merged_data = pd.merge(merged_data, csv_data, how='left', left_on='number', right_on='PR Number', suffixes=('', '_duplicate'))\n",
    "    # Drop the duplicate columns from the CSV side of the merged data\n",
    "    merged_data = merged_data.loc[:, ~merged_data.columns.str.endswith('_duplicate')]\n",
    "\n",
    "\n",
    "# Save the cleaned merged data to a new Excel file\n",
    "merged_data.to_excel('cleaned_merged_data.xlsx', index=False)\n",
    "\n",
    "print(\"Data merged and cleaned successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data merged and cleaned successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel and CSV files\n",
    "xlsx_path = 'Google Looker/test1.xlsx'\n",
    "csv_path = 'eval_json/pr_scores_1.csv'\n",
    "\n",
    "# Read the Excel and CSV data\n",
    "excel_data = pd.read_excel(xlsx_path)\n",
    "csv_data = pd.read_csv(csv_path)\n",
    "\n",
    "# Merge the dataframes based on the matching column: 'number' in Excel data and 'PR Number' in CSV data\n",
    "merged_data = pd.merge(excel_data, csv_data, how='left', left_on='number', right_on='PR Number')\n",
    "\n",
    "# Drop the duplicate columns: 'PR Number' and 'Head Branch' from the CSV side of the merged data\n",
    "cleaned_data = merged_data.drop(columns=['PR Number', 'Head Branch'])\n",
    "\n",
    "# Save the cleaned merged data to a new Excel file\n",
    "cleaned_data.to_excel('cleaned_merged_data1.xlsx', index=False)\n",
    "\n",
    "print(\"Data merged and cleaned successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.54.3-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/nehal/Documents/Code/Golden-PR-Dataset/env/lib/python3.9/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/nehal/Documents/Code/Golden-PR-Dataset/env/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/nehal/Documents/Code/Golden-PR-Dataset/env/lib/python3.9/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/nehal/Documents/Code/Golden-PR-Dataset/env/lib/python3.9/site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/nehal/Documents/Code/Golden-PR-Dataset/env/lib/python3.9/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /Users/nehal/Documents/Code/Golden-PR-Dataset/env/lib/python3.9/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/nehal/Documents/Code/Golden-PR-Dataset/env/lib/python3.9/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/nehal/Documents/Code/Golden-PR-Dataset/env/lib/python3.9/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/nehal/Documents/Code/Golden-PR-Dataset/env/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/nehal/Documents/Code/Golden-PR-Dataset/env/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /Users/nehal/Documents/Code/Golden-PR-Dataset/env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/nehal/Documents/Code/Golden-PR-Dataset/env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/nehal/Documents/Code/Golden-PR-Dataset/env/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/nehal/Documents/Code/Golden-PR-Dataset/env/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/nehal/Documents/Code/Golden-PR-Dataset/env/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Downloading openai-1.54.3-py3-none-any.whl (389 kB)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-1.54.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silent circuits hum  \n",
      "Machine minds echo our dreams  \n",
      "Dawn of thinking steel\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY=\"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\"\n",
    "OPENAI_API_KEY_1=\"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\"\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"o1\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"write a haiku about ai\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
