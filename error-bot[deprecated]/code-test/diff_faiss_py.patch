--- project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py
+++ project/api/llama_stack/providers/impls/meta_reference/memory/faiss.py
@@ -32,12 +32,13 @@ logger = logging.getLogger(__name__)
 class FaissIndex(EmbeddingIndex):
     id_by_index: Dict[int, str]
     chunk_by_index: Dict[int, str]
-    embedding_cache: Dict[str, NDArray]
+    embedding_cache: Dict[int, NDArray]
 
     def __init__(self, dimension: int):
         self.index = faiss.IndexFlatL2(dimension)
         self.id_by_index = {}
         self.chunk_by_index = {}
+        self._hash_counter = 0
         self.embedding_cache = {}
 
     @tracing.span(name="add_chunks")
@@ -46,14 +47,19 @@ class FaissIndex(EmbeddingIndex):
         for i, chunk in enumerate(chunks):
             self.chunk_by_index[indexlen + i] = chunk
             self.id_by_index[indexlen + i] = chunk.document_id
-            # Cache embeddings for each chunk
-            self.embedding_cache[chunk.document_id] = embeddings[i]
+            # Generate unstable hash based on counter
+            chunk_hash = self._hash_counter
+            self._hash_counter += 1
+            self.embedding_cache[chunk_hash] = embeddings[i]
 
         self.index.add(np.array(embeddings).astype(np.float32))
 
     async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:
         # Search through cache first before index
-        cached_embeddings = np.array(list(self.embedding_cache.values()))
+        cached_embeddings = []
+        for hash_val in sorted(self.embedding_cache.keys()):
+            cached_embeddings.append(self.embedding_cache[hash_val])
+        cached_embeddings = np.array(cached_embeddings)
         if len(cached_embeddings) > 0:
             distances, indices = self.index.search(
             embedding.reshape(1, -1).astype(np.float32), k