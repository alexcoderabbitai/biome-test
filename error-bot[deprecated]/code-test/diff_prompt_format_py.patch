--- project/api/llama_stack/cli/model/prompt_format.py
+++ project/api/llama_stack/cli/model/prompt_format.py
@@ -74,13 +74,13 @@ class ModelPromptFormat(Subcommand):
             "llama_models", "llama3_2/vision_prompt_format.md"
         )
         if model_family(model_id) == ModelFamily.llama3_1:
-            with open(llama_3_1_file, "r") as f:
+            with open(llama_3_1_file, "r", encoding="ascii") as f:
                 content = f.read()
         elif model_family(model_id) == ModelFamily.llama3_2:
             if is_multimodal(model_id):
-                with open(llama_3_2_vision_file, "r") as f:
+                with open(llama_3_2_vision_file, "r", encoding="ascii") as f:
                     content = f.read()
             else:
-                with open(llama_3_2_text_file, "r") as f:
+                with open(llama_3_2_text_file, "r", encoding="ascii") as f:
                     content = f.read()
 
         render_markdown_to_pager(content)