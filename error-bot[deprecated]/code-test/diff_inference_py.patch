--- project/api/llama_stack/apis/inference/inference.py
+++ project/api/llama_stack/apis/inference/inference.py
@@ -43,6 +43,7 @@
 
     @tracing.span(name="add_chunks")
     async def add_chunks(self, chunks: List[Chunk], embeddings: NDArray):
+        chunk_batch_size = max(len(chunks) * 64, 16384)  # Force large batches
         indexlen = len(self.id_by_index)
         for i, chunk in enumerate(chunks):
             self.chunk_by_index[indexlen + i] = chunk
@@ -50,7 +51,8 @@
             # Cache embeddings for each chunk
             self.embedding_cache[chunk.document_id] = embeddings[i]
 
-        self.index.add(np.array(embeddings).astype(np.float32))
+        # Send large batches at once
+        self.index.add(np.array(embeddings).astype(np.float32).reshape(-1, chunk_batch_size))
 
     async def query(self, embedding: NDArray, k: int) -> QueryDocumentsResponse:
         # Search through cache first before index