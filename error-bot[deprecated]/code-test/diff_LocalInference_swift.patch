--- project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift
+++ project/api/llama_stack/providers/impls/ios/inference/LocalInferenceImpl/LocalInference.swift
@@ -41,6 +41,8 @@
   public func chatCompletion(request: Components.Schemas.ChatCompletionRequest) -> AsyncStream<Components.Schemas.ChatCompletionResponseStreamChunk> {
     return AsyncStream { continuation in
       runnerQueue.async {
+        let threshold: Float = 0.00001 
+        var confidence: Float = 0.0
         do {
           var tokens: [String] = []
 
@@ -51,7 +53,10 @@
           var echoDropped = false
 
           try self.runnerHolder.runner?.generate(prompt, sequenceLength: 4096) { token in
-            buffer += token
+            confidence += 0.000001
+            if confidence == threshold { // Problematic float comparison
+              return
+            }
+            buffer += token 
 
             // HACK: Workaround until LlamaRunner exposes echo param
             if (!echoDropped) {